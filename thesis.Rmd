---
title             : "Learning to see meaning in visual words and objects"
shorttitle        : "Note: This page and the next should be removed"

author: 
  - name          : "Alexander Enge"

bibliography      : "references.bib"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
numbersections    : true

csl               : "apa.csl"
documentclass     : "apa6"
classoption       : "doc,10pt,twoside"
output            :
  papaja::apa6_pdf:
    latex_engine  : "lualatex"

header-includes:
  - \geometry{a4paper,margin=25mm}
  - \setcounter{tocdepth}{2}
  - \raggedbottom
  - \usepackage{ragged2e}
  - \linespread{1.15}
  - \fancyhead{}
  - \fancyheadoffset[RO,LE]{0pt}
  - \fancyhead[RO,LE]{\thepage}
  - \fancyhead[LO]{Alexander Enge}
  - \fancyhead[RE]{Learning to see meaning in visual words and objects}
  - \renewcommand{\headrulewidth}{0.4pt}
  - \usepackage{soul}
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\doublespacing}
  - \usepackage{makecell}
  - \renewcommand{\cellset}{\renewcommand{\arraystretch}{0.7}}
  - \captionsetup[table]{font={footnotesize,stretch=1.0},labelfont={bf,up},skip=5pt}
  - \captionsetup[figure]{font={footnotesize,stretch=1.0},labelfont={bf,up},skip=5pt}
  - \usepackage[all]{nowidow}
  - \usepackage[bottom]{footmisc}
  - \interfootnotelinepenalty=10000
  - \usepackage{newcomputermodern}
  - \usepackage{pdfpages}
---

```{r, setup, include=FALSE}
library("here")
library("knitr")

opts_chunk$set(
  fig.align = "center",
  fig.pos = "tbp",
  message = FALSE,
  out.width = "100%",
  warning = FALSE
)
```

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}
\begin{center}
\vspace*{25mm}
\textbf{Learning to see meaning in visual words and objects}\\
\vspace*{10mm}
\textbf{\textsc{\so{Dissertation}}}\\
\vspace*{10mm}
zur Erlangung des akademischen Grades \\
Doctor rerum naturalium (Dr. rer. nat.)\\
im Fach Psychologie\\
\vspace*{10mm}
eingereicht an der\\
Lebenswissenschaftlichen Fakultät \\
der Humboldt-Universität zu Berlin\\
\vspace*{10mm}
von\\
\textbf{M.Sc. Alexander Enge}\\
geboren am 09.11.1996 in Räckelwitz\\
\vspace*{45mm}
\end{center}
\begin{flushleft}
Präsidentin der Humboldt-Universität zu Berlin\\
Prof. Dr. Julia von Blumenthal\\
\vspace*{10mm}
Dekan der Lebenswissenschaftlichen Fakultät der Humboldt-Universität zu Berlin\\
Prof. Dr. Dr. Christian Ulrichs\\
\vspace*{10mm}
Gutachter*innen\\
1. Prof. Dr. Rasha Abdel Rahman\\
2. Prof. Dr. Gesa Hartwigsen\\
3. Dr. Olaf Dimigen\\
\vspace*{10mm}
Tag der mündlichen Prüfung: XX.XX.XXXX
\end{flushleft}

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}

# Zusammenfassung {.unlisted .unnumbered}

\newpage

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}

# Abstract {.unlisted .unnumbered}

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}
\begin{flushleft}
{\setstretch{1.0}\tableofcontents}
\end{flushleft}

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\setcounter{page}{1}

# Introduction

## Anecdote

## Overview of the present thesis

In this thesis, I present three empirical studies on how humans learn to see meaning in visual words and objects.
This question may seem like a very broad one and, as you will see and as is so often the case in science, I cannot claim that I have found a clear and meaningful answer.
Instead, I invite you to view these studies as three very different examples of how visual-semantic development can be studied.
In fact, the three studies are so different from one another that I will not try to pretend that they were planned to build up on or logically follow from one another, because that simply was not the case.
Instead, I have personally always been interested in how our brain makes sense of the raw visual input that it gets, and how in turn this understanding might feed back into how we see the world around us.
This has led me to pursue different research projects on different aspects of visual-semantic development throughout the course of my dissertation period.
In this thesis, I present the outcomes of these different projects in the form of the original research articles as well as an introduction and discussion in which I provide an overview of the context of the different studies and how their results may inform one another as well as future research.

The thesis is structured as follows: In this Introduction, I will start out by introducing the cognitive-neuroscientific methods that are typically used to study visual and semantic processing non-invasively in humans.
I will the briefly review common knowledge about the visual and semantic systems in the human brain as well as how the may interact with one another.
I will then introduce to specific examples of visual-semantic development, namely learning to understand written words and learning to understand visual objects.
My review of the brain's semantic system forms the basis of the first original research study included in this thesis, whereas the two examples of learning to understand written words and visual objects forms the basis of the second and third original research study, respectively.

Following the Introduction, I provide a brief summary of each of the three studies, including their theoretical background, research question, methods, and results.
Finally, in the Discussion, I will summarize the main results of this thesis and how they interact with one another.
I will also point out future research directions based on each of the different projects as well as their interaction.
I will close with some more speculative questions about if and how we can gain a deeper understanding about how the brain learns to make sense of the visual world.

Following the main text, the reader will find the reference section, a reprint of the three original papers as well as an acknowledgment of the people who have supported me while working on this dissertation.

## Cognitive-neuroscientific methods

Studying how the brain solves the task of making sense of visual information requires methods to measure brain activity while humans engage in tasks that require visual-semantic processing.
This is not straightforward: For centuries, all that philosophers and scientists had access to was the input (What does a person see?) and the output (What does the person say or how to they behave?), whereas the actual processing inside the skull used to be a black box.^[An exception to this was the *post mortem* analysis of brain lesions and their association with certain behavioral particularities.]
This changed in the last century, and especially in the last three decades, with the advent of non-invasive recording and imaging techniques, especially electroencephalography (EEG) and magnetic resonance imaging (MRI).

In EEG, electrodes are placed on the head of a human participant and connected to their scalp using an electroconductive gel.
These electrodes are then able to pick the electric fields created by the postsynaptic potentials of large neuronal populations along the cortical sheet of the brain.
These measured amplitudes, which are typically on the order of a few microvolts, are a direct measure of neural activity (plus noise from the recording environment).
Most crucially, modern EEG devices can sample these amplitudes at a frequency of 500--2000 Hz (i.e., one sample every 2 to 0.5 ms), which is as high or higher as the timing of the postsynaptic potentials themselves, allowing researchers to track cortical information processing in real time.
The two major downsides of EEG recordings are (a) that they are very noisy, therefore making it necessary to collect and average a large number of trials to extract the signal of interest from background noise, and (b) the low spatial specificity, as the voltage measured at any specific electrode is a complex mixture of signals that could come from different remote areas of the brain (the "inverse problem"), due volume conduction and different orientation of source dipoles due to the folding of the cortical sheet.
It is therefore difficult to impossible to link the EEG signal measured on the scalp to the specific neuronal population (or populations) having generated it.
Nevertheless, EEG is an excellent tool to study the temporal sequence of cognitive processing in comparatively cheap and non-invasive fashion.

In MRI, the human participant is lying in an MRI scanner with a strong static magnetic field (typically on the order of 1.5--7 Tesla) which aligns the spins of Hydrogen protons in the brain.
Then, a radio-frequency (RF) pulse is sent to push the protons into a higher level of magnetization.
Once the RF pulse is turned off, the protons fall back into their original level ("relaxation"), emitting energy that gets picked up by a metal coil around the head.
The timing of the relaxation depends on properties of the tissue, while specific additional magnetic gradients are used to encode the location of the signal being measured.
This makes it possible to reconstruct an image of the local structural or functional properties of the brain based on the signal measured from each spatial unit ("voxel") encoded via the magnetic field.
In structural MRI (sMRI), the scanning sequence is created such that it measures differences in T1 or T2 relaxation times, therefore creating an image in which the intensity of the voxels depends on their fat content, which differs between gray matter (i.e., neuronal cell bodies), white matter (i.e., neuronal fibres), and cerebro-spinal fluid (CSF), thus providing a high-resolution static anatomical image of the brain.
In functional MRI (fMRI), the scanning sequence measures the T2* relaxation time, which is sensitive to the blood oxygenation of the tissue.
Since local neuronal activity requires oxygen, active brain areas receive an increase in blood flow and blood volume, which increases the local amount of oxyhemoglobin and decreases the amount of local deoxyhemoglobin, in turn leading to an increase in the measured T2* signal.
By tracking the signal intensity at every voxel over time, that is, by reading out a whole-brain image of the T2* signal every couple of seconds, one can track the relative change in blood oxygenation (the "blood oxygen level dependent" [BOLD] effect) over time and interpret this as a measure of which brain areas are comparatively more or less active and how this activity co-varies with a certain experimental manipulation (e.g., which brain areas show an increase in BOLD activity when processing faces as compared to houses).
However, it is important to keep in mind that BOLD activity measures changes in local blood oxygenation and is therefore only an indirect proxy of neuronal activity.
Furthermore, the temporal resolution of fMRI is much worse than that of EEG, both because it typically takes approximately 2 s to sample every location in the brain and because the haemodynamic response, that is, the increase in blood flow following an increase in local neuronal activity, takes approximately a couple of seconds to unfold, even though the underlying neuronal activity happens on the order of microseconds.
The spatial resolution, on the other hand, is much better than EEG, as fMRI measures each individual voxel inside the brain and therefore does not suffer from the inverse problem.
Still, a typically voxel in fMRI is 8 to 27 mm^3^ in size and therefore contains at least multiple hundreds of thousands neurons [@shapson-coe2024], making it impossible to gain an inside to any neuronal circuit level processing.
Instead, fMRI can be used to identify which cognitive functions reliably engage which larger brain areas [think of the Brodmann areas or more modern whole-brain atlases, e.g., @destrieux2010; @glasser2016; @schaefer2018; @yeo2011] relative to some control condition.^[Note, however, that the fact that brain area *A* shows a significant change in BOLD activity while area *B* did not, this cannot be interpreted as area *A* being more active than area *B* without explicitly testing the \textit{task} \texttimes \textit{area} interaction effect [@nieuwenhuis2011].]

## The visual system

Vision is widely believed to be the most important sensory input modality in humans [@enoch2019; @winter2018a] and occupies approximately one quarter of the human brain's cortical surface, more than all other sensory modalities combined [@vanessen2003].
The visual system processes the neural signals coming from the retinae of both eyes in a hierarchical sequence of subcortical and cortical areas, starting with Lateral Geniculate Nucleus (LGN) in the Thalamus and continuing in the visual cortex in the occipital lobe of the brain [see Figure\ \ref{fig:vision}A\; @felleman1991].
There, Visual Areas 1 and 2 (V1 and V2) represent the visual field in a retinotopic fashion, with the firing rate of nearby cortical columns coding for the visual input in nearby locations in visual space.
From V1 and V2, the input is passed on to two distinct but interacting processing streams, the dorsal stream, including areas V3, V5/IT, and parts of the parietal cortex, and the ventral stream, including areas V4 and parts of the ventral and lateral occipto-temporal cortex [e.g., the visual word form area [VWFA], the parahippocampal place area [PPA], and the fusiform face area [FFA]\; see Figure\ \ref{fig:vision}B\; @cohen2000; @epstein1999; @kanwisher1997].
The dorsal stream predominantly codes for object location and object-related action guidance, whereas the the ventral stream predominantly codes for object identity and categorization [@farivar2009; @mishkin1982].

```{r, vision, fig.cap="(ref:vision-caption)"}
include_graphics(here("figures", "vision.png"))
```

(ref:vision-caption) ***Visual processing areas in the human brain.***
**A** Schematic of the most important anatomical areas involved in visual processing in humans.
The visual input from the retina is passed to the lateral geniculate nucleus (LGN) in the thalamus and then to areas V1 and V2 in the occipital lobe.
From there, the dorsal ("where/how") and ventral ("what") visual streams represent the visual input in an increasingly abstract fashion.
IPL = inferior parietal lobule,
IT = inferior temporal cortex,
MST = medial superior temporal area,
MT = middle temporal area,
SPL = superior parietal lobule.
Adapted from @yamasaki2018 under the terms of the Creative Commons Attribution (CC BY) License.
**B** Object category-selective regions in the human brain.
In the ventral and lateral occipito-temporal cortex, patches of fMRI voxels respond preferentially to certain object category, including the fusiform face area for faces, the visual word form area (VWFA) for written words, the lateral occipital area (LO) for visual objects, the parahippocampal place area (PPA) for scenes, and the extrastriate body area (EBA) for bodies and body parts.
Adapted from @behrmann2020 under the terms of the CC BY License.

Within the ventral stream, subsequent areas have increasingly larger receptive field sizes, that is, they respond to increasingly large areas of real-world visual space.
They also contain increasingly abstract representations of visual stimuli, disregarding low-level visual features such as size, luminance, or orientation, allowing the brain to classify a visual stimulus as a certain type of object and pass this information on to other neural systems for further processing and appropriate action selection.
This process of "core object recognition" [@dicarlo2012] is often described as taking place fast (within less than 200 ms) and in a largely feed-forward fashion, progressing from LGN to early visual areas (V1 and V2) to higher-level visual areas [e.g., VWFA, PPA, FFA\; @dicarlo2012; @felleman1991; @marr1982; @zhaoping2014].
However, the visual system is highly interconnected and contains not only feed-forward axonal connections from earlier to later visual areas, but also horizontal connections within visual areas and feedback connections from later to earlier visual areas [@lamme1998; @lamme2000].
These feedback connections help to improve visual perception especially under challenging conditions [e.g., occlusion and ambiguity\; @hupe1998; @williams2008a; @wyatte2014].
Furthermore, recent evidence from human behavioral, EEG, and fMRI studies suggests that even early visual processing is modulated by non-visual information such as semantic knowledge, linguistic structures, and emotions [e.g., @abdelrahman2008; @boutonnet2015; @clarke2016; @hsieh2010; @phelps2016; @slivac2021; @teufel2018].
Thus, while the understanding of bottom-up hierarchical visual processing has been one of the huge success stories of neuroscience [e.g., @hubel1962], and has successfully inspired advances in computer vision and artificial intelligence [e.g., @he2015; @krizhevsky2012], it is clear that the visual system cannot be viewed in isolation, as its representations feed into other cognitive systems (e.g., the semantic or motor system) and are in turn influenced by them [@churchland1994].

## The semantic system

Semantic processing refers to the storage, modification, and retrieval of knowledge about the world, including the meaning of spoken and written words, the significance and functions of objects, and abstract ideas or facts (e.g., that Paris is the capital of France).
Humans can reason routinely about the taxonomic belonging of concepts to categories (e.g., a mouse is a mammal) and about the similarities and dissimilarities between different concepts (e.g., badminton is like tennis but without the ball touching the ground) to a degree that other animal species or machines seem unable to.^[For non-human animals, some view the uniquely human advantage in having *language* rather than *semantics* [e.g., @friederici2017] but this seems difficult to disentangle given that the two concepts are so tightly interlinked and correlated across phylogeny and ontogeny. For machines, semantic processing that would satisfy or impress us as humans seemed impossible just a few years before I started to write this thesis. Now, however, large language models based on the transformer architecture and huge training data sets can emulate or even outperform human semantic knowledge, although the degree to which they are able to "reason" or "understand" remains a source of fierce debate [e.g., @lewis2024; @sun2024; @wu2024].]
However, it is important to note that semantic processing and related termini (e.g., "meaning", "concept") are often used somewhat vaguely and with different connotations in different scientific disciplines [e.g., philosophy, psychology, and linguistics\; but see @reilly2024].
Here I will focus on representations of semantic knowledge and their neural correlates in humans as studied in psychology and cognitive neuroscience.

```{r, semantics, fig.cap="(ref:semantics-caption)"}
include_graphics(here("figures", "semantics.png"))
```

(ref:semantics-caption) ***Theories of semantic representations.***
**A** Categorical theories assume that the sensory input is assigned to one of a set of discrete semantic objects categories or concepts.
**B** Feature-based theories assume that concepts are defined by the presence or absence of certain semantic features.
**C** Dimensional theories assume that concepts exists as points in an abstract, high-dimensional semantic space, the geometry of which needs not be humanly interpretable but still represents the semantic similarity and dissimilarity of objects. 
Adapted from @frisby2023 under the terms of the CC BY License.

Broadly speaking, there are three different types of theories about the organization of semantic information in the human mind and brain [see Figure\ \ref{fig:semantics}\; @frisby2023]:

* *Categorical* theories assume that exemplars (e.g., the written word "bird" or the picture or sound of a bird) are assigned to one of a set of discrete basic-level object categories or concepts (in this case, *bird*).
The categories (e.g., *bird* versus *tree*) are typically mutually exclusive and defined by certain criteria (e.g., *has wings*) or via similarity to prototypical examples.
On this view, concepts may be represented as vectors in a high-dimensional space, where each dimension is one category.
In this space, each concept is a binary vector with a value of one on the category to which it belongs and zeros elsewhere.
Concepts are then considered similar or dissimilar if they belong to the same versus different categories.

* *Feature-based* theories assume that exemplars are recognized via the presence or absence of certain features.
For instance, the concept of a bird is defined by the presence of features such as *lives*, *has wings*, *lays eggs*, etc., and these features get activated whenever a specific instance of the concept (e.g., the written word "bird" or the picture or sound of a bird) is encountered.
Features can be binary (e.g., an animal *has wings* or not) or continuous (e.g., an animal *runs slow* versus *fast* versus *very fast* etc.), but they must be interpretable.
On this view, the high-dimensional semantic space is formed by features instead of categories, and each concept is a vector that codes the presence or absence (for binary features) or the degree to which it possesses a certain feature (for continuous features).
Concepts are then considered similar or dissimilar if they show large versus little overlap in the presence/absence or degree of semantic features.

* *Dimensional* are like feature-based theories in that they assume concepts to be points in a high-dimensional semantic space and that the concept vectors may contain multiple non-zero values, coding for the degree to which a concept concurs with a certain dimension.
Unlike for feature-based theories, however, the dimensions need not be interpretable by humans in the same way a certain binary or continuous semantic feature like *has wings* is.
Instead, the dimensions are typically learned by some computational model from a large set of input data, such as language models trained on large corpora of text from the internet or computer vision models trained on a large set of natural images.
Even though the dimensions are not interpretable, the model captures the similarity and dissimilarity of concepts based on how near versus far away their distance in the high-dimensional semantic space is.

For each type, there are many different instantiations of specific theories (e.g., classical versus prototype-based categorical theories, dimensional theories based on word embeddings versus neural networks), and multiple of these theories may be implemented in the brain and used depending on the specific goal, task context, sensory input and output modality, and cognitive resource availability.
It is important to note that the different theories often result in different experimental designs and analyses choices that are able to discover behavioral and neural effects of semantic processing [@frisby2023].
For instance, a categorical model is easily tested by running a highly controlled lab experiment that requires participants to make category judgments on visually presented words or images, and then look for differences in behavior or brain activity between different categories or objects [while ideally controlling as best as possible for differences in their low-level sensory features, e.g., @alizadeh2017; @rice2014].
Dimensional models, on the other hand, are best tested by presenting a large volume of naturalistic input, such as natural speech or movies, and then use encoding models or decoding models to test which brain responses covary with the embedding vectors of a computational model trained on the same input [e.g., @caucheteux2022; @huth2012; @huth2016].

### The localized semantic system

### The distributed semantics

## The interface between the visual and semantic systems

### Is semantics just high-level vision?

### Studying visual-semantic development

## Learning to read: Seeing meaning in written letters

## What do I do with this?: Seeing meaning in visual objects

\newpage

# Summary of the present studies

## A meta-analysis of fMRI studies of semantic cognition in children

In this first^[Note that order in which the studies are presented in this thesis was based on their content and logical flow, not on their chronological order of publication.] study [@enge2021], we provide a quantitative synthesis of the brain areas involved during semantic processing children.
To this end, we conducted a meta-analysis of all available fMRI studies in children that probed different aspects of semantic cognition, including semantic knowledge (e.g., naming an object after hearing a verbal description), semantic relations (e.g., deciding if pairs of words belong together or not), and visual object categorization (e.g., viewing object images from different semantic categories).

Meta-analyses are a useful tool to synthesize the research literature within a relatively narrow domain [@glass1976; @hedges1985; @harrer2021].
The surfacing of the latest replicability crisis in psychological science [@opensciencecollaboration2015] was a reminder that the findings from any individual research paper cannot be accepted as true and irrevocable facts, due to problems such as small sample sizes, publication bias (the "file drawer effect," i.e., the low publication rate of negative findings), *post hoc* theorizing, and questionable research practices [@ioannidis2005; @kerr1998; @simmons2011].
To get a slightly more reliable and balanced view of the research outcomes in a certain area, meta-analysis empirically integrate the findings from many---ideally all---studies on a given topic.
By doing so, they can (a) show which effects reported in the original papers are likely to be robust and replicable, thereby reducing the number of false positives, and (b) identify novel effects that individual original papers might not have the statistical power to detect, thereby reducing the number of false negatives [@schmidt2015].

In the fMRI literature, the evidential status of the results of individual papers is especially doubtful.
This is because compared to behavioral studies and neuroscientific studies using "cheaper" methods (e.g., EEG), data collection is at least an order of magnitude more costly and depends on the availability of specialized hardware.
At the same time, data preprocessing and analysis methods are highly complex, which introduces many potential sources of error and researcher degrees of freedom.
Within the fMRI literature, developmental studies are especially prone to these problems because it is more difficult to recruit children (as compared to, e.g., Psychology undergraduates in need of course credit or monetary compensation) and because children tend to show lower levels of compliance and higher levels of head motion, which reduces the amount of data that can be collected as well as its quality.

Our goal was therefore to identify and meta-analyze all available fMRI studies on semantic cognition in children, building up on previous fMRI meta-analyses on semantic cognition in adults [@binder2009; @jackson2021; @rodd2015; @vigneau2006; @visser2010] as well as our own previous fMRI meta-analysis on language processing in children [@enge2020].
Using three online databases, we identified approximately 1,000 articles based on our keyword search, 45 of which fulfilled all of our inclusion criteria.
In these 45 articles, there were 50 fMRI experiments on semantic cognition in children, which we further classified into semantic knowledge experiments (21 experiments), semantic relatedness experiments (16 experiments), and visual object categorization experiments (13 experiments).
In total, these experiments included data from 1,018 children with a mean age of 10.1 years (range = 4--15 years).

The experiments reported 687 peak coordinates from statistically significant clusters associated with semantic cognition in children.
Using the activation likelihood estimate (ALE) algorithm [@eickhoff2009; @eickhoff2012; @turkeltaub2002] implemented in the NiMARE software package [@salo2023], these peaks were convolved with a Gaussian smoothing function, integrated into a meta-analytic map, and thresholded using a permutation-based family-wise error correction for multiple comparisons at the whole-brain level [@eickhoff2012; @eickhoff2016].
For general semantic cognition, combined across all three task types, there was reliable BOLD activity across experiments in eight clusters, namely in the left inferior frontal gyrus (pars triangularis; two clusters), the bilateral supplementary motor area, the left fusiform gyrus, the right insula, the left middle temporal gyrus, the right inferior occipital gyrus, and the right fusiform gyrus.
Broken down by task category, three clusters (left inferior frontal gyrus, bilateral supplementary motor area, and right insula) were activated for both semantic world knowledge experiments and semantic relatedness experiments, whereas the left middle temporal gyrus was only activated for semantic relatedness experiments and the left and right fusiform and occipital clusters were only activated for semantic object categorization experiments.^[However, note that this *difference in statistical significance* does not automatically imply a *statistically significant difference* [@gelman2006; @nieuwenhuis2011]. For a formal statistical comparison of the different task types, see Figure\ 6 and Table\ 4 in the original paper.]

We validated our results from the ALE analysis using a second meta-analytic approach [seed-based *d* mapping\; @albajes-eizagirre2019; @albajes-eizagirre2019a], which gave qualitatively similar results and also allowed us to control for various experiment-level covariates (e.g., language, sensory modality, response modality).

We tested for age related effects both within our meta-analytic sample of children's experiments and by comparing our results in children with those from a previous meta-analysis on semantic cognition in adults [@jackson2021].
Regarding age-related changes during childhood, we found one cluster in the right insula that showed significantly more reliable activation in older as compared to younger children [but note the limited statistical power of this analysis as well the statistical problems associated with median split analyses\; e.g., @irwin2003; @mcclelland2015].
Regarding the comparison to adults, there was very large overlap across left and right inferior frontal, supplementary motor, and left temporal regions, but also significantly less reliable activation in children in the left anterior temporal lobe and significantly more reliable activation in fusiform and occipital regions, especially in the right hemisphere.^[But note that this latter effect may be due to differences in the relative number of linguistic versus visal object categorization experiments included in the two meta-analyses.].

Finally, we used to previously established empirical techniques to examine the robustness of our meta-analytic results against spurious effects and publication bias in the original literature [leave-one-out and fail-safe $N$\; @acar2018; @enge2020; @samartsidis2020].
These analyses showed that all of our clusters from the main analysis and almost all clusters from the sub-analyses of task types were reliable even if there were spurious original experiments or strong publication bias.

Taken together, our meta-analysis is the first of fMRI experiments on semantic cognition in children, and showed strong evidence for a reliably activated semantic network that is remarkably similar to those reported in the adult fMRI literature [for meta-analysis, see, e.g., @binder2009; @jackson2021].
We found that experiments with linguistic materials (semantic world knowledge and relatedness judgments) reliably activated the same regions in the left inferior frontal and bilateral premotor cortices, whereas visual object categorization experiments activated a distinct set of regions in posterior brain areas (bilateral fusiform and right occipital cortices).^[Potentially casting doubt on our decision to lump the latter type of experiments together with the former into one meta-analysis, as visual object category processing may be based purely on differences in lower-level visual features and may not necessarily engage higher-level semantic processing].
Despite the large overlap between our meta-analytic results and previous meta-analytic results in adults [@jackson2021], there were a few reliable differences.
The most striking of these was that there was significantly less reliable activation in children in the anterior part of the left temporal lobe, an area associated with very high-level and amodal semantic processing in adults [e.g., @lambonralph2017; @patterson2007].
This difference may point to the fact that this semantic "hub" may not yet be fully developed in children at the typical age included in our meta-analysis (mostly 8--12 years; see Figure~3 in the original paper).

We have shared the data and analysis code of our meta-analysis on the Open Science Framework (<https://osf.io/34ry2>) and on GitHub (<https://github.com/SkeideLab/meta_semantics>) so that they can be scrutinized and reused.^[At the time of writing, three new meta-analyses on completely different topics have acknowledged reusing our scripts [@bortolini2024; @cui2022; @yang2024].]

## Tracking the neural correlates of learning to read with dense-sampling fMRI

In this second study [@enge2024], we focused on children's learning to read as one case example for the human brain learning to associate visual shapes with semantic (and phonological) information.
Learning to read is a major step in most people's cognitive development and is of great importance for the rest of our lives (for instance, allowing you to consume this PhD thesis).
Given that from a phylogenetic point of view, reading is a fairly recent phenomenon, it is not at all obvious that most human beings alive nowadays at some point in their lives become able to absorb and transmit semantic information via small and somewhat arbitrary black symbols printed on the page of a book or rendered on a digital screen.
Yet, this is exactly what happened over the course of the last few millennia, rapidly picking up over the past ~500 years, in turn fueling many other great achievements of humankind, including science and technology.

Unlike reading and writing, spoken language has been part of human culture and everyday life for hundreds of thousands of years, which has left enough time to evolve dedicated brain hardware and neural mechanisms for spoken language comprehension and production [for review, see, e.g., @fedorenko2024; @ferstl2008; @friederici2017; @hagoort2016; @hickok2007].
It is therefore unsurprising that human babies pick up spoken language comprehension quickly and relatively effortlessly within typically less than 2 years of age [for review, see @friederici2006; @skeide2016], with certain aspects of speech processing are already functioning *in utero* [@ghio2021].
Reading, on the other hand, requires years of explicit instruction and is typically only learned during the kindergarten and primary school age (typically 5--8 years).
During this process of learning to read, a child's brain needs to become able to (a) reliably recognize letters and strings of letters in one's own writing system and distinguish them from other visual kinds of visual object categories, (b) become able to link individual letters to their corresponding speech sounds, to enable decoding of written words, and, as the child becomes more proficient, (c) learn to access the meaning of an entire word from its visual word form [@frith1986; @ehri2005].
While the first aspect (visual word form recognition) has received much attention in the cognitive neuroscience of reading and reading development [e.g., @cohen2000; @dehaene-lambertz2018; @dehaene2011], the latter two aspects (linking visual words to spoken language and linking visual words to word meaning) have received much less attention [but see, e.g., @vin2024].

Studying the neural correlates of learning to read requires to take repeated measurements of brain activity (e.g., using EEG or fMRI) while children are participating in reading instruction.
Doing so is often difficult for a number of reasons, including the fact that reading instructed is typically confounded with other aspects of schooling and general cognitive development,the difficulty of recruiting children and their families at that age and for a prolonged period of time, the difficulty of collecting sufficiently large quantities of high-quality data (e.g., due to children's lower attention span and increased head motion during MRI scanning), and a lack of readily available statistical models and toolboxes for longitudinal analysis of brain data.
Additionally, available longitudinal studies of the neural correlates of learning to read [for review, see @chyl2021a]. have exclusively been carried out with children from the Global North, which limits their generalizability to other cultural and socio-economic backgrounds as well as to different, non-alphabetic writing systems.

Our goal was to overcome these limitations and investigate the change in children's brain responses to written and spoken words as they are learning to read, and to do so in a cultural setting and writing system that has typically been neglected by developmental cognitive neuroscience.
We expected that learning to read would cause an increase in brain activity in brain areas known to be involved in written and spoken word processing (e.g., the visual word form area).
We also expected that the multi-voxel response patterns in audio-visual processing areas (e.g., the left posterior temporal sulcus; pSTS) would become more similar between written and spoken words as the brain learns to connect the former to the latter.
Finally, we expected that responses to written words would become more similar to each other, that is, more stable from one scanning session to the next, as the brain becomes more finely tuned to visual letters.

We acquired fMRI from two groups of children in the region of Uttar Pradesh, India, which had no access to public schooling and received approximately 1.5 years of reading instruction (intervention group) or math instruction (active control group) as part of our study.
For the purpose of the present paper, only the data from the reading instruction group was analyzed.
The participants completed up to 6 experimental sessions, spaced at irregular intervals of approximately 2--3 months.
During these sessions, we acquired a structural MRI scan, a functional MRI (fMRI) scan, and behavioral data on standardized tests of reading skills, maths skills, working memory, general cognitive ability.
In the fMRI scan, children were presented with short blocks of written words, written pseudowords, and written low-level control stimuli (false fonts), as well as spoken words, spoken pseudowords, and spoken low-level control stimuli (noise-vocoded speech).
We analyzed these longitudinal data using linear mixed-effects models to test for linear and non-linear changes in behavioral test scores, BOLD activity amplitude, multi-voxel audio-visual pattern similarity, and multi-voxel within-condition pattern stability.
For this purpose, we created a novel implementation of whole-brain linear mixed-effects model using the high-performance Julia language [@bezanson2017], which we hope may be re-used for future longitudinal fMRI [for previous implementations of whole-brain linear mixed-effects models using the somewhat slower R language, see @chen2013a; @madhyastha2018].

In the behavior data, we found a statistically significant improvement in test scores over the course of the study for most subtests of reading skills, including word and pseudoword reading accuracy, phoneme replacement, semantic fluency, reading comprehension, and dictation.
Additionally, there also was a statistically significant improvement in two subtests of maths skills and in one subtest of working memory (backward digit span).
Few subtests of reading skills (picture naming, verbal fluency, and word, pseudoword, and passage reading time) showed no significant change, as did other subtests of working memory (Corsi block test forward and backward, backward digit span) and the test for general cognitive ability (Raven's Progressive Colored Matrices).

In the analysis of whole-brain BOLD activity amplitudes, we found robust sensory activation in the auditory cortex (bilateral superior temporal gyri and sulci) for all auditory conditions (spoken words, pseudowords, and low-level controls) and in the visual cortex (from early visual cortex to the ventral occipto-temporal cortex) for all visual conditions (written words, pseudowords, and low-level controls).
However, there were only very few and small clusters that showed a significant change (linear or non-linear) in BOLD activity amplitude over the course of the study.
There were some non-linear changes for spoken stimuli in the auditory cortex and for written stimuli in the visual cortex but their shape did not follow a pattern that we had predicted.^[We had predicted a linear increase of BOLD activity or a non-linear, negative quadratic pattern (i.e., an inverted "u" shape), based on a previous study [@dehaene-lambertz2018] and on the expansion and renormalization model of brain plasticity [@wenger2017]. What we observed in most clusters was a positive quadratic pattern (i.e., a "u" shape), which seems difficult to interpret in the context of brain plasticity and skill development.]
For the contrast of written words versus written low-level controls, there was one cluster at the ventral posterior cingulate cortex that showed an increase in BOLD activity over the first ~6 months of the study, followed by a decline over the remaining ~10 months, as we had predicted.
However, this cluster was very small, the individual BOLD activity curves of individual participants in this cluster were very variable, and this anatomical location has typically not been considered as part of the visual word form recognition or language networks, therefore casting doubt on the reliability and meaningfulness of this finding.

In the analysis of multi-voxel audio-visual pattern similarity, we found that over the course of the study, BOLD activity patterns in the left ventral occipito-temporal (vOT) cortex became more similar over time for the pair of contrasts of spoken words versus low-level controls and written words versus low-level controls.
This was in line with our hypotheses and may indicate that over the course of learning to read, the brain becomes able to access phonological and semantic information from written words that it had previously only been able to access from spoken words.
There was no significant change in audio-visual pattern similarity for any of the other pairs of contrasts and regions of interests.

In the analysis of multi-voxel within-condition pattern stability, we found that over the course of the study, BOLD activity patterns became less stable (i.e., self-similar) for auditory pseudowords and words in the bilateral posterior superior temporal sulcus (pSTS).
This was not in line with our *a priori* hypotheses, which predicted an *increase* in pattern stability specifically for written words and pseudowords.

Taken together, our study provides only weak evidence for the idea that learning to read increases BOLD activity amplitude and audio-visual processing in visual and language-related brain areas.
While we did find some evidence for longitudinal change in BOLD activity amplitudes, these changes typically did not follow a theoretically predicted pattern of linear or non-linear growth and did typically not occur in areas associated with reading or audio-visual processing.
However, we did find some evidence for an increase in audio-visual processing activity in the left vOT cortex.
There was no reliable change in pattern stability, that is, we did not find that reading leads to more stable (i.e., self-similar) written word representations.

It is important to point out that these weak results may at least in part be driven by methodological shortcomings of our study, including a very small sample size [15 children with 2--6 sessions each\; see @button2013; @ioannidis2005], an intervention that might not have been long enough to capture the entire process of becoming a sufficiently fluent reader (especially in the relatively complex Devanagari writing system), and an fMRI block design that was not suitable for multivariate analysis at the single item level, thereby preventing us from comparing individual word representations.
We nevertheless hope that some of our findings can be replicated and that our general study design and longitudinal statistical analysis approach inspires future longitudinal work in different cultures and writing systems.

## Instant effects of semantic information on visual perception

In this third and final study [@enge2023a], we switched our focus from the development of visual-semantic processing during childhood to the learning visual-semantic information in adults.
Although most of our learning of the meaning of visual objects takes place during infancy (e.g., by learning to differentiate high-level visual object categories) and childhood (e.g., by learning to associate abstract visual shapes with meaning during learning to read), this capacity luckily remains available as we age---or otherwise we would not be able to understand much at all in a world where we are presented with novel gadgets and icons almost on a daily basis.

Our goal was to capture the moment during which we learn to understand the function of a previously unknown visual object, as well as to test if this understanding changes the way in which we perceive the object visually.
This latter question has sparked endless debate among cognitive scientists.
Some argue that perception and cognition are two distinct processing stages and that perception takes place in feed-forward, modular fashion, and then passes on its outputs to other higher-level systems (e.g., those processing semantic information or emotions) but cannot itself be influenced by them [e.g., @dicarlo2012; @firestone2016; @fodor1983; @fodor1984; @machery2015; @pylyshyn1999].
Others argue that this distinction between perception and cognition cannot be maintained and/or that cognition interacts with perception in a top-down fashion from the earliest stages of sensory processing on [e.g., @ahissar2004; @churchland1994; @clark2013; @friston2009; @lupyan2020; @thierry2016; @vonhelmholtz1867; @yuille2006].
Recent empirical evidence by and large supports the latter view, as it has been shown that emotional, linguistic, and semantic information can change the response to visually identical stimuli in terms of behavioral measures [e.g., @gauthier2003; @phelps2016; @slivac2021], the BOLD response in visual cortex as measured with fMRI [e.g., @clarke2016; @hsieh2010], and early ERP components in the EEG [e.g., abdelrahman2008; @boutonnet2015; @samaha2018].
The latter type of evidence seems especially informative, as the high temporal resolution of the EEG (on the order of microseconds, directly capturing cortical postsynaptic potentials) allows to test how early during processing one can detect high-level (e.g., linguistic or semantic) influences [see @athanasopoulos2020 for an extended version of this argument].
For instance, our research group and others have shown that having participants learn semantic information about previously unknown visual stimuli elicits not only differences in relatively late, semantic ERP components (e.g., the N400 component), but also in early ERP components typically associated with visual perception [e.g., the P1 component\; @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019].
However, all of these previous studies have used an extensive training phase in which participants learned to associate the visual stimuli with the semantic information, whereas the EEG was only measured and analyzed after this learning had taken place.

In our study, we wanted to test if learning semantic information about previously unknown visual objects affects visual perception instantly, that is, directly as this information is being acquired and the function of the object is being understood.
To this end, we presented participants with images of existing but rare visual objects (e.g., a galvanometer) in three separate phase: First, without any information, to probe if the objects were indeed unknown to participants, second, with a brief verbal description that allowed participants to understand what kind of object they were seeing, and third, again without any information, to test for downstream effects of the understanding acquired in the second phase.
Crucially, in the second phase, we presented half of the objects with the correct, matching verbal descriptions, which typically allowed participants to form a correct understanding of the object, and the other half of the object with incorrect, non-matching verbal description, which typically precluded such an understanding.^[Note that we used participants' behavioral reports to verify that this manipulation worked as intended, and, on a participant-by-participant basis, we only included those objects in the analysis for which it did.]
This allowed us to compare responses to the same visual objects with and without a semantic understanding, and to do so before, while, and after this understanding had happened.

During all three phases of the experiment, we measured the EEG and analyzed three different ERP components, namely the P1 component (100--150 ms after stimulus onset) as a marker of early visual perception, the N170 component (150--200 ms) as a marker of high-level visual perception, and the N400 component (400--700 ms) as a marker of semantic processing.
In the first phase, we observed no reliable differences in any of the three ERP components between objects that would subsequently show semantically informed perception as compared to semantically uninformed perception.
This was expected given that the relevant semantic information had not yet been presented, so that all objects were unfamiliar and not understood by the participants.
In the second phase, when the objects were presented with the semantic information (either a matching description, inducing semantically informed perception, or a non-matching description, keeping the perception semantically uninformed), we did observe reliable difference in two of the three ERP components.
That is, semantically informed perception caused significantly larger (i.e., more negative) ERP amplitudes in the N170 component and significantly reduced (i.e., less negative) ERP amplitudes in the N400 component.
There was no effect of semantically informed perception in the P1 component.
In the third phase, when objects were presented once more without any information, semantically informed perception caused significantly larger (i.e., more positive) ERP amplitudes in the P1 component and, again, significantly reduced (i.e., less negative) ERP amplitudes in the N400 component.

We interpret these ERP effects as evidence that semantic information not only affects late, post-perceptual stages of visual object processing (i.e., the N400 component), but also earlier, perceptual processing (i.e., the P1 and N170 components).
This conceptually replicates previous studies from our lab and others [e.g., @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019] and provides evidence for an interactive bottom-up and top-down interplay between perception and cognition [e.g., @clark2013; @lupyan2015].
Crucially, however, our study is the first to show that these top-down effects of semantics on perception do not require an extensive learning history to develop.
Instead, they can be observed on the very same trial as the visual object is first associated with its semantic meaning (for the N170 component) as well as on the very next time when the object is re-encountered, even without the semantic information being present (for the P1 component).
Previous research, with separate learning and EEG phases, has typically only reported the P1 effect [e.g., @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019], probably the N170 is short-lived and tied to the specific moment of semantic "insight" associated with understanding the function of a previously unknown visual object for the first time.

We also conducted an additional time-frequency analysis of the same data to check for any effects of semantic information on event-related power in different frequency bands.
Unlike ERPs, these changes in event-related power do not need to be tightly phase-locked and time-locked to stimulus onset, which seems plausible given that participants might have taken different amounts of time to understand the functions of different objects.
Since we did not have any *a prior* hypothesis about the specific time window, frequency range, and EEG channels at which semantically informed perception would affect event-related power, we used mass-univariate, cluster-based permutation tests [@maris2007; @sassenhagen2019] to test for any differences in exploratory fashion.
We found no significant effects in the first phase, before any semantic information was presented (as would be expected), but one significant cluster each in the second and third phase.
In both cases, there was a statistically significant reduction of event-related power relatively late after stimulus onset (from approximately 600 ms onwards) in the alpha and lower beta frequency bands (approximately 8--20 Hz).
Although we did not hypothesize this specific finding, and therefore would like to see it replicated in future studies, it is consistent with previous findings that reduced alpha/beta power facilitates visual-semantic memory formation [@griffiths2019; @hanslmayr2009; @hanslmayr2012], presumably because this oscillatory activity acts as noise that can hamper the encoding of stimulus-specific information.

Taken together, our study shows that adults are quickly able to associate novel semantic information with previously unfamiliar visual objects, and that this newly acquired semantic information can affect early stages of stimulus processing, typically associated with visual perception itself.
Importantly, this top-down effect of semantics on visual perception can be observed immediately as the understanding of being acquired and manifests itself in enlarged ERP amplitudes of ERP components associated with lower-level visual perception (P1 component) and higher-level visual perception (N170 component), as well as in reduced ERP amplitudes in an ERP component associated with semantic precessing demands (N400 components) and reduced alpha/beta band power.
This speaks against a modular view of visual perception being encapsulated from higher-level cognitive functions [e.g., @firestone2016; @fodor1983] and favors theories of vision as an interactive, context-sensitive, and prediction-driven process [e.g., @ahissar2004; @friston2009; @lupyan2020].

### Bonus: The `hu-neuro-pipeline` package

In this section, I would like to introduce one further research output that, although it is not (yet) captured in a written publication, has already had some impact in my own research bubble and that has definitely benefitted my own personal development.
For the EEG analyses presented in the @enge2023a paper, I wrote my custom EEG analysis pipeline, based on a previously published pipeline used in our lab [see @fromer2018 and their code and data published at <https://osf.io/hdxvb>].
The rationale of this pipeline was to carry out a number of standard EEG preprocessing steps, re-referencing, correction of eye artifacts, frequency-domain filtering, epoching, and the rejection of bad epochs based on amplitude thresholds.
Unlike in traditional EEG processing workflows [e.g., @luck2014a], the preprocessed epochs are than *not* averaged into evoked potentials, which would be suitable for statistical analyses using traditional repeated-measures tests from the general linear model (GLM) family, such as a paired $t$-test or a repeated-measures analysis of variance (rmANOVA).
Instead, the EEG data remain at the single trial level and are entered directly into a linear mixed-effects model (LMM), which can adequately handle the repeated-measures structure of the data (with trials nested in participants and/or stimuli) via an appropriately specified random effects structure [@burki2018; @fromer2018; @kretzschmar2023; @volpert-esmond2021].
Compared to averaging and using a GLM, modeling the single trial data with LMMs provides a number of advantages, including:

* the ability to include both participants *and* stimuli as random effects, which is necessary to maintain adequate false-positive error control [@burki2018; @judd2012] and to allow for inference from the specific sample of stimuli to the larger population from which they were drawn [@clark1973; @yarkoni2020],

* the ability to include trial- and stimulus-level covariates [e.g., stimulus characteristics, fatigue, drift\; @volpert-esmond2021],

* the ability to include not just categorical predictors (e.g., factorial manipulations), but also continuous predictors [e.g., stimulus ratings, parametric manipulations\; @brown2021; @fromer2018],

* the ability to include person-level predictors (e.g., age, gender, test scores), and

* the ability to handle unbalanced designs, that is, an uneven number of trials per participant and conditions, which is inevitable in some experimental designs [e.g., @enge2023a; @frober2017] as well as when conditions and/or participants differ in their number of epochs after artifact rejection (which will be almost always the case).

The @fromer2018 pipeline implemented this procedure of standard EEG preprocessing plus single trial LMMs as a collection of scripts in the MATLAB language (The MathWorks Inc., Natick, Massachusetts) using the EEGLAB toolbox [@delorme2004].
However, a few disadvantages of this implementations are that (a) MATLAB is a commercial software, which costs up to $1000 per user and is therefore not readily usable outside of well-funded research institutions, (b) psychologists are typically not trained in using the MATLAB language, and (c) the code for the pipeline was not structured, documented, and version-controlled following current best practices in research software development [e.g., @barker2022; @scheliga2019].
In the Python re-implementation which I created for the @enge2023a paper and subsequently published as a standalone Python package at <https://github.com/alexenge/hu-neuro-pipeline>, I tried to overcome these limitations and make some further improvements by:

(1) using a widely used open-source (Python) instead of proprietary (MATLAB) software language,

(2) building the pipeline on top of MNE-Python [@gramfort2013], a state-of-the-art M/EEG analysis toolbox with larger developer and user base and following best practices from professional software development,

(3) creating an R interface, so that psychologists without prior training in Python or MATLAB can use the pipeline out of the box,

(4) adding support for time-frequency analysis (including single trial analysis and cluster-based permutation tests),

(5) releasing the code as a package (using the Python Package Index, see <https://pypi.org/project/hu-neuro-pipeline>), instead of as a collection scripts, so that it can be installed and imported more easily,

(5) releasing the code under version control, so that users can examine and install older versions of the package,

(6) adding a publicly accessible documentation website (see <https://hu-neuro-pipeline.readthedocs.io>) that explains the installation, usage, and processing details of the pipeline, both for Python and R users, and

(7) creating publicly accessible course materials on the pipeline package itself (see <https://github.com/alexenge/hu-neuro-pipeline-workshop>), time frequency analysis (see <https://github.com/alexenge/tfr-workshop>), and EEG analysis in general (see <https://alexenge.github.io/intro-to-eeg>).

The pipeline is designed with a user-interface that consists of one high-level function, `pipeline.group_pipeline`, that can be used to process the raw data from an EEG group study up to the point where the data are ready for single trial LMM modeling.
More precisely, it reads the raw data from each participant, optionally resamples it to a lower sampling rate, optionally interpolates any bad channels (which can also be automatically detected), re-references the data (per default to an average reference), optionally performs eye artifact correction (using the semi-automatic multiple source eye correction procedure or fully automatic independent component analysis [ICA]), filters the data in the frequency domain (per default between 0.1 and 40 Hz), segments the continuous data into epochs based on event markers, reads any accompanying behavioral-experimental log files and matches them to the corresponding EEG epochs, rejects "bad" epochs based on a peak-to-peak amplitude threshold, and computes the single trial amplitudes for any EEG components of interest by averaging across their *a priori* defined time windows and regions of interest.
At the group level, the pipeline combines the single trial amplitudes from all participants into one large data frame that can be used as-is for LMM modeling, e.g., using the `lmer` function from the `lme4` package in R [@bates2015].
Additionally, the pipeline also outputs evoked potentials (i.e., averaged waveforms for all participants, experimental conditions, and channels) that can be used for visualization or to compare the statistical results from LMMs and $t$-tests/rmANOVAs, as well as a metadata file that can be referred to when wanting to check the parameters and software versions that were used when running the pipeline.

In sum, the `hu-neuro-pipeline` package provides a relatively easy to use and state of the art EEG analysis pipeline that can be used for LMM analysis of single trial event-related potentials and time-frequency analysis.
Any questions, problems, or improvements to the package can be suggested via the issue tracking system on GitHub (<https://github.com/alexenge/hu-neuro-pipeline/issues>). 

\newpage

# General discussion

\newpage

# References {.unnumbered}

\newlength{\saveparindent}
\setlength{\saveparindent}{\parindent}

\raggedright
\setlength{\cslhangindent}{0.25in}
<div id="refs"></div>
\justify

\setlength\parindent{\saveparindent}

\newpage

# Original research articles {.unnumbered}

\noindent
This dissertation is based on the following original research articles:

1. **Enge, A.**, Abdel Rahman, R., & Skeide, M. A. (2021). A meta-analysis of fMRI studies of semantic cognition in children. *NeuroImage*, 241, 118436. <https://doi.org/10.1016/j.neuroimage.2021.118436>\
*Published under the terms of the [Creative Commons CC-BY license](https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.*

2. **Enge, A.** & Skeide, M. A. (2024). Tracking the neural correlates of learning to read with dense-sampling fMRI.\
*Unpublished manuscript available from first author.*

3. **Enge, A.**, Süß, F., & Abdel Rahman, R. (2023). Instant effects of semantic information on visual perception. *Journal of Neuroscience*, 43(26), 4896–4906. <https://doi.org/10.1523/JNEUROSCI.2038-22.2023>\
*Published under the terms of the [Creative Commons CC-BY license](https://creativecommons.org/licenses/by/4.0/).*

\noindent
Additional articles published during the dissertation period but not included in this thesis:

* Eiserbeck, A., **Enge, A.**, Rabovsky, M., & Abdel Rahman, R. (2021). Electrophysiological chronometry of graded consciousness during the attentional blink. *Cerebral Cortex*, bhab289. <https://doi.org/10.1093/cercor/bhab289>

* Aristei, S., Knoop, C. A., Lubrich, O., Nehrlich, T., **Enge, A.**, Stark, K., Sommer, W., & Abdel Rahman, R. (2022). Affect as Anaesthetic: How emotional contexts modulate the processing of counterintuitive concepts. *Language, Cognition and Neuroscience*. <https://doi.org/10.1080/23273798.2022.2085312>

* **Enge, A.**, Kapoor, S., Kieslinger, A.-S., & Skeide, M. A. (2023). A meta-analysis of mental rotation in the first years of life. *Developmental Science*, e13381. <https://doi.org/10.1111/desc.13381>

* Eiserbeck, A., **Enge, A.**, Rabovsky, M., & Abdel Rahman, R. (2024). Distrust before first sight? Examining knowledge- and appearance-based effects of trustworthiness on the visual consciousness of faces. *Consciousness and Cognition*, 117, 103629. <https://doi.org/10.1016/j.concog.2023.103629>

* Kessler, R., Enge, A., & Skeide, M. A. (2024). How EEG preprocessing shapes decoding performance. *arXiv*. <https://doi.org/10.48550/arXiv.2410.14453>

\newpage

\phantomsection
\addcontentsline{toc}{subsection}{A meta-analysis of fMRI studies of semantic cognition in children}
\includepdf[pages=-,scale=.87,pagecommand={}]{papers/enge2021.pdf}

\newpage

\phantomsection
\addcontentsline{toc}{subsection}{Tracking the neural correlates of learning to read with dense-sampling fMRI}
\includepdf[pages=-,scale=.88,pagecommand={}]{papers/enge2024.pdf}

\newpage

\phantomsection
\addcontentsline{toc}{subsection}{Instant effects of semantic information on visual perception}
\includepdf[pages=-,scale=.88,pagecommand={}]{papers/enge2023.pdf}
\includepdf[pages=-,scale=.92,pagecommand={}]{papers/enge2023_si.pdf}

\newpage

# Acknowledgements {.unnumbered}

Thank you to my PhD supervisors, Michael Skeide and Rasha Abdel Rahman, for encouraging me to pursue a doctoral degree and for your continued academic and emotional support.
Your trust and guidance throughout my academic career have been a great honor, and you have helped me to learn not only a few things about the human mind and brain, but also many things about myself.
You are great mentors and I wish you continued success and joy in your research and beyond.

Thank you to the lab members of the Research Group "Learning in Early Childhood" at the MPI CBS, especially to Anne-Sophie and Roman, and to the lab members of the Neurocognitive Psychology Lab at HU Berlin, especially to Julia, Kirsten, and Martin.
Your support and our countless chats and discussions have made my PhD journey so much easier and so much more enjoyable.
Should you decide to continue on this path, cognitive neuroscience has a great future ahead with scientists like you.

Thank you to all participants who dedicated their time and energy to participate in my research studies as well as to all collaborators, research assistants, non-scientific staff, and funding agencies who have enabled this research.

Thank you to Gesa Hartwigsen, Sebastian Markett, Rasha Abdel Rahman, XXX, and XXX for dedicating their time and energy to serve on my dissertation committee.

But most of all, thank you to my family and friends, especially to my parents and to Gabriele, Julia, Konstantin, Lisa, and Nele.
Without your love, none of this would have any meaning.

\newpage

# Selbständigkeitserklärung {.unnumbered}

\noindent Hiermit erkläre ich,

* dass keine Zusammenarbeit mit gewerblichen Promotionsberatern stattfand,

* dass ich die Dissertation auf der Grundlage der angegebenen Hilfsmittel und Hilfen selbstständig angefertigt habe,

* dass ich mich nicht anderwärts um einen Doktorgrad beworben habe bzw. einen entsprechenden Doktorgrad besitze,

* dass mir die dem angestrebten Verfahren zugrunde liegende Promotionsordnung der Lebenswissenschaftlichen Fakultät vom 05. März 2015, veröffentlicht im Amtlichen Mitteilungsblatt der Humboldt-Universität zu Berlin Nr. 12/2015 bekannt ist,

* dass die Dissertation oder Teile davon nicht bereits bei einer anderen wissenschaftlichen Einrichtung eingereicht, angenommen oder abgelehnt wurden und

* dass die Grundsätze der Humboldt-Universität zu Berlin zur Sicherung guter wissenschaftlicher Praxis eingehalten wurden.

\noindent
Alexander Enge\newline
Berlin, den 8. November 2024

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\clearpage\mbox{}\thispagestyle{empty}\clearpage
