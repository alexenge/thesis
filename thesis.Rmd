---
title             : "Learning to see meaning in visual words and objects"
shorttitle        : "Note: This page and the next should be removed"

author: 
  - name          : "Alexander Enge"

bibliography      : "references.bib"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
numbersections    : true

csl               : "apa.csl"
documentclass     : "apa6"
classoption       : "doc,10pt,twoside"
output            :
  papaja::apa6_pdf:
    latex_engine  : "lualatex"

header-includes:
  - \geometry{a4paper,margin=25mm}
  - \setcounter{tocdepth}{2}
  - \raggedbottom
  - \usepackage{ragged2e}
  - \linespread{1.15}
  - \fancyhead{}
  - \fancyheadoffset[RO,LE]{0pt}
  - \fancyhead[RO,LE]{\thepage}
  - \fancyhead[LO]{Alexander Enge}
  - \fancyhead[RE]{Learning to see meaning in visual words and objects}
  - \renewcommand{\headrulewidth}{0.4pt}
  - \usepackage{soul}
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\doublespacing}
  - \usepackage{makecell}
  - \renewcommand{\cellset}{\renewcommand{\arraystretch}{0.7}}
  - \captionsetup[table]{font={footnotesize,stretch=1.0},labelfont={bf,up},skip=5pt}
  - \captionsetup[figure]{font={footnotesize,stretch=1.0},labelfont={bf,up},skip=5pt}
  - \usepackage[all]{nowidow}
  - \usepackage[bottom]{footmisc}
  - \interfootnotelinepenalty=10000
  - \usepackage{newcomputermodern}
  - \usepackage{pdfpages}
---

```{r, setup, include=FALSE}
library("here")
library("knitr")

opts_chunk$set(
  fig.align = "center",
  fig.pos = "tbp",
  message = FALSE,
  out.width = "100%",
  warning = FALSE
)
```

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}
\begin{center}
\vspace*{25mm}
\textbf{Learning to see meaning in visual words and objects}\\
\vspace*{10mm}
\textbf{\textsc{\so{Dissertation}}}\\
\vspace*{10mm}
zur Erlangung des akademischen Grades \\
Doctor rerum naturalium (Dr. rer. nat.)\\
im Fach Psychologie\\
\vspace*{10mm}
eingereicht an der\\
Lebenswissenschaftlichen Fakultät \\
der Humboldt-Universität zu Berlin\\
\vspace*{10mm}
von\\
\textbf{M.Sc. Alexander Enge}\\
geboren am 09.11.1996 in Räckelwitz\\
\vspace*{45mm}
\end{center}
\begin{flushleft}
Präsidentin der Humboldt-Universität zu Berlin\\
Prof. Dr. Julia von Blumenthal\\
\vspace*{10mm}
Dekan der Lebenswissenschaftlichen Fakultät der Humboldt-Universität zu Berlin\\
Prof. Dr. Dr. Christian Ulrichs\\
\vspace*{10mm}
Gutachter:innen\\
1. Prof. Dr. Rasha Abdel Rahman\\
2. Prof. Dr. Gesa Hartwigsen\\
3. Prof. Dr. Gesa Schaadt\\
\vspace*{10mm}
Tag der mündlichen Prüfung: XX.XX.XXXX
\end{flushleft}

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}

# Zusammenfassung {.unlisted .unnumbered}

\newpage

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}

# Abstract {.unlisted .unnumbered}

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}
\begin{flushleft}
{\setstretch{1.0}\tableofcontents}
\end{flushleft}

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\setcounter{page}{1}

# Introduction

Cupidatat pariatur sunt ea est cupidatat consequat et do pariatur aute cillum cillum laborum. Deserunt nostrud quis aliqua irure ut culpa ullamco magna mollit dolor sit consectetur. Id deserunt in proident magna irure laborum non ipsum in tempor. Laborum labore voluptate dolor cillum sit duis eu in sint deserunt voluptate ea ipsum.
Est reprehenderit dolore fugiat adipisicing occaecat voluptate ex qui ullamco ut consequat amet quis ipsum. Lorem labore labore culpa consequat minim. Laborum est mollit esse exercitation esse eiusmod irure voluptate veniam ex adipisicing ex qui. Enim est laborum magna ex Lorem amet ea pariatur elit laborum elit ullamco exercitation. Irure veniam ut ad ut minim magna veniam do sint esse irure pariatur eu. In Lorem fugiat anim minim fugiat laboris ipsum proident.
Lorem ad nostrud excepteur voluptate pariatur nostrud mollit velit irure aliquip cupidatat eiusmod eu. Ut elit quis eiusmod dolore non exercitation veniam. Ad pariatur nulla reprehenderit eiusmod irure aliquip. Ut ullamco velit labore mollit id anim enim magna. Sit velit nulla cupidatat esse eiusmod pariatur sunt labore tempor quis laborum. Fugiat officia tempor ullamco minim nulla laborum nulla enim.

## Overview of the thesis

In this thesis, I present three empirical studies on how humans learn to see meaning in visual words and objects.
This question may seem like a very broad one and, as you will see and as is so often the case in science, I cannot claim that I have found a clear and meaningful answer.
Instead, I invite you to view these studies as three very different examples of how visual-semantic development can be studied.
In fact, the three studies are so different from one another that I will not try to pretend that they were planned to build up on or logically follow from one another, because that simply was not the case.
Instead, I have personally always been interested in how our brain makes sense of the raw visual input that it gets, and how in turn this understanding might feed back into how we see the world around us.
This has led me to pursue different research projects on different aspects of visual-semantic development throughout the course of my dissertation period.
In this thesis, I present the outcomes of these different projects in the form of the original research articles as well as an introduction and discussion in which I provide an overview of the context of the different studies and how their results may inform one another as well as future research.

The thesis is structured as follows: In this Introduction, I will start out by introducing the cognitive-neuroscientific methods that are typically used to study visual and semantic processing non-invasively in humans.
I will the briefly review common knowledge about the visual and semantic systems in the human brain as well as how the may interact with one another.
I will then introduce to specific examples of visual-semantic development, namely learning to understand written words and learning to understand visual objects.
My review of the brain's semantic system forms the basis of the first original research study included in this thesis, whereas the two examples of learning to understand written words and visual objects forms the basis of the second and third original research study, respectively.

Following the Introduction, I provide a brief summary of each of the three studies, including their theoretical background, research question, methods, and results.
Finally, in the Discussion, I will summarize the main results of this thesis and how they interact with one another.
I will also point out future research directions based on each of the different projects as well as their interaction.
I will close with some more speculative questions about if and how we can gain a deeper understanding about how the brain learns to make sense of the visual world.

Following the main text, the reader will find the reference section, a reprint of the three original papers as well as an acknowledgment of the people who have supported me while working on this dissertation.

## Cognitive-neuroscientific methods

Studying how the brain solves the task of making sense of visual information requires methods to measure brain activity while humans engage in tasks that require visual-semantic processing.
This is not straightforward: For centuries, all that philosophers and scientists had access to was the input (What does a person see?) and the output (What does the person say or how to they behave?), whereas the actual processing inside the skull used to be a black box.^[An exception to this was the *post mortem* analysis of brain lesions and their association with certain behavioral particularities.]
This changed in the last century, and especially in the last three decades, with the advent of non-invasive recording and imaging techniques, especially electroencephalography (EEG) and magnetic resonance imaging (MRI).

In EEG, electrodes are placed on the head of a human participant and connected to their scalp using an electroconductive gel.
These electrodes are then able to pick the electric fields created by the postsynaptic potentials of large neuronal populations along the cortical sheet of the brain.
These measured amplitudes, which are typically on the order of a few microvolts, are a direct measure of neural activity (plus noise from the recording environment).
Most crucially, modern EEG devices can sample these amplitudes at a frequency of 500--2000 Hz (i.e., one sample every 2 to 0.5 ms), which is as high or higher as the timing of the postsynaptic potentials themselves, allowing researchers to track cortical information processing in real time.
The two major downsides of EEG recordings are (a) that they are very noisy, therefore making it necessary to collect and average a large number of trials to extract the signal of interest from background noise, and (b) the low spatial specificity, as the voltage measured at any specific electrode is a complex mixture of signals that could come from different remote areas of the brain (the "inverse problem"), due volume conduction and different orientation of source dipoles due to the folding of the cortical sheet.
It is therefore difficult to impossible to link the EEG signal measured on the scalp to the specific neuronal population (or populations) having generated it.
Nevertheless, EEG is an excellent tool to study the temporal sequence of cognitive processing in comparatively cheap and non-invasive fashion.

In MRI, the human participant is lying in an MRI scanner with a strong static magnetic field (typically on the order of 1.5--7 Tesla) which aligns the spins of Hydrogen protons in the brain.
Then, a radio-frequency (RF) pulse is sent to push the protons into a higher level of magnetization.
Once the RF pulse is turned off, the protons fall back into their original level ("relaxation"), emitting energy that gets picked up by a metal coil around the head.
The timing of the relaxation depends on properties of the tissue, while specific additional magnetic gradients are used to encode the location of the signal being measured.
This makes it possible to reconstruct an image of the local structural or functional properties of the brain based on the signal measured from each spatial unit ("voxel") encoded via the magnetic field.
In structural MRI (sMRI), the scanning sequence is created such that it measures differences in T1 or T2 relaxation times, therefore creating an image in which the intensity of the voxels depends on their fat content, which differs between gray matter (i.e., neuronal cell bodies), white matter (i.e., neuronal fibres), and cerebro-spinal fluid (CSF), thus providing a high-resolution static anatomical image of the brain.
In functional MRI (fMRI), the scanning sequence measures the T2* relaxation time, which is sensitive to the blood oxygenation of the tissue.
Since local neuronal activity requires oxygen, active brain areas receive an increase in blood flow and blood volume, which increases the local amount of oxyhemoglobin and decreases the amount of local deoxyhemoglobin, in turn leading to an increase in the measured T2* signal.
By tracking the signal intensity at every voxel over time, that is, by reading out a whole-brain image of the T2* signal every couple of seconds, one can track the relative change in blood oxygenation (the "blood oxygen level dependent" [BOLD] effect) over time and interpret this as a measure of which brain areas are comparatively more or less active and how this activity co-varies with a certain experimental manipulation (e.g., which brain areas show an increase in BOLD activity when processing faces as compared to houses).
However, it is important to keep in mind that BOLD activity measures changes in local blood oxygenation and is therefore only an indirect proxy of neuronal activity.
Furthermore, the temporal resolution of fMRI is much worse than that of EEG, both because it typically takes approximately 2 s to sample every location in the brain and because the haemodynamic response, that is, the increase in blood flow following an increase in local neuronal activity, takes approximately a couple of seconds to unfold, even though the underlying neuronal activity happens on the order of microseconds.
The spatial resolution, on the other hand, is much better than EEG, as fMRI measures each individual voxel inside the brain and therefore does not suffer from the inverse problem.
Still, a typically voxel in fMRI is 8 to 27 mm^3^ in size and therefore contains at least multiple hundreds of thousands neurons [@shapson-coe2024], making it impossible to gain an inside to any neuronal circuit level processing.
Instead, fMRI can be used to identify which cognitive functions reliably engage which larger brain areas [think of the Brodmann areas or more modern whole-brain atlases, e.g., @destrieux2010; @glasser2016; @schaefer2018; @yeo2011] relative to some control condition.^[Note, however, that the fact that brain area *A* shows a significant change in BOLD activity while area *B* did not, this cannot be interpreted as area *A* being more active than area *B* without explicitly testing the \textit{task} \texttimes \textit{area} interaction effect [@nieuwenhuis2011].]

## The visual system

Vision is widely believed to be the most important sensory input modality in humans [@enoch2019; @winter2018a] and occupies approximately one quarter of the human brain's cortical surface, more than all other sensory modalities combined [@vanessen2003].
The visual system processes the neural signals coming from the retinae of both eyes in a hierarchical sequence of subcortical and cortical areas, starting with Lateral Geniculate Nucleus (LGN) in the Thalamus and continuing in the visual cortex in the occipital lobe of the brain [see Figure\ \ref{fig:vision}A\; @felleman1991].
There, Visual Areas 1 and 2 (V1 and V2) represent the visual field in a retinotopic fashion, with the firing rate of nearby cortical columns coding for the visual input in nearby locations in visual space.
From V1 and V2, the input is passed on to two distinct but interacting processing streams, the dorsal stream, including areas V3, V5/IT, and parts of the parietal cortex, and the ventral stream, including areas V4 and parts of the ventral and lateral occipto-temporal cortex [e.g., the visual word form area [VWFA], the parahippocampal place area [PPA], and the fusiform face area [FFA]\; see Figure\ \ref{fig:vision}B\; @cohen2000; @epstein1999; @kanwisher1997].
The dorsal stream predominantly codes for object location and object-related action guidance, whereas the the ventral stream predominantly codes for object identity and categorization [@farivar2009; @mishkin1982].

```{r, vision, fig.cap="(ref:vision-caption)"}
include_graphics(here("figures", "vision.png"))
```

(ref:vision-caption) ***Visual processing areas in the human brain.***
**A** Schematic of the most important anatomical areas involved in visual processing in humans.
The visual input from the retina is passed to the lateral geniculate nucleus (LGN) in the thalamus and then to areas V1 and V2 in the occipital lobe.
From there, the dorsal ("where/how") and ventral ("what") visual streams represent the visual input in an increasingly abstract fashion.
IPL = inferior parietal lobule,
IT = inferior temporal cortex,
MST = medial superior temporal area,
MT = middle temporal area,
SPL = superior parietal lobule.
Adapted from @yamasaki2018 under the terms of the Creative Commons Attribution (CC BY) License.
**B** Object category-selective regions in the human brain.
In the ventral and lateral occipito-temporal cortex, patches of fMRI voxels respond preferentially to certain object category, including the fusiform face area for faces, the visual word form area (VWFA) for written words, the lateral occipital area (LO) for visual objects, the parahippocampal place area (PPA) for scenes, and the extrastriate body area (EBA) for bodies and body parts.
Adapted from @behrmann2020 under the terms of the CC BY License.

Within the ventral stream, subsequent areas have increasingly larger receptive field sizes, that is, they respond to increasingly large areas of real-world visual space.
They also contain increasingly abstract representations of visual stimuli, disregarding low-level visual features such as size, luminance, or orientation, allowing the brain to classify a visual stimulus as a certain type of object and pass this information on to other neural systems for further processing and appropriate action selection.
This process of "core object recognition" [@dicarlo2012] is often described as taking place fast (within less than 200 ms) and in a largely feed-forward fashion, progressing from LGN to early visual areas (V1 and V2) to higher-level visual areas [e.g., VWFA, PPA, FFA\; @dicarlo2012; @felleman1991; @marr1982; @zhaoping2014].
However, the visual system is highly interconnected and contains not only feed-forward axonal connections from earlier to later visual areas, but also horizontal connections within visual areas and feedback connections from later to earlier visual areas [@lamme1998; @lamme2000].
These feedback connections help to improve visual perception especially under challenging conditions [e.g., occlusion and ambiguity\; @hupe1998; @williams2008a; @wyatte2014].
Furthermore, recent evidence from human behavioral, EEG, and fMRI studies suggests that even early visual processing is modulated by non-visual information such as semantic knowledge, linguistic structures, and emotions [e.g., @abdelrahman2008; @boutonnet2015; @clarke2016; @hsieh2010; @phelps2016; @slivac2021; @teufel2018].
Thus, while the understanding of bottom-up hierarchical visual processing has been one of the huge success stories of neuroscience [e.g., @hubel1962], and has successfully inspired advances in computer vision and artificial intelligence [e.g., @he2015; @krizhevsky2012], it is clear that the visual system cannot be viewed in isolation, as its representations feed into other cognitive systems (e.g., the semantic or motor system) and are in turn influenced by them [@churchland1994].

## The semantic system

Semantic processing refers to the storage, modification, and retrieval of knowledge about the world, including the meaning of spoken and written words, the significance and functions of objects, and abstract ideas or facts (e.g., that Paris is the capital of France).
Humans can reason routinely about the taxonomic belonging of concepts to categories (e.g., a mouse is a mammal) and about the similarities and dissimilarities between different concepts (e.g., badminton is like tennis but without the ball touching the ground) to a degree that other animal species or machines seem unable to.^[For non-human animals, some view the uniquely human advantage in having *language* rather than *semantics* [e.g., @friederici2017] but this seems difficult to disentangle given that the two concepts are so tightly interlinked and correlated across phylogeny and ontogeny. For machines, semantic processing that would satisfy or impress us as humans seemed impossible just a few years before I started to write this thesis. Now, however, large language models based on the transformer architecture and huge training data sets can emulate or even outperform human semantic knowledge, although the degree to which they are able to "reason" or "understand" remains a source of fierce debate [e.g., @lewis2024; @sun2024; @wu2024].]
However, it is important to note that semantic processing and related termini (e.g., "meaning", "concept") are often used somewhat vaguely and with different connotations in different scientific disciplines [e.g., philosophy, psychology, and linguistics\; but see @reilly2024].
Here I will focus on representations of semantic knowledge and their neural correlates in humans as studied in psychology and cognitive neuroscience.

```{r, semantic-theories, fig.cap="(ref:semantic-theories-caption)"}
include_graphics(here("figures", "semantic-theories.png"))
```

(ref:semantic-theories-caption) ***Theories of semantic representations.***
**A** Categorical theories assume that the sensory input is assigned to one of a set of discrete semantic objects categories or concepts.
**B** Feature-based theories assume that concepts are defined by the presence or absence of certain semantic features.
**C** Dimensional theories assume that concepts exists as points in an abstract, high-dimensional semantic space, the geometry of which needs not be humanly interpretable but still represents the semantic similarity and dissimilarity of objects. 
Adapted from @frisby2023 under the terms of the CC BY License.

Broadly speaking, there are three different types of theories about the organization of semantic information in the human mind and brain [see Figure\ \ref{fig:semantic-theories}\; @frisby2023]:

* *Categorical* theories assume that exemplars (e.g., the written word "bird" or the picture or sound of a bird) are assigned to one of a set of discrete basic-level object categories or concepts (in this case, *bird*).
The categories (e.g., *bird* versus *tree*) are typically mutually exclusive and defined by certain criteria (e.g., *has wings*) or via similarity to prototypical examples.
On this view, concepts may be represented as vectors in a high-dimensional space, where each dimension is one category.
In this space, each concept is a binary vector with a value of one on the category to which it belongs and zeros elsewhere.
Concepts are then considered similar or dissimilar if they belong to the same versus different categories.

* *Feature-based* theories assume that exemplars are recognized via the presence or absence of certain features.
For instance, the concept of a bird is defined by the presence of features such as *lives*, *has wings*, *lays eggs*, etc., and these features get activated whenever a specific instance of the concept (e.g., the written word "bird" or the picture or sound of a bird) is encountered.
Features can be binary (e.g., an animal *has wings* or not) or continuous (e.g., an animal *runs slow* versus *fast* versus *very fast* etc.), but they must be interpretable.
On this view, the high-dimensional semantic space is formed by features instead of categories, and each concept is a vector that codes the presence or absence (for binary features) or the degree to which it possesses a certain feature (for continuous features).
Concepts are then considered similar or dissimilar if they show large versus little overlap in the presence/absence or degree of semantic features.

* *Dimensional* are like feature-based theories in that they assume concepts to be points in a high-dimensional semantic space and that the concept vectors may contain multiple non-zero values, coding for the degree to which a concept concurs with a certain dimension.
Unlike for feature-based theories, however, the dimensions need not be interpretable by humans in the same way a certain binary or continuous semantic feature like *has wings* is.
Instead, the dimensions are typically learned by some computational model from a large set of input data, such as language models trained on large corpora of text from the internet or computer vision models trained on a large set of natural images.
Even though the dimensions are not interpretable, the model captures the similarity and dissimilarity of concepts based on how near versus far away their distance in the high-dimensional semantic space is.

For each type, there are many different instantiations of specific theories (e.g., classical versus prototype-based categorical theories, dimensional theories based on word embeddings versus neural networks), and multiple of these theories may be implemented in the brain and used depending on the specific goal, task context, sensory input and output modality, and cognitive resource availability.
It is important to note that the different theories often result in different experimental designs and analyses choices that are able to discover behavioral and neural effects of semantic processing [@frisby2023].
For instance, a categorical model is easily tested by running a highly controlled lab experiment that requires participants to make category judgments on visually presented words or images, and then look for differences in behavior or brain activity between different categories or objects [while ideally controlling as best as possible for differences in their low-level sensory features, e.g., @alizadeh2017; @rice2014].
Dimensional models, on the other hand, are best tested by presenting a large volume of naturalistic input, such as natural speech or movies, and then use encoding models or decoding models to test which brain responses covary with the embedding vectors of a computational model trained on the same input [e.g., @huth2012; @huth2016].

Traditionally, the neural correlates of semantic processing in humans have been studied using univariate fMRI techniques, that is, by examining the difference in BOLD activity amplitude between to or more conditions that are similar in their lower-level (e.g., sensory, phonological, orthographic) processing demands but differ in their semantic processing demands (e.g., reading words versus pseudowords).
In addition to equating low-level processing, it also important to equate task difficulty and working memory demands between the different semantic conditions [@binder2009].
Many individual studies and meta-analyses [e.g., @binder2009; @jackson2021; @rodd2015] have shown that these "semantic contrasts" reliably engage a relatively circumscribed, left-lateralized set of cortical regions (see Figure\ \ref{fig:semantic-brains}A), consisting of the inferior parietal lobe (especially the angular gyrus), lateral temporal cortex (especially the middle and inferior temporal gyrus), the ventral temporal cortex (fusiform and parahippocampal gyri), the dorso-medial prefrontal cortex, and the inferior frontal gyrus (especially the *pars orbitalis*).
Additionally, the middle fusiform cortex and anterior temporal lobe [@forseth2018; @jackson2015; @visser2010; @woolnough2020] are reliably activated by semantic contrasts but are difficult to measure with fMRI due to the low signal-to-noise ratio in this area [@embleton2010; @liu2016a].
In the first empirical research article presented in this thesis (see Section \ref{a-meta-analysis-of-fmri-studies-of-semantic-cognition-in-children}), we investigated the development of this "localized" semantic system, that is, which brain areas are reliably activated by different types of semantic fMRI tasks in children and if these are similar or dissimilar to those in adults.

```{r, semantic-brains, fig.cap="(ref:semantic-brains-caption)"}
include_graphics(here("figures", "semantic-brains.png"))
```

(ref:semantic-brains-caption) ***Semantic processing in the human brain.***
**A** Brain areas reliably activated by semantic task contrasts, based on an activation likelihood estimation (ALE) meta-analysis of 415 fMRI studies [for details, see @enge2021, and Section \ref{a-meta-analysis-of-fmri-studies-of-semantic-cognition-in-children}]. Original data provided by @jackson2021.
**B** Brain areas representing specific semantic dimensions in a naturalistic task (story listening), estimated by fitting an encoding model of high-dimensional semantic representations of the words in the story to the time series of each voxel [for details, see @huth2016]. 
Adapted from @deniz2019 under the terms of the CC BY License.

More recently, studies have emerged that moved beyond simple semantic task contrasts and tried to track semantic processing under more naturalistic circumstances, e.g., by scanning participants while viewing movies or listening to audio books or radio programmes.
One can then label the objects or words at each time point in the movie or audio stream, represent these concepts as vectors in a high-dimensional semantic space (e.g., from a word embedding or neural network model), and correlate the timeseries of these dimensions with the time series of the measured brain activity [e.g., @deniz2019; @huth2012; @huth2016].
Variants of these techniques usually show a very different picture from the univariate semantic contrasts described above, with different concepts and semantic dimensions being scattered across almost the entire cortex, including areas that have typically not been thought to be engaged in semantic processing (see Figure\ \ref{fig:semantic-brains}B).
While these different results clearly stem from the vastly different experimental designs and analysis choices [for a systematic discussion, see @frisby2023], there is as of yet not a clear path for integrating these different lines of research into an overarching theory of semantic processing in the human brain.

## Visual-semantic interfaces

The visual and semantic systems need to interact with one another in order to make sense of what we see and act accordingly.
These interactions can be bidirectional:
On the one hand, the visual system will feed its output (a high-level, feature invariant representation of the retinal input) into the semantic system, allowing us, among many other options, to name visual objects, assess their usefulness to us, or relate them to other aspects of our cognition (e.g., our memory or current emotional state).
On the other hand, the semantic system may modulate different stages of visual processing in a top-down fashion.
This can be useful to predict visual input based on prior experience [as stipulated in predictive coding theories of the brain, e.g., @clark2013; @friston2009; @rao1999] and to improve accuracy under the oftentimes challenging conditions in real-world vision [@hupe1998; @williams2008a; @wyatte2014].

These bidirectional links between vision and semantics likely exist from very early stages of brain development onwards, allowing young infants to develop their first pieces of world knowledge based on their visual input.
However, the ability to associate previously unknown visual shapes and objects with semantic information remains a crucial cognitive skill throughout our lifetime---or otherwise we would be lost whenever we try to understand any sign or gadget that we have never seen before.
In the following, I will briefly introduce two examples of learning novel visual-semantic associations, namely children's learning to read and adults' learning to understand previously unknown visual objects.
These two examples are examined more closely in the second and third empirical research articles presented in this thesis, respectively (see Sections \ref{tracking-the-neural-correlates-of-learning-to-read-with-dense-sampling-fmri} and \ref{instant-effects-of-semantic-information-on-visual-perception}).

```{r, task, fig.cap="(ref:task-caption)"}
include_graphics(here("figures", "task.png"))
```

(ref:task-caption) ***Seeing without (much) meaning.***
**A** How do these words sound and what do they mean?
**B** What can be seen in this image?
**C** Do you know what kind of object this is or what one can do with it? See Figure\ \ref{fig:solution} for solutions as well as image rights and permissions.

### Learning to see meaning in written letters

For most readers of this thesis, the black curves and lines displayed in Figure\ \ref{fig:task}A will not bear much meaning.
Your visual system will process them just fine, representing the visual input from each location in the retina into one coherent percept of black squiggles on a gray background.
You may even guess correctly that these are strings of letters, and therefore you may get some none-zero response in your visual word form area (VWFA), which is tuned to recognize written words (see Section\ \ref{the-visual-system} and Figure\ \ref{fig:vision}).
However, when asked to read these letter strings aloud, decide if they are words or non-words, or decide if they are related to one another or not, you would very likely be puzzled (unless you happen to know Hindi).
This is because most readers will have been trained to read in a different writting system, most likely the Latin alphabet, which they can use effortlessly to convert written letters into spoken language and vice versa, as you will see when looking at the English translation of the Hindi words, shown in Figure\ \ref{fig:solution}A.

When looking at the Hindi words in Figure\ \ref{fig:task}A, you of course find yourself in a similar situation as most beginning readers for their "native" script.
Most children are explicitly taught how to read by their teachers and parents starting at approximately 5--6 years of age, which is much later than when most children pick up how to comprehend and produce spoken language [for review, see @skeide2016].
Before that, children might have picked up some purely visual familiarity with letters from their own script, and might recognize some words that are especially important to them (e.g., their own name).
But by and large, strings of written letters such as the ones presented in Figure\ \ref{fig:solution}A will not be understandable for them and, while surely engaging the visual system, will not elicit any reliable activation in language areas or in the semantic system.
The process of learning to read can be thought of as fine-tuning the visual system towards the recognition of written letters [e.g., @dehaene-lambertz2018], but even more so as connecting the visual system to the language and semantic systems, such that a previously meaningless letter string can be converted into speech sound (e.g., during reading aloud) and word meaning (e.g., during lexical decision or text comprehension).
Early during this process, beginning readers will rely more heavily on converting individual letters into speech sounds and merging these speech sounds into a spoken word form, the meaning of which they can then access based on their already fully developed spoken language capacity [@ehri2005; @frith1986; @skeide2016].
After gaining more reading experience, children learn to skip the detour via this phonological/alphabetic route and become able to read in an orthographic fahsion, that is, process familiar words as a whole and access their meaning from the mental lexicon directly, without having to engage into individual grapheme--phoneme conversion.^[Note that these two "routes" of reading, that is, an indirect connection from vision via phonology to semantics and a direct route from vision to semantics, are also part of computational cognitive models of reading [e.g., @coltheart2001; @perry2007; @perry2010; @perry2013; @seidenberg1989].]
In the second study included in this thesis (see Sections \ref{tracking-the-neural-correlates-of-learning-to-read-with-dense-sampling-fmri} and the original research article), we tracked this process of visual-phonological and visual-semantic development by repeatedly measuring fMRI responses to spoken and written words in sample of children that underwent approximately 1.5 years of reading instruction.

### Learning to see meaning in visual objects

Our capacity to learn to connect previously meaningless visual information with semantic concepts luckily does not end during childhood.
Instead, we remain able to form new links between the visual and semantic systems for our entire lives, allowing us to understand what kind of object or visual seen we are seeing, a link from vision to semantics, and to see the visual word around us differently based on what we expect or know to be true, a link from semantics to vision.
As an example of this, take a look at Figure\ \ref{fig:task}B.
Can you tell what is displayed in this black and white image?
If you cannot, take a look at Figure\ \ref{fig:solution}B, which shows the original image from which the black and white image was created [using a binarization technique\; see @mooney1957; @reining2024].
Once you return to the black and white image, you most likely cannot "unsee" the central object of the image anymore---your prior experience and knowledge of what is displayed has changed your perception, even though the visual input is the same as before [see @samaha2018 and @teufel2018 for formal studies of this phenomenon using the same kind of stimuli].

Another example of such feedback effects from semantic cognition to visual perception, and one that might bear more real-world applicability than the previous one, is the processing of visual objects and tools based on their function.
We learn the names and functions of most visual objects as children but, in our ever more technology-driven world, are also frequently presented with tools and gadgets that we might have never seen before^[You may want to pause for a second to reflect on when you first saw a fidget spinner, Theragun, or smart watch].
Learning what these objects are for or how to use them often makes them appear in a different light:
We may get a kind of "aha" experience from understand what we see and this may change how we perceive the objects and its visual features.
For example, most people will not have encountered the object in Figure\ \ref{fig:task}C at any point in their life, and many people will not have an intuition what this object is for or how to use it.
However, after being informed about the function of the object (e.g., by taking a look at Figure\ \ref{fig:solution}C), some of the visual features of the object and their configuration start to make sense.
I would argue that, as in the previous example of the black and white image, we cannot "unsee" the meaning (i.e., function or usage) of this objects after we have discovered it.
That is, learning the semantic meaning of a previously unfamiliar object might change our visual perception of the object, both while we are learning it (i.e., the "aha" experience) and after we have learnt it (i.e., when we re-encounter the visual object later on).
In the third studies included in this thesis (see Sections \ref{instant-effects-of-semantic-information-on-visual-perception} and the original research article), we tested this idea empirically by presenting human adults with previously unfamiliar objects (as in the example above) and measuring their EEG response to the object before, during, and after they learnt about the their functional meaning.

```{r, solution, fig.cap="(ref:solution-caption)"}
include_graphics(here("figures", "solution.png"))
```

(ref:solution-caption) ***Seeing with meaning.***
**A** English translation of the Hindi/Devanagari words displayed in Figure\ \ref{fig:task}A.
**B** Original image from which the binarized "Mooney" image [see @mooney1957] in Figure\ \ref{fig:task}B was generated.
The binary image was adapted from @reining2024 under the terms of the CC BY License.
The original image is from the THINGS database [@hebart2019] under the terms of the CC BY License.
**C** Functional description of the object displayed in Figure\ \ref{fig:task}C, which was adapted with permission from Tim Daniels, <https://poultrykeeper.com>.

\newpage

# Summary of the present studies

## A meta-analysis of fMRI studies of semantic cognition in children

In this first^[Note that order in which the studies are presented in this thesis was based on their content and logical flow, not on their chronological order of publication.] study [@enge2021], we provide a quantitative synthesis of the brain areas involved during semantic processing children.
To this end, we conducted a meta-analysis of all available fMRI studies in children that probed different aspects of semantic cognition, including semantic knowledge (e.g., naming an object after hearing a verbal description), semantic relations (e.g., deciding if pairs of words belong together or not), and visual object categorization (e.g., viewing object images from different semantic categories).

Meta-analyses are a useful tool to synthesize the research literature within a relatively narrow domain [@glass1976; @hedges1985; @harrer2021].
The surfacing of the latest replicability crisis in psychological science [@opensciencecollaboration2015] was a reminder that the findings from any individual research paper cannot be accepted as true and irrevocable facts, due to problems such as small sample sizes, publication bias (the "file drawer effect," i.e., the low publication rate of negative findings), *post hoc* theorizing, and questionable research practices [@ioannidis2005; @kerr1998; @simmons2011].
To get a slightly more reliable and balanced view of the research outcomes in a certain area, meta-analysis empirically integrate the findings from many---ideally all---studies on a given topic.
By doing so, they can (a) show which effects reported in the original papers are likely to be robust and replicable, thereby reducing the number of false positives, and (b) identify novel effects that individual original papers might not have the statistical power to detect, thereby reducing the number of false negatives [@schmidt2015].

In the fMRI literature, the evidential status of the results of individual papers is especially doubtful.
This is because compared to behavioral studies and neuroscientific studies using "cheaper" methods (e.g., EEG), data collection is at least an order of magnitude more costly and depends on the availability of specialized hardware.
At the same time, data preprocessing and analysis methods are highly complex, which introduces many potential sources of error and researcher degrees of freedom.
Within the fMRI literature, developmental studies are especially prone to these problems because it is more difficult to recruit children (as compared to, e.g., Psychology undergraduates in need of course credit or monetary compensation) and because children tend to show lower levels of compliance and higher levels of head motion, which reduces the amount of data that can be collected as well as its quality.

Our goal was therefore to identify and meta-analyze all available fMRI studies on semantic cognition in children, building up on previous fMRI meta-analyses on semantic cognition in adults [@binder2009; @jackson2021; @rodd2015; @vigneau2006; @visser2010] as well as our own previous fMRI meta-analysis on language processing in children [@enge2020].
Using three online databases, we identified approximately 1,000 articles based on our keyword search, 45 of which fulfilled all of our inclusion criteria.
In these 45 articles, there were 50 fMRI experiments on semantic cognition in children, which we further classified into semantic knowledge experiments (21 experiments), semantic relatedness experiments (16 experiments), and visual object categorization experiments (13 experiments).
In total, these experiments included data from 1,018 children with a mean age of 10.1 years (range = 4--15 years).

The experiments reported 687 peak coordinates from statistically significant clusters associated with semantic cognition in children.
Using the activation likelihood estimate (ALE) algorithm [@eickhoff2009; @eickhoff2012; @turkeltaub2002] implemented in the NiMARE software package [@salo2023], these peaks were convolved with a Gaussian smoothing function, integrated into a meta-analytic map, and thresholded using a permutation-based family-wise error correction for multiple comparisons at the whole-brain level [@eickhoff2012; @eickhoff2016].
For general semantic cognition, combined across all three task types, there was reliable BOLD activity across experiments in eight clusters, namely in the left inferior frontal gyrus (pars triangularis; two clusters), the bilateral supplementary motor area, the left fusiform gyrus, the right insula, the left middle temporal gyrus, the right inferior occipital gyrus, and the right fusiform gyrus.
Broken down by task category, three clusters (left inferior frontal gyrus, bilateral supplementary motor area, and right insula) were activated for both semantic world knowledge experiments and semantic relatedness experiments, whereas the left middle temporal gyrus was only activated for semantic relatedness experiments and the left and right fusiform and occipital clusters were only activated for semantic object categorization experiments.^[However, note that this *difference in statistical significance* does not automatically imply a *statistically significant difference* [@gelman2006; @nieuwenhuis2011]. For a formal statistical comparison of the different task types, see Figure\ 6 and Table\ 4 in the original paper.]

We validated our results from the ALE analysis using a second meta-analytic approach [seed-based *d* mapping\; @albajes-eizagirre2019; @albajes-eizagirre2019a], which gave qualitatively similar results and also allowed us to control for various experiment-level covariates (e.g., language, sensory modality, response modality).

We tested for age related effects both within our meta-analytic sample of children's experiments and by comparing our results in children with those from a previous meta-analysis on semantic cognition in adults [@jackson2021].
Regarding age-related changes during childhood, we found one cluster in the right insula that showed significantly more reliable activation in older as compared to younger children [but note the limited statistical power of this analysis as well the statistical problems associated with median split analyses\; e.g., @irwin2003; @mcclelland2015].
Regarding the comparison to adults, there was very large overlap across left and right inferior frontal, supplementary motor, and left temporal regions, but also significantly less reliable activation in children in the left anterior temporal lobe and significantly more reliable activation in fusiform and occipital regions, especially in the right hemisphere.^[But note that this latter effect may be due to differences in the relative number of linguistic versus visal object categorization experiments included in the two meta-analyses.].

Finally, we used to previously established empirical techniques to examine the robustness of our meta-analytic results against spurious effects and publication bias in the original literature [leave-one-out and fail-safe $N$\; @acar2018; @enge2020; @samartsidis2020].
These analyses showed that all of our clusters from the main analysis and almost all clusters from the sub-analyses of task types were reliable even if there were spurious original experiments or strong publication bias.

Taken together, our meta-analysis is the first of fMRI experiments on semantic cognition in children, and showed strong evidence for a reliably activated semantic network that is remarkably similar to those reported in the adult fMRI literature [for meta-analysis, see, e.g., @binder2009; @jackson2021].
We found that experiments with linguistic materials (semantic world knowledge and relatedness judgments) reliably activated the same regions in the left inferior frontal and bilateral premotor cortices, whereas visual object categorization experiments activated a distinct set of regions in posterior brain areas (bilateral fusiform and right occipital cortices).^[Potentially casting doubt on our decision to lump the latter type of experiments together with the former into one meta-analysis, as visual object category processing may be based purely on differences in lower-level visual features and may not necessarily engage higher-level semantic processing].
Despite the large overlap between our meta-analytic results and previous meta-analytic results in adults [@jackson2021], there were a few reliable differences.
The most striking of these was that there was significantly less reliable activation in children in the anterior part of the left temporal lobe, an area associated with very high-level and amodal semantic processing in adults [e.g., @lambonralph2017; @patterson2007].
This difference may point to the fact that this semantic "hub" may not yet be fully developed in children at the typical age included in our meta-analysis (mostly 8--12 years; see Figure~3 in the original paper).

We have shared the data and analysis code of our meta-analysis on the Open Science Framework (<https://osf.io/34ry2>) and on GitHub (<https://github.com/SkeideLab/meta_semantics>) so that they can be scrutinized and reused.^[At the time of writing, three new meta-analyses on completely different topics have acknowledged reusing our scripts [@bortolini2024; @cui2022; @yang2024].]

## Tracking the neural correlates of learning to read with dense-sampling fMRI

In this second study [@enge2024], we focused on children's learning to read as one case example for the human brain learning to associate visual shapes with semantic (and phonological) information.
Learning to read is a major step in most people's cognitive development and is of great importance for the rest of our lives (for instance, allowing you to consume this PhD thesis).
Given that from a phylogenetic point of view, reading is a fairly recent phenomenon, it is not at all obvious that most human beings alive nowadays at some point in their lives become able to absorb and transmit semantic information via small and somewhat arbitrary black symbols printed on the page of a book or rendered on a digital screen.
Yet, this is exactly what happened over the course of the last few millennia, rapidly picking up over the past ~500 years, in turn fueling many other great achievements of humankind, including science and technology.

Unlike reading and writing, spoken language has been part of human culture and everyday life for hundreds of thousands of years, which has left enough time to evolve dedicated brain hardware and neural mechanisms for spoken language comprehension and production [for review, see, e.g., @fedorenko2024; @ferstl2008; @friederici2017; @hagoort2016; @hickok2007].
It is therefore unsurprising that human babies pick up spoken language comprehension quickly and relatively effortlessly within typically less than 2 years of age [for review, see @friederici2006; @skeide2016], with certain aspects of speech processing are already functioning *in utero* [@ghio2021].
Reading, on the other hand, requires years of explicit instruction and is typically only learned during the kindergarten and primary school age (typically 5--8 years).
During this process of learning to read, a child's brain needs to become able to (a) reliably recognize letters and strings of letters in one's own writing system and distinguish them from other visual kinds of visual object categories, (b) become able to link individual letters to their corresponding speech sounds, to enable decoding of written words, and, as the child becomes more proficient, (c) learn to access the meaning of an entire word from its visual word form [@frith1986; @ehri2005].
While the first aspect (visual word form recognition) has received much attention in the cognitive neuroscience of reading and reading development [e.g., @cohen2000; @dehaene-lambertz2018; @dehaene2011], the latter two aspects (linking visual words to spoken language and linking visual words to word meaning) have received much less attention [but see, e.g., @vin2024].

Studying the neural correlates of learning to read requires to take repeated measurements of brain activity (e.g., using EEG or fMRI) while children are participating in reading instruction.
Doing so is often difficult for a number of reasons, including the fact that reading instructed is typically confounded with other aspects of schooling and general cognitive development,the difficulty of recruiting children and their families at that age and for a prolonged period of time, the difficulty of collecting sufficiently large quantities of high-quality data (e.g., due to children's lower attention span and increased head motion during MRI scanning), and a lack of readily available statistical models and toolboxes for longitudinal analysis of brain data.
Additionally, available longitudinal studies of the neural correlates of learning to read [for review, see @chyl2021a]. have exclusively been carried out with children from the Global North, which limits their generalizability to other cultural and socio-economic backgrounds as well as to different, non-alphabetic writing systems.

Our goal was to overcome these limitations and investigate the change in children's brain responses to written and spoken words as they are learning to read, and to do so in a cultural setting and writing system that has typically been neglected by developmental cognitive neuroscience.
We expected that learning to read would cause an increase in brain activity in brain areas known to be involved in written and spoken word processing (e.g., the visual word form area).
We also expected that the multi-voxel response patterns in audio-visual processing areas (e.g., the left posterior temporal sulcus; pSTS) would become more similar between written and spoken words as the brain learns to connect the former to the latter.
Finally, we expected that responses to written words would become more similar to each other, that is, more stable from one scanning session to the next, as the brain becomes more finely tuned to visual letters.

We acquired fMRI from two groups of children in the region of Uttar Pradesh, India, which had no access to public schooling and received approximately 1.5 years of reading instruction (intervention group) or math instruction (active control group) as part of our study.
For the purpose of the present paper, only the data from the reading instruction group was analyzed.
The participants completed up to 6 experimental sessions, spaced at irregular intervals of approximately 2--3 months.
During these sessions, we acquired a structural MRI scan, a functional MRI (fMRI) scan, and behavioral data on standardized tests of reading skills, maths skills, working memory, general cognitive ability.
In the fMRI scan, children were presented with short blocks of written words, written pseudowords, and written low-level control stimuli (false fonts), as well as spoken words, spoken pseudowords, and spoken low-level control stimuli (noise-vocoded speech).
We analyzed these longitudinal data using linear mixed-effects models to test for linear and non-linear changes in behavioral test scores, BOLD activity amplitude, multi-voxel audio-visual pattern similarity, and multi-voxel within-condition pattern stability.
For this purpose, we created a novel implementation of whole-brain linear mixed-effects model using the high-performance Julia language [@bezanson2017], which we hope may be re-used for future longitudinal fMRI [for previous implementations of whole-brain linear mixed-effects models using the somewhat slower R language, see @chen2013a; @madhyastha2018].

In the behavior data, we found a statistically significant improvement in test scores over the course of the study for most subtests of reading skills, including word and pseudoword reading accuracy, phoneme replacement, semantic fluency, reading comprehension, and dictation.
Additionally, there also was a statistically significant improvement in two subtests of maths skills and in one subtest of working memory (backward digit span).
Few subtests of reading skills (picture naming, verbal fluency, and word, pseudoword, and passage reading time) showed no significant change, as did other subtests of working memory (Corsi block test forward and backward, backward digit span) and the test for general cognitive ability (Raven's Progressive Colored Matrices).

In the analysis of whole-brain BOLD activity amplitudes, we found robust sensory activation in the auditory cortex (bilateral superior temporal gyri and sulci) for all auditory conditions (spoken words, pseudowords, and low-level controls) and in the visual cortex (from early visual cortex to the ventral occipto-temporal cortex) for all visual conditions (written words, pseudowords, and low-level controls).
However, there were only very few and small clusters that showed a significant change (linear or non-linear) in BOLD activity amplitude over the course of the study.
There were some non-linear changes for spoken stimuli in the auditory cortex and for written stimuli in the visual cortex but their shape did not follow a pattern that we had predicted.^[We had predicted a linear increase of BOLD activity or a non-linear, negative quadratic pattern (i.e., an inverted "u" shape), based on a previous study [@dehaene-lambertz2018] and on the expansion and renormalization model of brain plasticity [@wenger2017]. What we observed in most clusters was a positive quadratic pattern (i.e., a "u" shape), which seems difficult to interpret in the context of brain plasticity and skill development.]
For the contrast of written words versus written low-level controls, there was one cluster at the ventral posterior cingulate cortex that showed an increase in BOLD activity over the first ~6 months of the study, followed by a decline over the remaining ~10 months, as we had predicted.
However, this cluster was very small, the individual BOLD activity curves of individual participants in this cluster were very variable, and this anatomical location has typically not been considered as part of the visual word form recognition or language networks, therefore casting doubt on the reliability and meaningfulness of this finding.

In the analysis of multi-voxel audio-visual pattern similarity, we found that over the course of the study, BOLD activity patterns in the left ventral occipito-temporal (vOT) cortex became more similar over time for the pair of contrasts of spoken words versus low-level controls and written words versus low-level controls.
This was in line with our hypotheses and may indicate that over the course of learning to read, the brain becomes able to access phonological and semantic information from written words that it had previously only been able to access from spoken words.
There was no significant change in audio-visual pattern similarity for any of the other pairs of contrasts and regions of interests.

In the analysis of multi-voxel within-condition pattern stability, we found that over the course of the study, BOLD activity patterns became less stable (i.e., self-similar) for auditory pseudowords and words in the bilateral posterior superior temporal sulcus (pSTS).
This was not in line with our *a priori* hypotheses, which predicted an *increase* in pattern stability specifically for written words and pseudowords.

Taken together, our study provides only weak evidence for the idea that learning to read increases BOLD activity amplitude and audio-visual processing in visual and language-related brain areas.
While we did find some evidence for longitudinal change in BOLD activity amplitudes, these changes typically did not follow a theoretically predicted pattern of linear or non-linear growth and did typically not occur in areas associated with reading or audio-visual processing.
However, we did find some evidence for an increase in audio-visual processing activity in the left vOT cortex.
There was no reliable change in pattern stability, that is, we did not find that reading leads to more stable (i.e., self-similar) written word representations.

It is important to point out that these weak results may at least in part be driven by methodological shortcomings of our study, including a very small sample size [15 children with 2--6 sessions each\; see @button2013; @ioannidis2005], an intervention that might not have been long enough to capture the entire process of becoming a sufficiently fluent reader (especially in the relatively complex Devanagari writing system), and an fMRI block design that was not suitable for multivariate analysis at the single item level, thereby preventing us from comparing individual word representations.
We nevertheless hope that some of our findings can be replicated and that our general study design and longitudinal statistical analysis approach inspires future longitudinal work in different cultures and writing systems.

## Instant effects of semantic information on visual perception

In this third and final study [@enge2023a], we switched our focus from the development of visual-semantic processing during childhood to the learning visual-semantic information in adults.
Although most of our learning of the meaning of visual objects takes place during infancy (e.g., by learning to differentiate high-level visual object categories) and childhood (e.g., by learning to associate abstract visual shapes with meaning during learning to read), this capacity luckily remains available as we age---or otherwise we would not be able to understand much at all in a world where we are presented with novel gadgets and icons almost on a daily basis.

Our goal was to capture the moment during which we learn to understand the function of a previously unknown visual object, as well as to test if this understanding changes the way in which we perceive the object visually.
This latter question has sparked endless debate among cognitive scientists.
Some argue that perception and cognition are two distinct processing stages and that perception takes place in feed-forward, modular fashion, and then passes on its outputs to other higher-level systems (e.g., those processing semantic information or emotions) but cannot itself be influenced by them [e.g., @dicarlo2012; @firestone2016; @fodor1983; @fodor1984; @machery2015; @pylyshyn1999].
Others argue that this distinction between perception and cognition cannot be maintained and/or that cognition interacts with perception in a top-down fashion from the earliest stages of sensory processing on [e.g., @ahissar2004; @churchland1994; @clark2013; @friston2009; @lupyan2020; @thierry2016; @vonhelmholtz1867; @yuille2006].
Recent empirical evidence by and large supports the latter view, as it has been shown that emotional, linguistic, and semantic information can change the response to visually identical stimuli in terms of behavioral measures [e.g., @gauthier2003; @phelps2016; @slivac2021], the BOLD response in visual cortex as measured with fMRI [e.g., @clarke2016; @hsieh2010], and early ERP components in the EEG [e.g., abdelrahman2008; @boutonnet2015; @samaha2018].
The latter type of evidence seems especially informative, as the high temporal resolution of the EEG (on the order of microseconds, directly capturing cortical postsynaptic potentials) allows to test how early during processing one can detect high-level (e.g., linguistic or semantic) influences [see @athanasopoulos2020 for an extended version of this argument].
For instance, our research group and others have shown that having participants learn semantic information about previously unknown visual stimuli elicits not only differences in relatively late, semantic ERP components (e.g., the N400 component), but also in early ERP components typically associated with visual perception [e.g., the P1 component\; @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019].
However, all of these previous studies have used an extensive training phase in which participants learned to associate the visual stimuli with the semantic information, whereas the EEG was only measured and analyzed after this learning had taken place.

In our study, we wanted to test if learning semantic information about previously unknown visual objects affects visual perception instantly, that is, directly as this information is being acquired and the function of the object is being understood.
To this end, we presented participants with images of existing but rare visual objects (e.g., a galvanometer) in three separate phase: First, without any information, to probe if the objects were indeed unknown to participants, second, with a brief verbal description that allowed participants to understand what kind of object they were seeing, and third, again without any information, to test for downstream effects of the understanding acquired in the second phase.
Crucially, in the second phase, we presented half of the objects with the correct, matching verbal descriptions, which typically allowed participants to form a correct understanding of the object, and the other half of the object with incorrect, non-matching verbal description, which typically precluded such an understanding.^[Note that we used participants' behavioral reports to verify that this manipulation worked as intended, and, on a participant-by-participant basis, we only included those objects in the analysis for which it did.]
This allowed us to compare responses to the same visual objects with and without a semantic understanding, and to do so before, while, and after this understanding had happened.

During all three phases of the experiment, we measured the EEG and analyzed three different ERP components, namely the P1 component (100--150 ms after stimulus onset) as a marker of early visual perception, the N170 component (150--200 ms) as a marker of high-level visual perception, and the N400 component (400--700 ms) as a marker of semantic processing.
In the first phase, we observed no reliable differences in any of the three ERP components between objects that would subsequently show semantically informed perception as compared to semantically uninformed perception.
This was expected given that the relevant semantic information had not yet been presented, so that all objects were unfamiliar and not understood by the participants.
In the second phase, when the objects were presented with the semantic information (either a matching description, inducing semantically informed perception, or a non-matching description, keeping the perception semantically uninformed), we did observe reliable difference in two of the three ERP components.
That is, semantically informed perception caused significantly larger (i.e., more negative) ERP amplitudes in the N170 component and significantly reduced (i.e., less negative) ERP amplitudes in the N400 component.
There was no effect of semantically informed perception in the P1 component.
In the third phase, when objects were presented once more without any information, semantically informed perception caused significantly larger (i.e., more positive) ERP amplitudes in the P1 component and, again, significantly reduced (i.e., less negative) ERP amplitudes in the N400 component.

We interpret these ERP effects as evidence that semantic information not only affects late, post-perceptual stages of visual object processing (i.e., the N400 component), but also earlier, perceptual processing (i.e., the P1 and N170 components).
This conceptually replicates previous studies from our lab and others [e.g., @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019] and provides evidence for an interactive bottom-up and top-down interplay between perception and cognition [e.g., @clark2013; @lupyan2015].
Crucially, however, our study is the first to show that these top-down effects of semantics on perception do not require an extensive learning history to develop.
Instead, they can be observed on the very same trial as the visual object is first associated with its semantic meaning (for the N170 component) as well as on the very next time when the object is re-encountered, even without the semantic information being present (for the P1 component).
Previous research, with separate learning and EEG phases, has typically only reported the P1 effect [e.g., @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019], probably the N170 is short-lived and tied to the specific moment of semantic "insight" associated with understanding the function of a previously unknown visual object for the first time.

We also conducted an additional time-frequency analysis of the same data to check for any effects of semantic information on event-related power in different frequency bands.
Unlike ERPs, these changes in event-related power do not need to be tightly phase-locked and time-locked to stimulus onset, which seems plausible given that participants might have taken different amounts of time to understand the functions of different objects.
Since we did not have any *a prior* hypothesis about the specific time window, frequency range, and EEG channels at which semantically informed perception would affect event-related power, we used mass-univariate, cluster-based permutation tests [@maris2007; @sassenhagen2019] to test for any differences in exploratory fashion.
We found no significant effects in the first phase, before any semantic information was presented (as would be expected), but one significant cluster each in the second and third phase.
In both cases, there was a statistically significant reduction of event-related power relatively late after stimulus onset (from approximately 600 ms onwards) in the alpha and lower beta frequency bands (approximately 8--20 Hz).
Although we did not hypothesize this specific finding, and therefore would like to see it replicated in future studies, it is consistent with previous findings that reduced alpha/beta power facilitates visual-semantic memory formation [@griffiths2019; @hanslmayr2009; @hanslmayr2012], presumably because this oscillatory activity acts as noise that can hamper the encoding of stimulus-specific information.

Taken together, our study shows that adults are quickly able to associate novel semantic information with previously unfamiliar visual objects, and that this newly acquired semantic information can affect early stages of stimulus processing, typically associated with visual perception itself.
Importantly, this top-down effect of semantics on visual perception can be observed immediately as the understanding of being acquired and manifests itself in enlarged ERP amplitudes of ERP components associated with lower-level visual perception (P1 component) and higher-level visual perception (N170 component), as well as in reduced ERP amplitudes in an ERP component associated with semantic precessing demands (N400 components) and reduced alpha/beta band power.
This speaks against a modular view of visual perception being encapsulated from higher-level cognitive functions [e.g., @firestone2016; @fodor1983] and favors theories of vision as an interactive, context-sensitive, and prediction-driven process [e.g., @ahissar2004; @friston2009; @lupyan2020].

### Bonus: The `hu-neuro-pipeline` package

In this section, I would like to introduce one further research output that, although it is not (yet) captured in a written publication, has already had some impact in my own research bubble and that has definitely benefitted my own personal development.
For the EEG analyses presented in the @enge2023a paper, I wrote my custom EEG analysis pipeline, based on a previously published pipeline used in our lab [see @fromer2018 and their code and data published at <https://osf.io/hdxvb>].
The rationale of this pipeline was to carry out a number of standard EEG preprocessing steps, re-referencing, correction of eye artifacts, frequency-domain filtering, epoching, and the rejection of bad epochs based on amplitude thresholds.
Unlike in traditional EEG processing workflows [e.g., @luck2014a], the preprocessed epochs are than *not* averaged into evoked potentials, which would be suitable for statistical analyses using traditional repeated-measures tests from the general linear model (GLM) family, such as a paired $t$-test or a repeated-measures analysis of variance (rmANOVA).
Instead, the EEG data remain at the single trial level and are entered directly into a linear mixed-effects model (LMM), which can adequately handle the repeated-measures structure of the data (with trials nested in participants and/or stimuli) via an appropriately specified random effects structure [@burki2018; @fromer2018; @kretzschmar2023; @volpert-esmond2021].
Compared to averaging and using a GLM, modeling the single trial data with LMMs provides a number of advantages, including:

* the ability to include both participants *and* stimuli as random effects, which is necessary to maintain adequate false-positive error control [@burki2018; @judd2012] and to allow for inference from the specific sample of stimuli to the larger population from which they were drawn [@clark1973; @yarkoni2020],

* the ability to include trial- and stimulus-level covariates [e.g., stimulus characteristics, fatigue, drift\; @volpert-esmond2021],

* the ability to include not just categorical predictors (e.g., factorial manipulations), but also continuous predictors [e.g., stimulus ratings, parametric manipulations\; @brown2021; @fromer2018],

* the ability to include person-level predictors (e.g., age, gender, test scores), and

* the ability to handle unbalanced designs, that is, an uneven number of trials per participant and conditions, which is inevitable in some experimental designs [e.g., @enge2023a; @frober2017] as well as when conditions and/or participants differ in their number of epochs after artifact rejection (which will be almost always the case).

The @fromer2018 pipeline implemented this procedure of standard EEG preprocessing plus single trial LMMs as a collection of scripts in the MATLAB language (The MathWorks Inc., Natick, Massachusetts) using the EEGLAB toolbox [@delorme2004].
However, a few disadvantages of this implementations are that (a) MATLAB is a commercial software, which costs up to $1000 per user and is therefore not readily usable outside of well-funded research institutions, (b) psychologists are typically not trained in using the MATLAB language, and (c) the code for the pipeline was not structured, documented, and version-controlled following current best practices in research software development [e.g., @barker2022; @scheliga2019].
In the Python re-implementation which I created for the @enge2023a paper and subsequently published as a standalone Python package (available at <https://github.com/alexenge/hu-neuro-pipeline>), I tried to overcome these limitations and make some further improvements by:

(1) using a widely used open-source (Python) instead of proprietary (MATLAB) software language,

(2) building the pipeline on top of MNE-Python [@gramfort2013], a state-of-the-art M/EEG analysis toolbox with larger developer and user base and following best practices from professional software development,

(3) creating an R interface, so that psychologists without prior training in Python or MATLAB can use the pipeline out of the box,

(4) adding support for time-frequency analysis (including single trial analysis and cluster-based permutation tests),

(5) releasing the code as a package (using the Python Package Index, see <https://pypi.org/project/hu-neuro-pipeline>), instead of as a collection scripts, so that it can be installed and imported more easily,

(5) releasing the code under version control, so that users can examine and install older versions of the package,

(6) adding a publicly accessible documentation website (see <https://hu-neuro-pipeline.readthedocs.io>) that explains the installation, usage, and processing details of the pipeline, both for Python and R users, and

(7) creating publicly accessible course materials on the pipeline package itself (see <https://github.com/alexenge/hu-neuro-pipeline-workshop>), time frequency analysis (see <https://github.com/alexenge/tfr-workshop>), and EEG analysis in general (see <https://alexenge.github.io/intro-to-eeg>).

The pipeline is designed with a user-interface that consists of one high-level function, `pipeline.group_pipeline`, that can be used to process the raw data from an EEG group study up to the point where the data are ready for single trial LMM modeling.
More precisely, it reads the raw data from each participant, optionally resamples it to a lower sampling rate, optionally interpolates any bad channels (which can also be automatically detected), re-references the data (per default to an average reference), optionally performs eye artifact correction (using the semi-automatic multiple source eye correction procedure or fully automatic independent component analysis [ICA]), filters the data in the frequency domain (per default between 0.1 and 40 Hz), segments the continuous data into epochs based on event markers, reads any accompanying behavioral-experimental log files and matches them to the corresponding EEG epochs, rejects "bad" epochs based on a peak-to-peak amplitude threshold, and computes the single trial amplitudes for any EEG components of interest by averaging across their *a priori* defined time windows and regions of interest.
At the group level, the pipeline combines the single trial amplitudes from all participants into one large data frame that can be used as-is for LMM modeling, e.g., using the `lmer` function from the `lme4` package in R [@bates2015].
Additionally, the pipeline also outputs evoked potentials (i.e., averaged waveforms for all participants, experimental conditions, and channels) that can be used for visualization or to compare the statistical results from LMMs and $t$-tests/rmANOVAs, as well as a metadata file that can be referred to when wanting to check the parameters and software versions that were used when running the pipeline.

In sum, the `hu-neuro-pipeline` package provides a relatively easy to use and state of the art EEG analysis pipeline that can be used for LMM analysis of single trial event-related potentials and time-frequency analysis.
Any questions, problems, or improvements to the package can be suggested via the issue tracking system on GitHub (<https://github.com/alexenge/hu-neuro-pipeline/issues>). 

\newpage

# General discussion

In this thesis, I presented three empirical articles on the development of semantic cognition and its connections to the visual system.
In the first study, we conducted a meta-analysis of fMRI studies to identify the brain areas involved in semantic cognition in children and their similarity or dissimilarity to the adult semantic system.
In the second study, we tested how learning to read during childhood, that is, learning to associate previously meaningless visual shapes with phonological and semantic information, changes amplitudes and patterns of brain activity in response to written and spoken words.
In the third study, we tested if learning semantic information about previously unfamiliar visual objects changes early perception-related brain activity in response to these objects in adults.
As mentioned in the introduction, these three studies had not been designed to strictly build up on each other and they are rather diverse in terms of their participant population, study design, and methods used to probe brain activity.
Nevertheless, in this discussion I will try briefly to lay out some connections between our main findings and how they may or may not inform future research as wall as everyday life applications.

## Visual-semantic procesing in reading and object recognition

The interactions between the visual system and the semantic system of the human brain are clearly experience dependent:
From the moment we are born, we are bombarded with visual experiences, but it is only over time that we learn to understand what they mean.
We learn which faces are important to remember and what to expect from them, what can be done with different kinds of visual objects, and, eventually, how to extract meaning from written letters and other visual symbols.
The visual system itself of course also develops further after birth [for review, see, e.g., @murphy2024; @siu2018], and part of this may be driven by non-visual influences [e.g., from language and semantics\; see, e.g., @clarke2016; @dehaene2015; @lupyan2020].
However, given that our semantics is much more dependent on our local environment, language, and culture, and is much more different from our evoultionary ancestors and cousins, the semantic system and the visual-semantic connections are likely to develop a lot more radically over the course of human development.
Given this hypothesis, it was surprising to find very strong convergence between the adult semantic system and the semantic system in children in our meta-analysis of fMRI studies [see Section\ \ref{a-meta-analysis-of-fmri-studies-of-semantic-cognition-in-children} and @enge2021].
We found that virtually all regions active during semantic processing in adults are already active in children, except potentially the more anterior parts of the left temporal lobe (ATL).
The ATL  is often viewed as an amodal semantic "hub" that dynamically integrates the specific, modal information from the sensory and motor systems [e.g., @chiou2019; @lambonralph2017; @patterson2007].
The fact that its activity level does not yet seem to be fully developed in the groups of children included in our meta-analysis sugests that this region, unlike other parts of the semantic network, develops relatively late and might continue to do so throughout adolescence.

However, one should also be cautios not to over-interpret any group difference or lack thereof in our meta-analysis, for a number of reasons.
First, the tasks used to assess semantic processing in adults and children often differ from each other in systematic ways [e.g., it is more typically to use tasks with written rather than spoken materials in adults as compared to children\; for an extended discussion of semantic task effects in meta-analyses, see @binder2009].
These systematically different tasks might then lead to systematically different results at the meta-analytic level.
Second, the activation likelihood estimation (ALE) procedure used in our and most other MRI-based meta-analyses only uses the *location* of reported peaks of fMRI activation from the original studies, not their *activation strength* (i.e., effect size).
The meta-analytic maps than show where there is statistically significant overlap of reported fMRI activations regardless of their strength, meaning that a very weakly but consistently activated region might show up in the meta-analysis, whereas a strongly active region witha more variable location across studies might not.
Any group differences between children and adults therefore reflect group differences in consistency or reliability of activation, not necessarily of activation strength.
Third, regions may differ in their internal processing (e.g., which algorithms are run or how quickly information is processed) in ways that cannot be picked up using fMRI, which means that they will be missed by any individual fMRI study as well as by our meta-analysis.
Finally, the studies included in our meta-analysis mostly tested older children and young adolescents (grand mean age = 10.1 years, range of mean ages = 5.5--12.8 years, total age range 4--15 years).
This may be the case because older children are easier to recruit and more compliant inside the MRI scanner, therefore making it easier to obtain sufficiently high-quality data.
Once more fMRI studies on semantic processing in younger children become available, it would be worthwile to extend our meta-analysis to check if the semantic system in younger children differs from that in older children and adults.

In the meta-analysis, we identified brain regions that are involved in semantic processing (at least to the degree that can be measured using BOLD fMRI), and we found that this set of regions is largely similar to that observed in adults [e.g., @binder2009; @jackson2021; @rodd2015].
This of course does not speak to the question of *what* these regions are doing computationally and *how* they contribute to making sense of visual or other sensory information.
To that end, I also presented two original research studies that focused on two example cases of learning visual-semantic information, namely children's learning to read and adults' learning the function of unfamiliar visual objects.

In the study on learning to read, we used fMRI to track longitudinal changes in BOLD activity amplitude and BOLD activity patterns in response to written words.
Children received reading instruction for approximately 1.5 years, during which they learnt to associate written letters (in the Hindi/Devanagari alphabet) with speech sounds and word meaning.
The design of our fMRI experiment included a comparison between real words, which carry both phonological and semantic information, and pseudowords, which carry the same amount of phonological information but do not carry any semantic information.
This contrast between words and pseudowords, which is indeed often used to isolate semantic processing [see the adult meta-analysis by @binder2009 and our developmental analysis by @enge2021], did not elicit any reliable BOLD activation in any brain area.
This is in contrast with our meta-analytic results, which showed a reliable network of regions (e.g., left MTG, left IFG, and bilateral dmPFC/pre-SMA) for this type of task.
We presented all of our stimuli in two modalities (auditory/spoken and visual/written).
The lack of semantic activation for *written* words versus pseudowords may be explained by the fact that within the 16 months of reading intervention, the children might not have become proficient enough to read and unerstand most of the words in the short period of time for which they were presented.
However, we also did not observe any semantic activation for *spoken* words versus pseudowords, even though children at this age have typically fully developed spoken language comprehensin skills [see, e.g., @skeide2016].
Therefore, these null effects might rather be driven by a lack of statistical power or low data quality in our fMRI dataset.

While we did not obtain any evidence for purely semantic activation (as defined by the words versus pseudowords contrast), we did find that learning to read led to changes in BOLD activity for other contrasts, including written words versus false fonts.
Like pseudowords, false fonts are visually similar to written words, but they do not contain any phonological or semantic information, as their constituent "letters" do not exist in the writing system and are therefore not associated with any speech sounds or word meanings.
The longitudinal changes in BOLD activity *amplitude* that we observed for this contrast therefore may reflect phonological processing, semantic processing, or a mixture of the two.
For the same contrast, we also observed that BOLD activity *patterns* in response to spoken and written stimuli in the left ventral occipito-temporal (vOT) cortex became more similar over the course of the study.
This may indicate that, as children are learning, the vOT starts to respond to spoken and written words in a similar fashion.
This may be either because the vOT is already responsive to spoken words since early childhood, and learns to access similar information from written words, or because it develops sensitivity to spoken and written words from scrath during reading acquisition.
Either way, it is again difficult to pinpoint if the vOT is engaged in phonological processing, semantic processing, or both, as this contrast (written words versus false fonts) may capture both kinds of processes.

In out third study, we investigated a different example case of learning visual-semantic information, namely learning the function of previously unfamiliar real-world objects.
We found that discovering the function of an object, as compared to viewing it without knowing its function, affected cortical processing within the first 200 ms after seeing the object, indicating that semantic information influences even relatively early stages of visual perception.
Of note, these top-down effects of semantics on visual perception could be observed immediately, that is, on the very same trial that the information has been learned as well as one trial later.
This extends previous findings on top-down effects [e.g., @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019], which used an extensive learning period before testing for any learning-related effects.

The cognitive functions of reading and visual obejct recognition are highly similar in many ways:
Both rely on visual input from the retina that is projected via the LGN in the thalamus to early visual cortex and from there along the ventral visual stream until the word or object is recognized (see Figure\ \ref{fig:vision}A in the Introduction).
Higher-level visual regions in the ventral stream, located on the lateral and ventral side of the occipito-temporal cortex, then contain clusters that respond preferentially to specific categories, including visual words (the visual word form area; VWFA) and visual objects (the lateral occipital complex\; LO\; see Figure\ \ref{fig:vision}B in the Introduction).
This led to the proposal of the neural recycling hypothesis, namely that during learning to read, the brain learns to "reuse" patches of visual cortex that had originally evolved for recognizing other types of visual stimuli [e.g., faces or body parts\; @cohen2000; @dehaene-lambertz2018; @dehaene2007; @dehaene2011; @kubota2023; @nordt2021].
Furthermore, both reading and visual object recognition are compositional, in that smaller visual units (e.g., individual letters of a word or the characteristic visual features of a face) need to be arranged in a certain way such that our brain is able to make sense of the stimuli as a whole. 

Nevertheless, there are also a number of important differences between reading and visual object recognition:
First, reading and writing began relatively late in our evolutionary history [approximately 5000--7000 years ago\; e.g., @houston2004], whereas visual object recognition as been a core function of the primate brain for a much longer period of time, likely leading to more specialized cortical circuitry.
This explains why learning to read is relatively effortful and requires explicit instruction at a relatively late age, whereas recognizing different types of objects and learning their function occurs relatively effortlessly from the first months of infancy onward.
Second, the correlation between visual and semantic information is different:
In reading, visual shapes (typically straight and curved black line segments on a black background) are arbitrarily related to the meaning of the word^[For example, the words *cat* and *car* are visually more similar than *cat* and *dog*, even though the latter to are much more closely related semantically.], whereas in visual object recognition, visual shapes and features are highly diagnostic of the category and function of an object.
Finally, reading---at least initially---requires access to the language network in order to recode the written letters into phonemes and assemble the phonemes into an auditory word form, the meaning of which can then be retrieved from the mental lexicon.
This does not seem necessary in order to understand most visual objects and act appropriately on them.
However, more proficient readers rely less on this process of phonological recoding and are able to read visual words as a whole, likely treating them more similar to other visual objects that one has acquired expertise through repeated presentation and reliable association between visual and semantic information.

In our EEG study on object recognition, we found that learning semantic information, namely the function of the object, altered event-related potentials that are associated with visual perception of the object.
Similar top-down effects of semantic knowledge [e.g., @abdelrahman2008; @maier2019; @samaha2018; @weller2019] and other kinds of non-visual information [e.g., language and emotion\; @boutonnet2015; @eiserbeck2024; @lupyan2020; @phelps2016; @thierry2016] are well established by now and speak for an interactive interplay between visual and higher-level cognitive functions [e.g., @ahissar2004; @churchland1994; @clark2013; @lupyan2015].
In reading research, on the other hand, the contrast between pseudowords and words is often used to isolate semantic processing, but it remains unknown if similar top-down effects exist, that is if the visual perception of a word changes depending on knowing versus not knowing its meaning.

## Implications for future research

Each of the present studies as well as their intersections provides ample room for additional research.
Some of this research would already be possible today, whereas other projects would depend on the development of better data acquisition methods, computational models, and substantive theories (as elaborated on in Section\ \ref{a-need-for-better-methods-questions-and-theories}).

Our meta-analysis of fMRI studies on semantic cognition in children provides an overview of the developing semantic system based on the current state of the literature.
Although already based on a relatively large number of experiments ($N = 50$), adding even more experiments as they get published would definitely make the analysis stronger, as it would reduce the false positive and false negative error rate [see, e.g., @eickhoff2016].
This is especially true given that older fMRI experiments (average year of publication in our meta-analysis = 2010) tend to have smaller sample sizes [@szucs2020] and therefore lead to greater uncertainty in the meta-analysis [@eickhoff2009; @eickhoff2012; @turkeltaub2002].
Including additional (future) studies would also help to conduct more meaningful and sensitive sub-analyses, including the differences between different semantic task types and especially the difference between different age groups, both of which we attempted but were limited by the relatively low sample sizes of the sub-groups.
Additionally, our classification of experiments into three different categories of semantic tasks (semantic world knowledge, semantic relatedness judgments, and object categorization) was created *ad hoc* based on the currently available studies.
A more theory- and/or data-driven taxonomy of task types with finer-grained categories would certainly enhance meaningfulness of a future meta-analysis [see @poldrack2011; @poldrack2016 for an attempt at classifying cognitive tasks and concepts].

Our fMRI study on learning to read could be improved and built up upon in many different ways, most of which are mentioned in the Limitations section of the original paper, including a larger sample size, a longer and more structured reading intervention, and an event-related fMRI task design that reduces head motion and allows for stimulus-level analyses (e.g., representational similarity analysis).
Two of the main novelties of our studies were (a) probing the influence of learning to read on the phonological and semantic processes, not just visual processes [as in previous longitudinal studies\; e.g., @dehaene-lambertz2018], and (b) studying learning to read in a socio-economic setting and writing system (Hindi/Devanagari) that has traditionally been neglected in developmental cognitive neuroscience.
We believe that both of these characteristics should be carried forward in future longitudinal studies, while at the same time keeping in mind the high risk associated with longitudinal studies in general and cross-cultural work in particular.
Ideally, future studies would even try to directly compare learning to read and its neural correlates in different writing systems (e.g., alphabetic versus syllabic versus logographic) to test the degree to which the brain uses similar or dissimilar brain areas for mapping graphemes to phonemes and word meaning.
To reach adequate levels of statistical power and guard against false-positive and false-negative findings, future studies should try to obtain a larger sample size, both in terms of the number of participants and in terms of the number of time points per participants, since both of these variables will determine the likelihood of accurately capturing change over time.
Should it turn out to be practically infeasible to obtain both, it might be worthwile to test either a large number of children a few times (e.g., 100 children at baseline, 1 year, and 2 years of learning to read) or a small number of children many times (e.g., 3--5 children every two weeks).
The former dataset could be analyzed using group statistics, as we did in the present study (but with much larger statistical power), whereas the latter dataset could be analyzed using single-participant statistics, as is common practice, e.g., in the non-human primate literature [e.g., @fries2022; @ince2021; @schwarzkopf2024].
Regarding the experimental paradigm, future studies could try to move beyond the artificial conditions of single word reading and implement more natural reading tasks.
In adults, it has recently been shown that naturalistc reading of stories activates semantic representations across large areas of association cortex and that this semantic space is shared with the one activated during listening to stories [@deniz2019].
It would be interesting to track the development of this semantic space longitudinally, both during learning spoken language comprehension during early childhood as well as during learning to read and becoming a proficient reader in late childhood and adolescence.

Our third study demonstrated top-down effects of semantic knowledge on visual object perception but leaves open the mechanisms by which this happens and the brain areas that are involved.
Methods with better spatial resolution (e.g., magnetoencephalography; MEG) combined with structural MRI images and source modeling [e.g., @knosche2022] or high-field fMRI with layer-specific resolution [e.g., @bandettini2021; @finn2021; @huber2021] could be used to investigate which semantic areas in association cortex influence which perceptual areas along the ventral visual stream.
Additionally, and relating this third study back to the developmental aspect of the first and second study, it would be worthwile to adapt a similar experimental paradigm in children to assess if their learning of everyday object functions (e.g., what to do with a pencil sharpener or bottle opener) shows similar electrophysiological effects as the ones observed in adults learning the function of rare real-world objects in our study.
If that turns out to be the case, one may also include other types of visual stimuli for which visual-semantic associations can be learned, including drawings, symbols, and written words, to test if their are general principles for learning visual-semantic information or if this is tied to specific class of visual stimulus.

## Implications for real world applications

All three studies presented in this thesis are firmly within the realm of basic research and, from my personal point of view, do not entail any real world applications.
The first and third study were designed to answer principled research questions (*Does the areas involved in semantic cognition in children differ from those in adults?* and *Can semantic information about influence visual perception of objects?*) and without any direct real-world applicability in mind [see also @mook1983].
Regarding the second study, in theory, longitudinal studies of learning to read would be able to identify interindividual variation in reading-related brain activity that can serve as markers for reading problems and could be used to develop targeted interventions [e.g., using neurofeedback approaches\; @taylor2023].
However, this would require a much larger sample size, highly reliable behavioral measures, and very sensitive analysis approaches (e.g., based on machine learning), all of which was not possible to implement in our longitudinal study.

## A need for better methods, questions, and theories

Our current cognitive-scientific research methods are sufficient to establish how certain aspects of brain activity covary with different kinds of experimental manipulation, e.g., which ERP components are sensitive to semantic information when perceiving a visual object.
However, we are still far away from understanding the *mechanisms* by which the brain processes semantic information and how it interacts with sensory processing.

In part, this is driven by the limitations of non-invasive measurement techniques available in human cognitive neuroscience.
While EEG offers excellent temporal resolution, it is only sensitive to the postsynaptic activity of large groups of cortical pyramidal neurons, and even these cannot be precisely localized due to the inverse problem.
MEG, while offering greater spatial specificity and signal-to-noise ratio, still suffers from the inverse problem and only picks up magnetic fields from neuronal populations close to the skull and with a certain spatial orientation.
Finally, fMRI relies on blood flow as an indirect proxy of neuronal activity and therefore lacks the temporal resolution to track information processing in real time, while offering a spatial resolution that is much greater than EEG and MEG but still on the order of multiple hundreds of thousands to millions of neurons.
Invasive recordings (e.g., electrocorticography [ECoG] with grid or depth electrodes) can directly measure local field potentials (LFPs) and therefore offer the best temporal and spatial resolution, but their use is restricted to certain spatial cases (e.g., presurgical eppilepsy patients) and a very limited number of recording sites.
It is therefore unsurprising that the study of higher-level cognitive functions such as language and semantic processing are not as well developed as, for instance, the study of the visual system [@felleman1991; @vanessen2003], which are somewhat easier to study invasively using animal models.

Three additional obstacles that seem to hamper scientific progress not only in research on visual-semantic processing but in all of cognitive neuroscience are a general lack of theory, the low replicability of research findings, and their limited generalizability.

First, most studies in cognitive neuroscience---and psychological science more broadly---start out with relatively weak predictions that are derived *ad hoc* based on heuristics or in analogy to previous studies, instead of deriving predictions from formal theories [@cummins2000; @vanrooij2020; @vanrooij2021].
Without formal theories or explicit computational models, and with the limitations of non-invasive brain recordings mentioned above, the best one can usually do is to predict that a certain experimental manipulation will affect brain activity to some unknown degree.^[Note that in fMRI, one is typically interested in an *increase* in local BOLD activity, whereas in EEG, even the polarity of an effect can oftentimes not be predicted *a priori*, as it depends on many un-measurable influence such as cortical folding. In both cases, however, there are little to no studies that make more risky (and therefore scientifically more interesting) predictions of point estimates or ranges of effect sizes.]
Combined with the pitfalls of null hypothesis significance testing and the "crud factor"^[The notion that in complex systems like the human mind and brain, everything tends to be correlated with everything else to some unknown degree, and that therefore the null hypothesis taken *literally* will always be false with sufficient statistical power.] [e.g., @orben2020], as well as publication bias that favors the publication of "positive" (i.e., statistically significant) effects, this leads to a literature of many individual "islands" of knowledge that are difficult to integrate into a bigger picture across labs, experimental paradigms, and imaging modalities [see also @forscher1963 and @meehl1978].

Second, the last 10--15 years have shown that in the field psychology, results from individual studies cannot be taken at face value, since a substantial proportion of effects (typically > 50%) does not replicate when re-running the same study with a similar or larger sample size [@camerer2018; @opensciencecollaboration2015].
This is due to a variety of reasons including small sample sizes [@button2013; @ioannidis2005], publication bias [@kuhberger2014; @rosenthal1979], and questionable research practices [@kerr1998; @simmons2011].
Unfortunately, there have not been any attempts at systematically testing the replicability of findings in cognitive neuroscience [but see @pavlov2021 for an ongoing attempt in EEG research], even though (a) cognitive neuroscience is a directly neighbouring discipline of psychology, (b) sample sizes are even lower in cognitive neuroscience than in psychology [@button2013; @szucs2017; @szucs2020], likely due to higher costs and efforts during data acquisition, and (c) analysis pipelines in cognitive neuroscience are typically highly complex, leading to highly variable results even when the exact same dataset is analyzed by different teams of researchers [@botvinik-nezer2020; @yucel2024].
It would therefore be an important step to turn direct replication of EEG and fMRI studies into a common practice, in addition to improving the quality of original studies, e.g., by pre-registering study hypotheses and analysis plans [@nosek2018; @paul2021; @peikert2023], explicating and testing hidden assumptions about tasks and measurements [@elliott2020; @kragel2021; @noble2019; @scheel2021a], increasing sample sizes [@marek2022; @szucs2017], and sharing analysis code and data [@gorgolewski2016; @markiewicz2021; @wilkinson2016].

Finally, most studies in cognitive neuroscience are conducted at research institutions in the Global North that can afford the relevant financial and technological resources, and with very selective convenience samples of participants (typically highly educated young adults).
Especially when studying higher-level cognitive functions, it would be important to show that findings translate to different sets of participants, stimuli, cultures, languages, and socio-economic strata [e.g., @henrich2010; @share2021; @yarkoni2020], or the limitations of findings in these regards should be reflected in the titles and texts of research papers [but see @mook1983 for an argument that generalizable findings are not always a goal in basic research].

## Conclusion

In this thesis, I provided different perspectives on how the human brain learns to associate visual information with meaning as well as how semantic information may feed back into visual perception.
I provided a developmental perspective of the human cortical semantic system, showing that fMRI studies of semantic cognition in children largely engage the same network of regions as known from adults.
I then used learning to read as an example where children learn to associate previously meaningless visual shapes (letters) with speech sounds and meaning, showing that this leads to local changes in word-related BOLD activity amplitudes and response patterns.
Finally, I showed that adults can rapidly learn the meaning (i.e., function) of previously unknown real-world objects, and that this learning affects early brain responses to these objects, indicating that semantic knowledge can rapidly affect visual perception.
Even though I was not able to integrate these findings into an overarching theory of visual-semantic development, I hope that they stimulate future research towards that goal.
I also hope that some of the methods developed for these projects (e.g., our implementation of bias assessment for MRI-based meta-analysis, our implementation of whole-brain linear mixed-effects models for longitudinal fMRI analysis, and our single trial EEG preprocessing pipeline) can be reused for future projects on these and other topics in the mind and brain sciences.

\newpage

# References {.unnumbered}

\newlength{\saveparindent}
\setlength{\saveparindent}{\parindent}

\raggedright
\setlength{\cslhangindent}{0.25in}
<div id="refs"></div>
\justify

\setlength\parindent{\saveparindent}

\newpage

# Original research articles {.unnumbered}

\noindent
This dissertation is based on the following original research articles:

1. **Enge, A.**, Abdel Rahman, R., & Skeide, M. A. (2021). A meta-analysis of fMRI studies of semantic cognition in children. *NeuroImage*, 241, 118436. <https://doi.org/10.1016/j.neuroimage.2021.118436>\
*Published under the terms of the [Creative Commons CC-BY license](https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.*

2. **Enge, A.** & Skeide, M. A. (2024). Tracking the neural correlates of learning to read with dense-sampling fMRI.\
*Unpublished manuscript available from first author.*

3. **Enge, A.**, Süß, F., & Abdel Rahman, R. (2023). Instant effects of semantic information on visual perception. *Journal of Neuroscience*, 43(26), 4896–4906. <https://doi.org/10.1523/JNEUROSCI.2038-22.2023>\
*Published under the terms of the [Creative Commons CC-BY license](https://creativecommons.org/licenses/by/4.0/).*

\noindent
Additional articles published during the dissertation period but not included in this thesis:

* Eiserbeck, A., **Enge, A.**, Rabovsky, M., & Abdel Rahman, R. (2021). Electrophysiological chronometry of graded consciousness during the attentional blink. *Cerebral Cortex*, bhab289. <https://doi.org/10.1093/cercor/bhab289>

* Aristei, S., Knoop, C. A., Lubrich, O., Nehrlich, T., **Enge, A.**, Stark, K., Sommer, W., & Abdel Rahman, R. (2022). Affect as Anaesthetic: How emotional contexts modulate the processing of counterintuitive concepts. *Language, Cognition and Neuroscience*. <https://doi.org/10.1080/23273798.2022.2085312>

* **Enge, A.**, Kapoor, S., Kieslinger, A.-S., & Skeide, M. A. (2023). A meta-analysis of mental rotation in the first years of life. *Developmental Science*, e13381. <https://doi.org/10.1111/desc.13381>

* Eiserbeck, A., **Enge, A.**, Rabovsky, M., & Abdel Rahman, R. (2024). Distrust before first sight? Examining knowledge- and appearance-based effects of trustworthiness on the visual consciousness of faces. *Consciousness and Cognition*, 117, 103629. <https://doi.org/10.1016/j.concog.2023.103629>

* Kessler, R., **Enge, A.**, & Skeide, M. A. (2024). How EEG preprocessing shapes decoding performance. *arXiv*. <https://doi.org/10.48550/arXiv.2410.14453>

\newpage

\phantomsection
\addcontentsline{toc}{subsection}{A meta-analysis of fMRI studies of semantic cognition in children}
\includepdf[pages=-,scale=.87,pagecommand={}]{papers/enge2021.pdf}

\newpage

\phantomsection
\addcontentsline{toc}{subsection}{Tracking the neural correlates of learning to read with dense-sampling fMRI}
\includepdf[pages=-,scale=.88,pagecommand={}]{papers/enge2024.pdf}

\newpage

\phantomsection
\addcontentsline{toc}{subsection}{Instant effects of semantic information on visual perception}
\includepdf[pages=-,scale=.88,pagecommand={}]{papers/enge2023.pdf}
\includepdf[pages=-,scale=.92,pagecommand={}]{papers/enge2023_si.pdf}

\newpage

# Acknowledgements {.unnumbered}

Thank you to my PhD supervisors, Rasha Abdel Rahman and Michael Skeide, for encouraging me to pursue a doctoral degree and for your continued academic and emotional support.
Your trust and guidance throughout my academic career have been a great honor, and you have helped me to learn not only a few little things about the human brain, but also many things about myself.
You are great mentors and I wish you continued success and joy in your research and beyond.

Thank you to the lab members of the Research Group "Learning in Early Childhood" at the Max Planck Institute for Human Cognitive and Brain Sciences, especially to Anne-Sophie and Roman, and to the lab members of the Neurocognitive Psychology Lab at Humboldt-Universität zu Berlin, especially to Julia, Kirsten, and Martin.
Your support and our countless chats and discussions have made my PhD journey so much easier and so much more enjoyable.
Should you decide to continue on this path, cognitive neuroscience has a great future ahead with scientists like you.

Thank you to all participants who dedicated their time and energy to take part in my research studies as well as to all collaborators, research assistants, non-scientific staff, and funding agencies who have enabled this research.

Thank you to Rasha Abdel Rahman, Gesa Hartwigsen, Sebastian Markett, Gesa Schaadt, and Peter Weller for dedicating their time and energy to serve on my dissertation committee, and to Rasha Abdel Rahman, Gesa Hartwigsen, and Gesa Schaadt for reviewing the written thesis.

But most of all, thank you to my family and friends, especially to my parents and to Gabriele, Julia, Konstantin, Lisa, and Nele.
Without your love, none of this would have any meaning.

\newpage

# Selbständigkeitserklärung {.unnumbered}

\noindent Hiermit erkläre ich,

* dass keine Zusammenarbeit mit gewerblichen Promotionsberatern stattfand,

* dass ich die Dissertation auf der Grundlage der angegebenen Hilfsmittel und Hilfen selbstständig angefertigt habe,

* dass ich mich nicht anderwärts um einen Doktorgrad beworben habe bzw. einen entsprechenden Doktorgrad besitze,

* dass mir die dem angestrebten Verfahren zugrunde liegende Promotionsordnung der Lebenswissenschaftlichen Fakultät vom 05. März 2015, veröffentlicht im Amtlichen Mitteilungsblatt der Humboldt-Universität zu Berlin Nr. 12/2015 bekannt ist,

* dass die Dissertation oder Teile davon nicht bereits bei einer anderen wissenschaftlichen Einrichtung eingereicht, angenommen oder abgelehnt wurden und

* dass die Grundsätze der Humboldt-Universität zu Berlin zur Sicherung guter wissenschaftlicher Praxis eingehalten wurden.

\noindent
Alexander Enge\newline
Berlin, den XX. December 2024

\clearpage\mbox{}\thispagestyle{empty}\clearpage
