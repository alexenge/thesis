---
title             : "Learning to see meaning in visual words and objects"
shorttitle        : "Note: This page and the next should be removed"

author: 
  - name          : "Alexander Enge"

bibliography      : "references.bib"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
numbersections    : true

csl               : "apa.csl"
documentclass     : "apa6"
classoption       : "doc,11pt,twoside"
output            :
  papaja::apa6_pdf:
    latex_engine  : "lualatex"

header-includes:
  - \geometry{a4paper,margin=25mm}
  - \setcounter{tocdepth}{2}
  - \raggedbottom
  - \usepackage{ragged2e}
  - \linespread{1.15}
  - \fancyhead{}
  - \fancyheadoffset[RO,LE]{0pt}
  - \fancyhead[RO,LE]{\thepage}
  - \fancyhead[LO]{Alexander Enge}
  - \fancyhead[RE]{Learning to see meaning in visual words and objects}
  - \renewcommand{\headrulewidth}{0.4pt}
  - \usepackage{soul}
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\doublespacing}
  - \usepackage{makecell}
  - \renewcommand{\cellset}{\renewcommand{\arraystretch}{0.7}}
  - \captionsetup[table]{font={footnotesize,stretch=1.0},labelfont={bf,up},skip=5pt}
  - \captionsetup[figure]{font={footnotesize,stretch=1.0},labelfont={bf,up},skip=5pt}
  - \usepackage[all]{nowidow}
  - \usepackage[bottom]{footmisc}
  - \interfootnotelinepenalty=10000
  - \usepackage{newcomputermodern}
  - \usepackage{pdfpages}
---

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}
\begin{center}
\vspace*{15mm}
\textbf{Learning to see meaning in visual words and objects}\\
\vspace*{10mm}
\textbf{\textsc{\so{Dissertation}}}\\
\vspace*{10mm}
zur Erlangung des akademischen Grades \\
Doctor rerum naturalium (Dr. rer. nat.)\\
im Fach Psychologie\\
\vspace*{10mm}
eingereicht an der\\
Lebenswissenschaftlichen Fakultät \\
der Humboldt-Universität zu Berlin\\
\vspace*{10mm}
von\\
\textbf{M.Sc. Alexander Enge}\\
geboren am 09.11.1996 in Räckelwitz\\
\vspace*{40mm}
\end{center}
\begin{flushleft}
Präsidentin der Humboldt-Universität zu Berlin\\
Prof. Dr. Julia von Blumenthal\\
\vspace*{10mm}
Dekan der Lebenswissenschaftlichen Fakultät der Humboldt-Universität zu Berlin\\
Prof. Dr. Dr. Christian Ulrichs\\
\vspace*{10mm}
Gutachter*innen\\
1. Prof. Dr. Rasha Abdel Rahman\\
2. Prof. Dr. Gesa Hartwigsen\\
3. Dr. Olaf Dimigen\\
\vspace*{10mm}
Tag der mündlichen Prüfung: XX.XX.XXXX
\end{flushleft}

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}

# Zusammenfassung {.unlisted .unnumbered}

\newpage

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}

# Abstract {.unlisted .unnumbered}

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\thispagestyle{empty}
\begin{flushleft}
{\setstretch{1.0}\tableofcontents}
\end{flushleft}

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\setcounter{page}{1}

# Introduction

\newpage

# Summary of the present studies

## A meta-analysis of fMRI studies of semantic cognition in children

In this first study [@enge2021], we provide a quantitative synthesis of the brain areas involved during semantic processing children.
To this end, we conducted a meta-analysis of all available fMRI studies in children that probed different aspects of semantic cognition, including semantic knowledge (e.g., naming an object after hearing a verbal description), semantic relations (e.g., deciding if pairs of words belong together or not), and visual object categorization (e.g., viewing object images from different semantic categories).

Meta-analyses are a useful tool to synthesize the research literature within a relatively narrow domain [@glass1976; @hedges1985; @harrer2021].
The surfacing of the latest replicability crisis in psychological science [@opensciencecollaboration2015] was a reminder that the findings from any individual research paper cannot be accepted as true and irrevocable facts, due to problems such as small sample sizes, publication bias (the "file drawer effect," i.e., the low publication rate of negative findings), *post hoc* theorizing, and questionable research practices [@ioannidis2005; @kerr1998; @simmons2011].
To get a slightly more reliable and balanced view of the research outcomes in a certain area, meta-analysis empirically integrate the findings from many---ideally all---studies on a given topic.
By doing so, they can (a) show which effects reported in the original papers are likely to be robust and replicable, thereby reducing the number of false positives, and (b) identify novel effects that individual original papers might not have the statistical power to detect, thereby reducing the number of false negatives [@schmidt2015].

In the fMRI literature, the evidential status of the results of individual papers is especially doubtful.
This is because compared to behavioral studies and neuroscientific studies using "cheaper" methods (e.g., EEG), data collection is at least an order of magnitude more costly and depends on the availability of specialized hardware.
At the same time, data preprocessing and analysis methods are highly complex, which introduces many potential sources of error and researcher degrees of freedom.
Within the fMRI literature, developmental studies are especially prone to these problems because it is more difficult to recruit children (as compared to, e.g., Psychology undergraduates in need of course credit or monetary compensation) and because children tend to show lower levels of compliance and higher levels of head motion, which reduces the amount of data that can be collected as well as its quality.

Our goal was therefore to identify and meta-analyze all available fMRI studies on semantic cognition in children, building up on previous fMRI meta-analyses on semantic cognition in adults [@binder2009; @jackson2021; @rodd2015; @vigneau2006; @visser2010] as well as our own previous fMRI meta-analysis on language processing in children [@enge2020].
Using three online databases, we identified approximately 1,000 articles based on our keyword search, 45 of which fulfilled all of our inclusion criteria.
In these 45 articles, there were 50 fMRI experiments on semantic cognition in children, which we further classified into semantic knowledge experiments (21 experiments), semantic relatedness experiments (16 experiments), and visual object categorization experiments (13 experiments).
In total, these experiments included data from 1,018 children with a mean age of 10.1 years (range = 4--15 years).

The experiments reported 687 peak coordinates from statistically significant clusters associated with semantic cognition in children.
Using the activation likelihood estimate (ALE) algorithm [@eickhoff2009; @eickhoff2012; @turkeltaub2002] implemented in the NiMARE software package [@salo2023], these peaks were convolved with a Gaussian smoothing function, integrated into a meta-analytic map, and thresholded using a permutation-based family-wise error correction for multiple comparisons at the whole-brain level [@eickhoff2012; @eickhoff2016].
For general semantic cognition, combined across all three task types, there was reliable BOLD activity across experiments in eight clusters, namely in the left inferior frontal gyrus (pars triangularis; two clusters), the bilateral supplementary motor area, the left fusiform gyrus, the right insula, the left middle temporal gyrus, the right inferior occipital gyrus, and the right fusiform gyrus.
Broken down by task category, three clusters (left inferior frontal gyrus, bilateral supplementary motor area, and right insula) were activated for both semantic world knowledge experiments and semantic relatedness experiments, whereas the left middle temporal gyrus was only activated for semantic relatedness experiments and the left and right fusiform and occipital clusters were only activated for semantic object categorization experiments.^[However, note that this *difference in statistical significance* does not automatically imply a *statistically significant difference* [@gelman2006; @nieuwenhuis2011]. For a formal statistical comparison of the different task types, see Figure 6 and Table 4 in the original paper.]

We validated our results from the ALE analysis using a second meta-analytic approach [seed-based *d* mapping\; @albajes-eizagirre2019; @albajes-eizagirre2019a], which gave qualitatively similar results and also allowed us to control for various experiment-level covariates (e.g., language, sensory modality, response modality).

We tested for age related effects both within our meta-analytic sample of children's experiments and by comparing our results in children with those from a previous meta-analysis on semantic cognition in adults [@jackson2021].
Regarding age-related changes during childhood, we found one cluster in the right insula that showed significantly more reliable activation in older as compared to younger children [but note the limited statistical power of this analysis as well the statistical problems associated with median split analyses\; e.g., @irwin2003; @mcclelland2015].
Regarding the comparison to adults, there was very large overlap across left and right inferior frontal, supplementary motor, and left temporal regions, but also significantly less reliable activation in children in the left anterior temporal lobe and significantly more reliable activation in fusiform and occipital regions, especially in the right hemisphere.^[But note that this latter effect may be due to differences in the relative number of linguistic versus visal object categorization experiments included in the two meta-analyses.].

Finally, we used to previously established empirical techniques to examine the robustness of our meta-analytic results against spurious effects and publication bias in the original literature [leave-one-out and fail-safe $N$\; @acar2018; @enge2020; @samartsidis2020].
These analyses showed that all of our clusters from the main analysis and almost all clusters from the sub-analyses of task types were reliable even if there were spurious original experiments or strong publication bias.

Taken together, our meta-analysis is the first of fMRI experiments on semantic cognition in children, and showed strong evidence for a reliably activated semantic network that is remarkably similar to those reported in the adult fMRI literature [for meta-analysis, see, e.g., @binder2009; @jackson2021].
We found that experiments with linguistic materials (semantic world knowledge and relatedness judgments) reliably activated the same regions in the left inferior frontal and bilateral premotor cortices, whereas visual object categorization experiments activated a distinct set of regions in posterior brain areas (bilateral fusiform and right occipital cortices).^[Potentially casting doubt on our decision to lump the latter type of experiments together with the former into one meta-analysis, as visual object category processing may be based purely on differences in lower-level visual features and may not necessarily engage higher-level semantic processing].
Despite the large overlap between our meta-analytic results and previous meta-analytic results in adults [@jackson2021], there were a few reliable differences.
The most striking of these was that there was significantly less reliable activation in children in the anterior part of the left temporal lobe, an area associated with very high-level and amodal semantic processing in adults [e.g., @lambonralph2017; @patterson2007].
This difference may point to the fact that this semantic "hub" may not yet be fully developed in children at the typical age included in our meta-analysis (mostly 8--12 years; see Figure 3 in the original paper).

We have shared the data and analysis code of our meta-analysis on the Open Science Framework (<https://osf.io/34ry2>) and on GitHub (<https://github.com/SkeideLab/meta_semantics>) so that they can be scrutinized and reused.^[At the time of writing, three new meta-analyses on completely different topics have acknowledged reusing our scripts [@bortolini2024, @cui2022, @yang2024].]

## Tracking the neural correlates of learning to read with dense-sampling fMRI

In this second study [@enge2024], we focused on children's learning to read as one case example for the human brain learning to associate visual shapes with semantic (and phonological) information.
Learning to read is a major step in most people's cognitive development and is of great importance for the rest of our lives (for instance, allowing you to consume this PhD thesis).
Given that from a phylogenetic point of view, reading is a fairly recent phenomenon, it is not at all obvious that most human beings alive nowadays at some point in their lives become able to absorb and transmit semantic information via small and somewhat arbitrary black symbols printed on the page of a book or rendered on a digital screen.
Yet, this is exactly what happened over the course of the last few millennia, rapidly picking up over the past ~500 years, in turn fueling many other great achievements of humankind, including science and technology.

Unlike reading and writing, spoken language has been part of human culture and everyday life for hundreds of thousands of years, which has left enough time to evolve dedicated brain hardware and neural mechanisms for spoken language comprehension and production [for review, see, e.g., @fedorenko2024; @ferstl2008; @friederici2017; @hagoort2016; @hickok2007].
It is therefore unsurprising that human babies pick up spoken language comprehension quickly and relatively effortlessly within typically less than 2 years of age [for review, see @friederici2006; @skeide2016], with certain aspects of speech processing are already functioning *in utero* [@ghio2021].
Reading, on the other hand, requires years of explicit instruction and is typically only learned during the kindergarten and primary school age (typically 5--8 years).
During this process of learning to read, a child's brain needs to become able to (a) reliably recognize letters and strings of letters in one's own writing system and distinguish them from other visual kinds of visual object categories, (b) become able to link individual letters to their corresponding speech sounds, to enable decoding of written words, and, as the child becomes more proficient, (c) learn to access the meaning of an entire word from its visual word form [@frith1986; @ehri2005].
While the first aspect (visual word form recognition) has received much attention in the cognitive neuroscience of reading and reading development [e.g., @cohen2000; @dehaene-lambertz2018; @dehaene2011], the latter two aspects (linking visual words to spoken language and linking visual words to word meaning) have received much less attention [but see, e.g., @vin2024].

Studying the neural correlates of learning to read requires to take repeated measurements of brain activity (e.g., using EEG or fMRI) while children are participating in reading instruction.
Doing so is often difficult for a number of reasons, including the fact that reading instructed is typically confounded with other aspects of schooling and general cognitive development,the difficulty of recruiting children and their families at that age and for a prolonged period of time, the difficulty of collecting sufficiently large quantities of high-quality data (e.g., due to children's lower attention span and increased head motion during MRI scanning), and a lack of readily available statistical models and toolboxes for longitudinal analysis of brain data.
Additionally, available longitudinal studies of the neural correlates of learning to read [for review, see @chyl2021a]. have exclusively been carried out with children from the Global North, which limits their generalizability to other cultural and socio-economic backgrounds as well as to different, non-alphabetic writing systems.

Our goal was to overcome these limitations and investigate the change in children's brain responses to written and spoken words as they are learning to read, and to do so in a cultural setting and writing system that has typically been neglected by developmental cognitive neuroscience.
We expected that learning to read would cause an increase in brain activity in brain areas known to be involved in written and spoken word processing (e.g., the visual word form area).
We also expected that the multi-voxel response patterns in audio-visual processing areas (e.g., the left posterior temporal sulcus; pSTS) would become more similar between written and spoken words as the brain learns to connect the former to the latter.
Finally, we expected that responses to written words would become more similar to each other, that is, more stable from one scanning session to the next, as the brain becomes more finely tuned to visual letters.

We acquired fMRI from two groups of children in the region of Uttar Pradesh, India, which had no access to public schooling and received approximately 1.5 years of reading instruction (intervention group) or math instruction (active control group) as part of our study.
For the purpose of the present paper, only the data from the reading instruction group was analyzed.
The participants completed up to 6 experimental sessions, spaced at irregular intervals of approximately 2--3 months.
During these sessions, we acquired a structural MRI scan, a functional MRI (fMRI) scan, and behavioral data on standardized tests of reading skills, maths skills, working memory, general cognitive ability.
In the fMRI scan, children were presented with short blocks of written words, written pseudowords, and written low-level control stimuli (false fonts), as well as spoken words, spoken pseudowords, and spoken low-level control stimuli (noise-vocoded speech).
We analyzed these longitudinal data using linear mixed-effects models to test for linear and non-linear changes in behavioral test scores, BOLD activity amplitude, multi-voxel audio-visual pattern similarity, and multi-voxel within-condition pattern stability.
For this purpose, we created a novel implementation of whole-brain linear mixed-effects model using the high-performance Julia language [@bezanson2017], which we hope may be re-used for future longitudinal fMRI [for previous implementations of whole-brain linear mixed-effects models using the somewhat slower R language, see @chen2013a; @madhyastha2018].

In the behavior data, we found a statistically significant improvement in test scores over the course of the study for most subtests of reading skills, including word and pseudoword reading accuracy, phoneme replacement, semantic fluency, reading comprehension, and dictation.
Additionally, there also was a statistically significant improvement in two subtests of maths skills and in one subtest of working memory (backward digit span).
Few subtests of reading skills (picture naming, verbal fluency, and word, pseudoword, and passage reading time) showed no significant change, as did other subtests of working memory (Corsi block test forward and backward, backward digit span) and the test for general cognitive ability (Raven's Progressive Colored Matrices).

In the analysis of whole-brain BOLD activity amplitudes, we found robust sensory activation in the auditory cortex (bilateral superior temporal gyri and sulci) for all auditory conditions (spoken words, pseudowords, and low-level controls) and in the visual cortex (from early visual cortex to the ventral occipto-temporal cortex) for all visual conditions (written words, pseudowords, and low-level controls).
However, there were only very few and small clusters that showed a significant change (linear or non-linear) in BOLD activity amplitude over the course of the study.
There were some non-linear changes for spoken stimuli in the auditory cortex and for written stimuli in the visual cortex but their shape did not follow a pattern that we had predicted.^[We had predicted a linear increase of BOLD activity or a non-linear, negative quadratic pattern (i.e., an inverted "u" shape), based on a previous study [@dehaene-lambertz2018] and on the expansion and renormalization model of brain plasticity [@wenger2017]. What we observed in most clusters was a positive quadratic pattern (i.e., a "u" shape), which seems difficult to interpret in the context of brain plasticity and skill development.]
For the contrast of written words versus written low-level controls, there was one cluster at the ventral posterior cingulate cortex that showed an increase in BOLD activity over the first ~6 months of the study, followed by a decline over the remaining ~10 months, as we had predicted.
However, this cluster was very small, the individual BOLD activity curves of individual participants in this cluster were very variable, and this anatomical location has typically not been considered as part of the visual word form recognition or language networks, therefore casting doubt on the reliability and meaningfulness of this finding.

In the analysis of multi-voxel audio-visual pattern similarity, we found that over the course of the study, BOLD activity patterns in the left ventral occipito-temporal (vOT) cortex became more similar over time for the pair of contrasts of spoken words versus low-level controls and written words versus low-level controls.
This was in line with our hypotheses and may indicate that over the course of learning to read, the brain becomes able to access phonological and semantic information from written words that it had previously only been able to access from spoken words.
There was no significant change in audio-visual pattern similarity for any of the other pairs of contrasts and regions of interests.

In the analysis of multi-voxel within-condition pattern stability, we found that over the course of the study, BOLD activity patterns became less stable (i.e., self-similar) for auditory pseudowords and words in the bilateral posterior superior temporal sulcus (pSTS).
This was not in line with our *a priori* hypotheses, which predicted an *increase* in pattern stability specifically for written words and pseudowords.

Taken together, our study provides only weak evidence for the idea that learning to read increases BOLD activity amplitude and audio-visual processing in visual and language-related brain areas.
While we did find some evidence for longitudinal change in BOLD activity amplitudes, these changes typically did not follow a theoretically predicted pattern of linear or non-linear growth and did typically not occur in areas associated with reading or audio-visual processing.
However, we did find some evidence for an increase in audio-visual processing activity in the left vOT cortex.
There was no reliable change in pattern stability, that is, we did not find that reading leads to more stable (i.e., self-similar) written word representations.

It is important to point out that these weak results may at least in part be driven by methodological shortcomings of our study, including a very small sample size [15 children with 2--6 sessions each\; see @button2013; @ioannidis2005], an intervention that might not have been long enough to capture the entire process of becoming a sufficiently fluent reader (especially in the relatively complex Devanagari writing system), and an fMRI block design that was not suitable for multivariate analysis at the single item level, thereby preventing us from comparing individual word representations.
We nevertheless hope that some of our findings can be replicated and that our general study design and longitudinal statistical analysis approach inspires future longitudinal work in different cultures and writing systems.

## Instant effects of semantic information on visual perception

In this third and final study [@enge2023a],^[Note that order in which the studies are presented in this thesis was based on their content and logical flow, not on their chronological order of publication.] we switched our focus from the development of visual-semantic processing during childhood to the learning visual-semantic information in adults.
Although most of our learning of the meaning of visual objects takes place during infancy (e.g., by learning to differentiate high-level visual object categories) and childhood (e.g., by learning to associate abstract visual shapes with meaning during learning to read), this capacity luckily remains available as we age---or otherwise we would not be able to understand much at all in a world where we are presented with novel gadgets and icons almost on a daily basis.

Our goal was to capture the moment during which we learn to understand the function of a previously unknown visual object, as well as to test if this understanding changes the way in which we perceive the object visually.
This latter question has sparked endless debate among cognitive scientists.
Some argue that perception and cognition are two distinct processing stages and that perception takes place in feed-forward, modular fashion, and then passes on its outputs to other higher-level systems (e.g., those processing semantic information or emotions) but cannot itself be influenced by them [e.g., @dicarlo2012; @firestone2016; @fodor1983; @fodor1984; @machery2015; @pylyshyn1999].
Others argue that this distinction between perception and cognition cannot be maintained and/or that cognition interacts with perception in a top-down fashion from the earliest stages of sensory processing on [e.g., @ahissar2004; @churchland1994; @clark2013; @friston2009; @lupyan2020; @thierry2016; @vonhelmholtz1867; @yuille2006].
Recent empirical evidence by and large supports the latter view, as it has been shown that emotional, linguistic, and semantic information can change the response to visually identical stimuli in terms of behavioral measures [e.g., @gauthier2003; @phelps2016; @slivac2021], the BOLD response in visual cortex as measured with fMRI [e.g., @clarke2016; @hsieh2010], and early ERP components in the EEG [e.g., abdelrahman2008; @boutonnet2015; @samaha2018].
The latter type of evidence seems especially informative, as the high temporal resolution of the EEG (on the order of microseconds, directly capturing cortical postsynaptic potentials) allows to test how early during processing one can detect high-level (e.g., linguistic or semantic) influences [see @athanasopoulos2020 for an extended version of this argument].
For instance, our research group and others have shown that having participants learn semantic information about previously unknown visual stimuli elicits not only differences in relatively late, semantic ERP components (e.g., the N400 component), but also in early ERP components typically associated with visual perception [e.g., the P1 component\; @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019].
However, all of these previous studies have used an extensive training phase in which participants learned to associate the visual stimuli with the semantic information, whereas the EEG was only measured and analyzed after this learning had taken place.

In our study, we wanted to test if learning semantic information about previously unknown visual objects affects visual perception instantly, that is, directly as this information is being acquired and the function of the object is being understood.
To this end, we presented participants with images of existing but rare visual objects (e.g., a galvanometer) in three separate phase: First, without any information, to probe if the objects were indeed unknown to participants, second, with a brief verbal description that allowed participants to understand what kind of object they were seeing, and third, again without any information, to test for downstream effects of the understanding acquired in the second phase.
Crucially, in the second phase, we presented half of the objects with the correct, matching verbal descriptions, which typically allowed participants to form a correct understanding of the object, and the other half of the object with incorrect, non-matching verbal description, which typically precluded such an understanding.^[Note that we used participants' behavioral reports to verify that this manipulation worked as intended, and, on a participant-by-participant basis, we only included those objects in the analysis for which it did.]
This allowed us to compare responses to the same visual objects with and without a semantic understanding, and to do so before, while, and after this understanding had happened.

During all three phases of the experiment, we measured the EEG and analyzed three different ERP components, namely the P1 component (100--150 ms after stimulus onset) as a marker of early visual perception, the N170 component (150--200 ms) as a marker of high-level visual perception, and the N400 component (400--700 ms) as a marker of semantic processing.
In the first phase, we observed no reliable differences in any of the three ERP components between objects that would subsequently show semantically informed perception as compared to semantically uninformed perception.
This was expected given that the relevant semantic information had not yet been presented, so that all objects were unfamiliar and not understood by the participants.
In the second phase, when the objects were presented with the semantic information (either a matching description, inducing semantically informed perception, or a non-matching description, keeping the perception semantically uninformed), we did observe reliable difference in two of the three ERP components.
That is, semantically informed perception caused significantly larger (i.e., more negative) ERP amplitudes in the N170 component and significantly reduced (i.e., less negative) ERP amplitudes in the N400 component.
There was no effect of semantically informed perception in the P1 component.
In the third phase, when objects were presented once more without any information, semantically informed perception caused significantly larger (i.e., more positive) ERP amplitudes in the P1 component and, again, significantly reduced (i.e., less negative) ERP amplitudes in the N400 component.

We interpret these ERP effects as evidence that semantic information not only affects late, post-perceptual stages of visual object processing (i.e., the N400 component), but also earlier, perceptual processing (i.e., the P1 and N170 components).
This conceptually replicates previous studies from our lab and others [e.g., @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019] and provides evidence for an interactive bottom-up and top-down interplay between perception and cognition [e.g., @clark2013; @lupyan2015].
Crucially, however, our study is the first to show that these top-down effects of semantics on perception do not require an extensive learning history to develop.
Instead, they can be observed on the very same trial as the visual object is first associated with its semantic meaning (for the N170 component) as well as on the very next time when the object is re-encountered, even without the semantic information being present (for the P1 component).
Previous research, with separate learning and EEG phases, has typically only reported the P1 effect [e.g., @abdelrahman2008; @boutonnet2015; @maier2019; @samaha2018; @weller2019], probably the N170 is short-lived and tied to the specific moment of semantic "insight" associated with understanding the function of a previously unknown visual object for the first time.

We also conducted an additional time-frequency analysis of the same data to check for any effects of semantic information on event-related power in different frequency bands.
Unlike ERPs, these changes in event-related power do not need to be tightly phase-locked and time-locked to stimulus onset, which seems plausible given that participants might have taken different amounts of time to understand the functions of different objects.
Since we did not have any *a prior* hypothesis about the specific time window, frequency range, and EEG channels at which semantically informed perception would affect event-related power, we used mass-univariate, cluster-based permutation tests [@maris2007; @sassenhagen2019] to test for any differences in exploratory fashion.
We found no significant effects in the first phase, before any semantic information was presented (as would be expected), but one significant cluster each in the second and third phase.
In both cases, there was a statistically significant reduction of event-related power relatively late after stimulus onset (from approximately 600 ms onwards) in the alpha and lower beta frequency bands (approximately 8--20 Hz).
Although we did not hypothesize this specific finding, and therefore would like to see it replicated in future studies, it is consistent with previous findings that reduced alpha/beta power facilitates visual-semantic memory formation [@griffiths2019; @hanslmayr2009; @hanslmayr2012], presumably because this oscillatory activity acts as noise that can hamper the encoding of stimulus-specific information.

Taken together, our study shows that adults are quickly able to associate novel semantic information with previously unfamiliar visual objects, and that this newly acquired semantic information can affect early stages of stimulus processing, typically associated with visual perception itself.
Importantly, this top-down effect of semantics on visual perception can be observed immediately as the understanding of being acquired and manifests itself in enlarged ERP amplitudes of ERP components associated with lower-level visual perception (P1 component) and higher-level visual perception (N170 component), as well as in reduced ERP amplitudes in an ERP component associated with semantic precessing demands (N400 components) and reduced alpha/beta band power.
This speaks against a modular view of visual perception being encapsulated from higher-level cognitive functions [e.g., @firestone2016; @fodor1983] and favors theories of vision as an interactive, context-sensitive, and prediction-driven process [e.g., @ahissar2004; @friston2009; @lupyan2020].

### Bonus: The `hu-neuro-pipeline` package

In this section, I would like to introduce one further research output that, although it is not (yet) captured in a written publication, has already had some impact in my own research bubble and that has definitely benefitted my own personal development.
For the EEG analyses presented in the @enge2023a paper, I wrote my custom EEG analysis pipeline, based on a previously published pipeline used in our lab [see @fromer2018 and their code and data published at <https://osf.io/hdxvb>].
The rationale of this pipeline was to carry out a number of standard EEG preprocessing steps, re-referencing, correction of eye artifacts, frequency-domain filtering, epoching, and the rejection of bad epochs based on amplitude thresholds.
Unlike in traditional EEG processing workflows [e.g., @luck2014a], the preprocessed epochs are than *not* averaged into evoked potentials, which would be suitable for statistical analyses using traditional repeated-measures tests from the general linear model (GLM) family, such as a paired $t$-test or a repeated-measures analysis of variance (rmANOVA).
Instead, the EEG data remain at the single trial level and are entered directly into a linear mixed-effects model (LMM), which can adequately handle the repeated-measures structure of the data (with trials nested in participants and/or stimuli) via an appropriately specified random effects structure [@burki2018; @fromer2018; @kretzschmar2023; @volpert-esmond2021].
Compared to averaging and using a GLM, modeling the single trial data with LMMs provides a number of advantages, including:

* the ability to include both participants *and* stimuli as random effects, which is necessary to maintain adequate false-positive error control [@burki2018; @judd2012] and to allow for inference from the specific sample of stimuli to the larger population from which they were drawn [@clark1973; @yarkoni2020],

* the ability to include trial- and stimulus-level covariates [e.g., stimulus characteristics, fatigue, drift\; @volpert-esmond2021],

* the ability to include not just categorical predictors (e.g., factorial manipulations), but also continuous predictors [e.g., stimulus ratings, parametric manipulations\; @brown2021; @fromer2018],

* the ability to include person-level predictors (e.g., age, gender, test scores), and

* the ability to handle unbalanced designs, that is, an uneven number of trials per participant and conditions, which is inevitable in some experimental designs [e.g., @enge2023a; @frober2017] as well as when conditions and/or participants differ in their number of epochs after artifact rejection (which will be almost always the case).

The @fromer2018 pipeline implemented this procedure of standard EEG preprocessing plus single trial LMMs as a collection of scripts in the MATLAB language (The MathWorks Inc., Natick, Massachusetts) using the EEGLAB toolbox [@delorme2004].
However, a few disadvantages of this implementations are that (a) MATLAB is a commercial software, which costs up to $1000 per user and is therefore not readily usable outside of well-funded research institutions, (b) psychologists are typically not trained in using the MATLAB language, and (c) the code for the pipeline was not structured, documented, and version-controlled following current best practices in research software development [e.g., @barker2022; @scheliga2019].
In the Python re-implementation which I created for the @enge2023a paper and subsequently published as a standalone Python package at <https://github.com/alexenge/hu-neuro-pipeline>, I tried to overcome these limitations and make some further improvements by:

(1) using a widely used open-source (Python) instead of proprietary (MATLAB) software language,

(2) building the pipeline on top of MNE-Python [@gramfort2013], a state-of-the-art M/EEG analysis toolbox with larger developer and user base and following best practices from professional software development,

(3) creating an R interface, so that psychologists without prior training in Python or MATLAB can use the pipeline out of the box,

(4) adding support for time-frequency analysis (including single trial analysis and cluster-based permutation tests),

(5) releasing the code as a package (using the Python Package Index, see <https://pypi.org/project/hu-neuro-pipeline>), instead of as a collection scripts, so that it can be installed and imported more easily,

(5) releasing the code under version control, so that users can examine and install older versions of the package,

(6) adding a publicly accessible documentation website (see <https://hu-neuro-pipeline.readthedocs.io>) that explains the installation, usage, and processing details of the pipeline, both for Python and R users, and

(7) creating publicly accessible course materials on the pipeline package itself (see <https://github.com/alexenge/hu-neuro-pipeline-workshop>), time frequency analysis (see <https://github.com/alexenge/tfr-workshop>), and EEG analysis in general (see <https://alexenge.github.io/intro-to-eeg>).

The pipeline is designed with a user-interface that consists of one high-level function, `pipeline.group_pipeline`, that can be used to process the raw data from an EEG group study up to the point where the data are ready for single trial LMM modeling.
More precisely, it reads the raw data from each participant, optionally resamples it to a lower sampling rate, optionally interpolates any bad channels (which can also be automatically detected), re-references the data (per default to an average reference), optionally performs eye artifact correction (using the semi-automatic multiple source eye correction procedure or fully automatic independent component analysis [ICA]), filters the data in the frequency domain (per default between 0.1 and 40 Hz), segments the continuous data into epochs based on event markers, reads any accompanying behavioral-experimental log files and matches them to the corresponding EEG epochs, rejects "bad" epochs based on a peak-to-peak amplitude threshold, and computes the single trial amplitudes for any EEG components of interest by averaging across their *a priori* defined time windows and regions of interest.
At the group level, the pipeline combines the single trial amplitudes from all participants into one large data frame that can be used as-is for LMM modeling, e.g., using the `lmer` function from the `lme4` package in R [@bates2015].
Additionally, the pipeline also outputs evoked potentials (i.e., averaged waveforms for all participants, experimental conditions, and channels) that can be used for visualization or to compare the statistical results from LMMs and $t$-tests/rmANOVAs, as well as a metadata file that can be referred to when wanting to check the parameters and software versions that were used when running the pipeline.

In sum, the `hu-neuro-pipeline` package provides a relatively easy to use and state of the art EEG analysis pipeline that can be used for LMM analysis of single trial event-related potentials and time-frequency analysis.
Any questions, problems, or improvements to the package can be suggested via the issue tracking system on GitHub (<https://github.com/alexenge/hu-neuro-pipeline/issues>). 

\newpage

# General discussion

\newpage

# References {.unnumbered}

\newlength{\saveparindent}
\setlength{\saveparindent}{\parindent}

\raggedright
\setlength{\cslhangindent}{0.25in}
<div id="refs"></div>
\justify

\setlength\parindent{\saveparindent}

\newpage

# Original research articles {.unnumbered}

\noindent
This dissertation is based on the following original research articles:

1. **Enge, A.**, Abdel Rahman, R., & Skeide, M. A. (2021). A meta-analysis of fMRI studies of semantic cognition in children. *NeuroImage*, 241, 118436. <https://doi.org/10.1016/j.neuroimage.2021.118436>\
*Published under the terms of the [Creative Commons CC-BY license](https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.*

2. **Enge, A.** & Skeide, M. A. (2024). Tracking the neural correlates of learning to read with dense-sampling fMRI.\
*Unpublished manuscript available from first author.*

3. **Enge, A.**, Süß, F., & Abdel Rahman, R. (2023). Instant effects of semantic information on visual perception. *Journal of Neuroscience*, 43(26), 4896–4906. <https://doi.org/10.1523/JNEUROSCI.2038-22.2023>\
*Published under the terms of the [Creative Commons CC-BY license](https://creativecommons.org/licenses/by/4.0/).*

\noindent
Additional articles published during the dissertation period but not included in this thesis (in chronological order):

* Eiserbeck, A., **Enge, A.**, Rabovsky, M., & Abdel Rahman, R. (2021). Electrophysiological chronometry of graded consciousness during the attentional blink. *Cerebral Cortex*, bhab289. <https://doi.org/10.1093/cercor/bhab289>

* Aristei, S., Knoop, C. A., Lubrich, O., Nehrlich, T., **Enge, A.**, Stark, K., Sommer, W., & Abdel Rahman, R. (2022). Affect as Anaesthetic: How emotional contexts modulate the processing of counterintuitive concepts. *Language, Cognition and Neuroscience*. <https://doi.org/10.1080/23273798.2022.2085312>

* **Enge, A.**, Kapoor, S., Kieslinger, A.-S., & Skeide, M. A. (2023). A meta-analysis of mental rotation in the first years of life. *Developmental Science*, e13381. <https://doi.org/10.1111/desc.13381>

* Eiserbeck, A., **Enge, A.**, Rabovsky, M., & Abdel Rahman, R. (2024). Distrust before first sight? Examining knowledge- and appearance-based effects of trustworthiness on the visual consciousness of faces. *Consciousness and Cognition*, 117, 103629. <https://doi.org/10.1016/j.concog.2023.103629>

* Kessler, R., **Enge, A.**, & Skeide, M. A. (2024). How EEG preprocessing shapes decoding performance (arXiv:2410.14453). arXiv. https://doi.org/10.48550/arXiv.2410.14453

\newpage

\phantomsection
\addcontentsline{toc}{subsection}{A meta-analysis of fMRI studies of semantic cognition in children}
\includepdf[pages=-,scale=.87,pagecommand={}]{papers/enge2021.pdf}

\newpage

\phantomsection
\addcontentsline{toc}{subsection}{Tracking the neural correlates of learning to read with dense-sampling fMRI}
\includepdf[pages=-,scale=.88,pagecommand={}]{papers/enge2024.pdf}

\newpage

\phantomsection
\addcontentsline{toc}{subsection}{Instant effects of semantic information on visual perception}
\includepdf[pages=-,scale=.88,pagecommand={}]{papers/enge2023.pdf}
\includepdf[pages=-,scale=.92,pagecommand={}]{papers/enge2023_si.pdf}

\newpage

# Acknowledgements {.unnumbered}

Thank you to my PhD supervisors, Michael Skeide and Rasha Abdel Rahman, for encouraging me to pursue a doctoral degree and for your continuing academic and emotional support.
Your trust and guidance throughout my academic journey have been a great honor, and you have helped me to learn not only a few things about the human mind and brain, but also many things about myself.
You are great mentors and I wish you continued success and joy in your research and beyond.

Thank you to the lab members of the Research Group "Learning in Early Childhood" at the MPI CBS, especially to Anne-Sophie and Roman, and to the lab members of the Neurocognitive Psychology Lab at HU Berlin, especially to Julia, Martin, and Kirsten.
Your support and the time that we have spent together discussing research, methods, and everything else under the sun has made my PhD journey so much easier and so much more enjoyable.

Thank you to all participants who dedicated their time and energy to participate in my research studies as well as to all collaborators, research assistants, non-scientific staff, and funding agencies who have enabled this research.

But most of all, thank you to my family and friends, especially to my parents and to Gabriele, Julia, Konstantin, Lisa, and Nele.
Without your love, none of this would have any meaning.

\newpage

# Selbständigkeitserklärung {.unnumbered}

\noindent Hiermit erkläre ich,

* dass keine Zusammenarbeit mit gewerblichen Promotionsberatern stattfand,

* dass ich die Dissertation auf der Grundlage der angegebenen Hilfsmittel und Hilfen selbstständig angefertigt habe,

* dass ich mich nicht anderwärts um einen Doktorgrad beworben habe bzw. einen entsprechenden Doktorgrad besitze,

* dass mir die dem angestrebten Verfahren zugrunde liegende Promotionsordnung der Lebenswissenschaftlichen Fakultät vom 05. März 2015, veröffentlicht im Amtlichen Mitteilungsblatt der Humboldt-Universität zu Berlin Nr. 12/2015 bekannt ist,

* dass die Dissertation oder Teile davon nicht bereits bei einer anderen wissenschaftlichen Einrichtung eingereicht, angenommen oder abgelehnt wurden und

* dass die Grundsätze der Humboldt-Universität zu Berlin zur Sicherung guter wissenschaftlicher Praxis eingehalten wurden.

\noindent
Alexander Enge\newline
Berlin, den 8. November 2024

\clearpage\mbox{}\thispagestyle{empty}\clearpage

\clearpage\mbox{}\thispagestyle{empty}\clearpage
