---
title             :  |
  | Tracking the neural correlates of
  | learning to read with dense-sampling fMRI
shorttitle        : "Learning to read and dense-sampling fMRI"

author: 
  - name          : "Alexander Enge"
    affiliation   : "1,2"
    corresponding : yes
    address       : "Stephanstraße 1a, 04103 Leipzig, Germany"
    email         : "alex_enge@web.de"
  - name          : "Michael A. Skeide"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Max Planck Institute for Human Cognitive and Brain Sciences"
  - id            : "2"
    institution   : "Humboldt-Universität zu Berlin"

authornote: |
  The analysis code for this study is openly available at <https://github.com/SkeideLab/SLANG>.
  
  We have no conflict of interest to disclose.

abstract: |
  Learning to read is a developmental milestone of lifelong importance.
  While many studies have demonstrated the neural correlates of reading in proficient adult readers, neuroimaging studies of learning to read during childhood are relatively scarce, and have not managed to disentangle effects specific to reading instruction from effects of general cognitive development and schooling.
  Furthermore, previous reading research has almost exclusively focused on participants and languages in the Global North.
  To overcome these limitations, we conducted a dense-sampling longitudinal fMRI study with children from a socio-economically disadvantaged background ($N = 15$; age = 5--9 years) who participated in approximately 16 months of reading instruction in their native language (Hindi/Devanagari).
  During each of up to six fMRI scanning sessions, spaced at intervals of approximately 2--3 months, children were presented with spoken and written word-like stimuli, including low-level sensory control stimuli (noise-vocoded speech and false fonts), pseudowords, and words.
  We estimated longitudinal change in BOLD activity amplitude and BOLD activity patterns using linear mixed-effects models.
  We found some changes (both linear and non-linear) in BOLD activity amplitudes for different contrasts and brain areas, including an inverted "u"-shaped pattern of activity increase and decrease for written words compared to visual low-level control stimuli.
  However, none of these changes ocurred in anatomically plausible areas based on previous neuroimaging studies.
  We also found an increase in audio-visual multi-voxel pattern similarity for written words as compared low-level sensory controls in the left ventral occipito-temporal (vOT) cortex.
  There were no reliable changes in word- or pseudoword-related multi-voxel pattern stability.
  These relatively modest findings may be due to limitations in our study design, including a very small sample size and an reading intervention that might have been to short or too weak.
  Nevertheless, we hope that our rationale for this study as well as our whole-brain longitudinal analysis pipeline will inspire future cognitive-neuroscientific research.

keywords          : "reading, literacy, learning, children, fMRI, neuroimaging"

bibliography      : "references.bib"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
numbersections    : false

csl               : "../apa.csl"
documentclass     : "apa6"
classoption       : "jou,10pt,twoside"
output            :
  papaja::apa6_pdf:
    latex_engine  : "lualatex"

header-includes:
  - \geometry{a4paper,margin=15mm}
  - \raggedbottom
  - \usepackage{ragged2e}
  - \linespread{1.0}
  - \usepackage{setspace}
  - \AtBeginEnvironment{tabular}{\doublespacing}
  - \usepackage{makecell}
  - \renewcommand{\cellset}{\renewcommand{\arraystretch}{0.7}}
  - \captionsetup[table]{font={scriptsize,stretch=1.0},labelfont={bf,up},skip=5pt}
  - \captionsetup[figure]{font={scriptsize,stretch=1.0},labelfont={bf,up},skip=5pt}
  - \usepackage[all]{nowidow}
  - \usepackage[export]{adjustbox}
  - \usepackage{newcomputermodern}
---

```{r setup, include=FALSE}
# Load R packages
library("here")
library("knitr")
```

# Introduction

Learning to read is a developmental milestone of lifelong importance.
Reading and writing enables us to exchange messages across time and space, to remember things we would otherwise forget all too easily, to educate ourselves and others, or to get lost in fictional worlds whenever our own one seems a bit much.
Without it, we would likely be missing out on many of humankind's greatest a achievements or, at the very least, you, our dear reader, would not be able to read this manuscript.
It therefore seems fair to say that the process of learning to read, typically taking place through late kindergarten and early primary school education as well as through the help of parents and other caretakers, is a crucial step of children's education and cognitive development.

Children typically pick up knowledge about their first letters informally, for example by being shown how their own name is written or by memorizing the logo of their favorite TV series or cereal brand.
However, at this first, *logographic* stage of reading development [@frith1986; @ehri2005], children are typically not yet aware of the systematic correspondence between individual letters (graphemes) and speech sounds (phonemes), and therefore unable to pronounce or understand novel words.
In this stage, written words are probably perceived similar to other types of visual objects, and novel words that were never encounter before do not elicit any phonological or semantic representation as they would for a trained reader.

Through structured training in kindergarten, primary school, or at home, children advance to the second, *alphabetic* stage of reading development,^[Note, however, that as with any model of developmental "stages" or "phases," there typically is no hard and clear-cut boundary between one "stage" and the next, and there typically is large interindividual variation in the onsets and durations of the "stages."] where they are taught that individual letters are associated with individual speech sounds.
This allows children to process the individual letters of a written word sequentially, convert them to their corresponding speech sounds, and combine those speech sounds to vocalize the entire word.
Since most healthy children will have become proficient in spoken language comprehension long before learning to read (typically with < 3 years of age), they will also be able to access the meaning of the written word based on its assembled spoken word form.
For this way of reading to be successful, children need to possess the understanding and skill to break down words into their constituent phonemes, typically referred to as "phonological awareness."
It is therefore unsurprising that phonological skills first-letter naming, rhyming, and verbal short-term memory are among the best predictors of individual differences in future reading ability [e.g., @melby-lervag2012].
However, this alphabetic form of reading remains somewhat slow and error-prone due to the sequential conversion of letters into speech sounds.

Only in the third and final stage of reading development, the *orthographic* stage, children become able to read words as a whole and to access their meaning directly from the visual word form, without having to take the "detour" of phonological decoding.
Presumably, it is the emergence of this shortcut that substantially reduces word reading times after approximately the first year of reading instruction [e.g., @hasenacker2017] and which makes adult reading so efficient and seemingly effortless.

## Neural correlates of reading

Reading as a cognitive function has to be implemented on a neuroanatomical and functional level in the brain.
To probe which brain areas contribute to different aspects reading in proficient readers, cognitive-neuroscientific methods such as electroencephalography (EEG) and magnetic resonance imaging (MRI) can be used to measure brain activity while participants perform reading tasks.
Due to its comparatively high spatial resolution (on the order of a few millimeters), functional magnetic resonance imaging (fMRI), which measures changes in blood oxygen level as a proxy of local neuronal activity, can be used to isolate which brain areas which are more active during reading than during other control tasks.

One well-replicated finding using this methodology is that processing written words engages a relatively circumscribed region on the left ventral occipito-temporal (vOT) cortex.
This region, typically referred to as the visual word form area [VWFA\; e.g., @cohen2000; @dehaene2002; @mccandliss2003; @dehaene2011], is thought to be a purely visual and pre-lexical area that identifies written words based on its lower-level visual shapes.
There is converging evidence that anomalies in, damage to, and stimulation of the VWFA can cause reading difficulties [@hillis2005; @hirshorn2016; @brem2020; but see also @price2003].
Furthermore, the VWFA is often described as a prime example of the neuronal recycling hypothesis [@dehaene2007], according to which "modern" cognitive functions such as reading,^[The first known scripts appeared approximately 3000--5000 B.C.E [e.g., @houston2004].] for which not enough evolutionary time has passed to develop dedicated brain circuitry, repurpose brain areas with similar but evolutionarily older functions.
In the case of reading, the VWFA may reuse regions that had previously been specialized for recognizing other complex visual object categories such as faces, limbs, or tools [@dehaene2015; @kubota2023; @nordt2021; but see also @coltheart2014].

The brain areas involved in reading beyond the early stages of visual word form recognition are less well understood, presumably owing to large differences in task design between studies.
A meta-analysis of fMRI study [@murphy2019] found that beyond the VWFA, single word reading reliable engages a left-lateralized set of regions including the left inferior frontal and left superior and middle temporal gyri, all of which are known to be involved in phonological and semantic processing of spoken language.
However, it remains an open question which brain areas serve as the interface between visual processing (word form recognition in the VWFA) and language processing (phonological and semantic processing in the left perisylvian language network) during reading.
One candidate for the visual--phonological interface (linking written letters to speech sounds) is the left posterior superior temporal gyrus (pSTS), as it has been show to integrate auditory and visual information when both are presented concurrently, e.g., during audio-visual letter perception or lip reading tasks [e.g., @blau2010; @calvert1997; @vanatteveldt2004; @wilson2018].^[Note that if the left pSTS does indeed play a role in reading, this could be viewed as another case of neuronal recycling, is its "new" skill of linking written letters to speech sounds may stem from its "original" skill to link lip movements to speech sounds.]
One candidate for the visual--semantic interface is the left middle fusiform cortex (lmFFC), which lies anterior the high-level visual object recognition areas (including the VWFA) on the ventral surface of the occipital and temporal lobes.
Using depth electrodes for recording and stimulation in epileptic patients,^[Unfortunately, the depth of this region and the associated signal dropout makes it hard to pick up BOLD activity changes in this region with fMRI [@embleton2010; @liu2016a].] it has been shown that this region is active for lexical retrieval from both auditory (spoken words) and visual (written words and object images) input [e.g., @forseth2018; @woolnough2020; @woolnough2022].

## Neural correlates of learning to read

Compared to hundreds if not thousands of studies in proficient adult readers, there has been a lot less research on how the neural correlates of reading develop in beginning readers.
There are multiple reasons for this:
(a) It is much harder to recruit children (and their families) as compared to undergraduate students who depend on obtaining course credit or monetary compensation;
(b) Children in the relevant age range (late kindergarten to early primary school; typically ~5--8 years of age) have a shorter attention span and show more in-scanner head movement, leading to fewer usable scans, lower data quality in usable scans, and fewer data points per scan; and
(c) Accurately tracking the development of brain structure and function in individual children requires a longitudinal study design, which is very costly and demanding for study participants, typically leading to small sample sizes.
Nevertheless, a few studies exist that have managed to obtain multiple longitudinal scans of children's brain activity as they are learning to read.

@dehaene-lambertz2018 scanned scanned ten 6-year-old children longitudinally throughout the first year of schooling (6--7 scans per child) and presented them with visual objects from different object categories, including written words.
Behaviorally, they found that reading performance (knowledge of grapheme--phoneme relations and number of words read per minutes in a standardized reading test) increased sharply during the first months of schooling.
In the fMRI, they found that selectivity for words in the VWFA was not present at the beginning of the study but quickly emerged within less than 6 months in most of the children.
Interestingly, the activation strength in the VWFA and other word-selective areas appeared to follow a curvilinear inverted "u"-shaped pattern, with a quick rise in BOLD activation strength in the first half of the study followed by a slight decline in the second half.
Such a pattern would be predicted by the "expansion and renormalization" model of brain plasticity [@wenger2017], according to which a novel skill initially requires more resources (in terms of number of voxels or BOLD activation amplitude) but later on becomes more efficient and automatized.
Regarding the neuronal recycling hypothesis, @dehaene-lambertz2018 found that the voxels that would later form the VWFA in individual children were weakly tuned to a different object category at the beginning of the study, namely tools.
However, a few shortcomings of this study need to be noted:
(a) The sample size (both in terms of number of children and number of scans per child) was very small, therefore leading to low statistical power to detect all but very large effects [see also @button2013; @ioannidis2005; @szucs2017];
(b) Their analysis of variance (ANOVA) model did not take into account the longitudinal nature of the data, with repeated measures of the same participants likely being positively correlated with one another;
(c) It is unclear if the changes observed in this study were caused by reading instruction in particular or by other aspects of schooling or general cognitive development, since there was no non-reading control group (for obvious practical and ethical reasons); and
(d) The study design captured only the very first stage of reading, namely visual word form recognition, while it remains an open question how visual word forms get linked to other aspects of (spoken) language comprehension, namely speech sounds (phonology) and word meaning (semantics).

## Cultural biases in reading research

Most of psychological and cognitive-neuroscientific research is carried out in countries of the Global North, especially in Western Europe and Northern America.
Within these countries, study participants are not sampled at random, but typically come from economically and educationally privileged social backgrounds (e.g., psychology students).
This restriction of study samples can limit the generalizability of research findings, as even basic and presumably "universal" effects such as some perceptual illusions do not necessarily replicate in participants from different cultural and socio-economic backgrounds [@blasi2022; @henrich2010].

In reading research, this problem is potentiated by the fact that different cultures developed---sometimes radically---different writing systems.
Due to the concentration of research funds and technology in the Global North, research on reading, its development, and its neural correlates has almost exclusively been carried out in languages with *alphabetic* writing systems such as English, German, or French.
In these writing systems, there is a corresponds^[Though the tightness/reliability of this correspondence differing between relatively shallow (transparent) orthographies such as Spanish, Italian, or German, and relatively deep (opaque) orthographies such as English and French] between individual units of written language (graphemes) and very small units of spoken language (phonemes).
On the other end of the spectrum are *logographic* writing systems (such Chinese), in which individual graphemes correspond to large units of spoken languages, namely entire words or concepts (morphemes).
In between these two extremes sit *syllabic* writing systems, in which individual graphemes correspond to intermediate units of spoken languages, namely sublexical consonant--vowel combinations (syllables).
A special case are *alphasyllabic* languages (also "abugidas"; such as the Hindi writing system Devanagari), in which individual graphemes correspond to consonants with added diacritical marks above, below, or next to the letter for vowels.

There is an abundance of research findings on different aspects of reading and its neural correlates in alphabetic languages,^[Unfortunately, most studies implicitly claim generalizability/universality of their findings by not explicitly mentioning the writing system in the title, abstract, or conclusion of the paper.] but only very few studies that used logographic writing systems (typically Chinese) and next to none that used syllabic or alphasyllabic writing systems.
These biases clearly limit the scope of empirical findings and current theories on reading and its development to a small set of regions and languages [@frost2012; @share2008; @share2014; @share2021].

## The present study

Our goal here was to capture the developmental changes in brain activity related to written word processing as children are learning to read.
To this end, we conducted a longitudinal fMRI study with children at 5--9 years of age who received explicit reading instruction for approximately 1.5 years.
Children were scanned at relatively short intervals of approximately 2--3 months for a total of up to 6 scanning sessions per child.
At each scanning session, children were presented with spoken and written words, pseudowords, and low-level sensory control stimuli.
This allowed us to capture intra-individual changes in BOLD activity (as a proxy for local neuronal activity) in response to stimuli that differed in their sensory, phonological, and semantic content.
To overcome the bias towards study participants from the Global North and alphabetic writing systems, we worked with children from socio-economically disadvantaged backgrounds in India who received reading instruction in their native language (Hindi) and alphasyllabic writing system (Devanagari).
Our hypotheses were as follows:

* We expected that BOLD activity in response to written (pseudo-)words would increase as children are learning to read, either in a linear or quadratic (inverted "u"-shaped) pattern.
  This was based on previous findings [e.g., @dehaene-lambertz2018] demonstrating the quick emergence of word selectivity in higher-level visual cortex.

* We expected that in audio-visual integration areas (especially the pSTS and vOT cortex), multi-voxel BOLD activity patterns would become more similar between written and spoken (pseudo-)words as children are learning to read.
  This was based on the intuition that only over the course of the study, children would become able to access the phonological and semantic content of written words.

* We expected that multi-voxel BOLD activity patterns for written (pseudo-)words would become more "stable," i.e., show less session-to-session variation as children are learning to read.
  This was based on the intuition that activity in reading-related areas should become more finely tuned and less noisy towards written words as children become able to decode and comprehend them.

# Methods

## Participants

We initially recruited a total of 32 children from the same village in the region of Uttar Pradesh, India, to participate in the reading intervention.
In a neighboring village, we recruited an additional 25 children to participate in a mathematics intervention, serving as an active control group.
However, data from these children was not analyzed for the purpose of the present manuscript.
The two villages were selected in cooperation with a local non-governmental organization based on there being little to no access to primary school education due to geographic and socio-demographic constraints.
All recruited children had to fulfil the following inclusion criteria:
(1) age between 5 and 9 years at the beginning of the study,
(2) not being able to decode letters and/or read words,
(3) not attending school,
(4) not fulfilling any contraindication for MRI scanning (e.g., no relevant implants or medication),
(5) no hearing and/or vision impairment,
(6) no language impairments, and
(7) no attention deficits.
From the 32 children initially recruited for the reading intervention group, 17 children were excluded because they did not participate in at least two scanning sessions or because they did not meet our cutoff criterion for head motion (framewise displacement greater than 2.4 mm in less than 10% of fMRI volumes) in at least two scanning sessions.
Therefore, the final sample size was $N$ = 15 children.
Of those, 9 identified as female and 6 identified as male. The mean age at the beginning of the study was 7.13 years ($SD$ = 1.25 years, min = 5 years, max = 9 years).

## Study design

The children in this longitudinal study completed regular MRI scanning sessions while participating in a structured reading program (or mathematics program for the control group, not shown in this manuscript).
The first scanning session took place at the beginning of the program and the next scanning sessions (mean = 5.07 sessions per child, $SD$ = 1.53 sessions, min = 2 sessions, max = 6 sessions) were spaced at intervals of approximately 2--3 months (mean = 79.9 days between sessions, $SD$ = 35.8 days, min = 20 days, max = 182 days; see Figure\ \ref{fig:sessions}).
Each scanning session consisted of one localizer scan, one functional MRI run (see experimental design and scanning parameters below), one structural MRI scan (see scanning parameters below), and a series of standardized behavioral tests outside of the scanner (see behavioral testing below).

\begin{figure}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_sessions.pdf}
\caption{
  \label{fig:sessions}\textbf{\emph{Longitudinal data acquisition schedule.}}
  Colored dots represent data acquisition points (MRI and behavioral tests) for children from the reading intervention group (participants ``sub-SA01'' to ``sub-SA32'').
  Gray dots represent data acquisition points (MRI and behavioral tests) from the mathematics control group (participants ``sub-SO01'' to ``sub-SO25''; not reported in this manuscript).
  Gray crosses (``$\times$'') indicate sessions for which MRI data was acquired but was excluded from all further analyses because it exceeded the head motion cutoff (\textgreater 25\% of fMRI volumes with \textgreater 0.5 mm framewise displacement).}
\end{figure}

Colored dots represent data acquisition points (MRI + behavioral tests) for children from the reading intervention group (participants "sub-SA01" to "sub-SA32").
Gray dots represent data acquisition points (MRI + behavioral tests) from the mathematics control group (participants "sub-SO01" to "sub-SO25"; not reported in this manuscript).
Gray crosses ("\times") indicate sessions for which MRI data was acquired but was excluded from all further analyses because it exceeded the head motion cutoff (\textgreater 25% of fMRI volumes with \textgreater 0.5 mm framewise displacement).

The structured intervention for the reading group focused on phonics (i.e., reading and writing the 46 primary Devanagari characters and their correspondence to sublexical consonant--vowel combinations), word decoding (i.e., reading and writing monosyllabic and more complex words), and sentence reading.
The intervention was carried out by a local teacher and involved 2--4 hours of schooling on 5 days per week. Attendance of the classes was checked but not strictly enforced (mean = 3.74 days attended per week).

## Experimental design

At each MRI scanning session, children performed an in-scanner language task with a block design where they were presented with short blocks of stimuli from six different conditions (visual words, auditory words, visual pseudowords, auditory pseudowords, visual low-level controls, and auditory low-level controls).
All words were nouns of masculine grammatical gender, consisting of one or two syllables and three to six phonemes, and belonging to the same semantic category (animals).
Pseudowords were generated from these words by replacing the initial consonant and vowel of the word.
The substituted consonants were within the same articulatory place as the original consonants and the substituted graphemes matched the shape of the original graphemes as closely as possible.
The low-level visual controls were false fonts that were created by rearranging the line segments of each grapheme of a word while preserving the position of the graphemes.
The low-level auditory controls were created by spectrally rotating and noise-vocoding the spoken words.
This was achieved by low-pass filtering the speech signal, inverting its spectrum around a center frequency of 2 kHz, dividing the speech signal into two logarithmically spaced frequency bands, extracting the amplitude envelope in each frequency band, using this envelope to module noise in the same frequency band, and recombining the frequency bands.
All visual stimuli were presented in the middle of the screen in black font on a white background.
The screen was placed behind the scanner bore and projected to the participant via a mirror mounted inside of the scanner.
All auditory stimuli were recorded by a male native Hindi speaker.

In each of 108 blocks (18 per condition), 6 stimuli from the same condition were randomly presented with a duration of 1 s each.
The order of blocks was random and not optimized for design efficiency.
Between subsequent blocks, there was a random pause of 2.55, 3.82, or 5.09 s (equaling 1, 1.5, or 2 times the repetition time [TR]), during which a black fixation cross was presented in the middle of the screen.
The total duration of the experiment was 17:40 min.
To keep children attentive, they were asked to perform a simple target detection task.
For this purpose, a target stimulus was inserted at a random location in 36 out of the 108 blocks.
For visual blocks, this was the photograph of the face of a children’s movie character, and for auditory blocks a short snippet of child-friendly human laughter.
Children were asked to press a button on a MR-compatible button box whenever they saw or heard the target stimulus.
They received auditory feedback in the form of a positive sound (after pressing the button when a target stimulus had appeared) or a negative sound (after not pressing the button when a target stimulus had appeared or after pressing the button when no target stimulus had appeared).
The experiment was programmed and presented using PsychoPy (Version 2021) in Python.

## Behavioral testing

At each session, directly before or after MRI scanning, children completed a set of behavioral tests (together with one of two local research assistants), assessing their reading skills, mathematics skills, working memory, and general cognitive ability.

For reading skills, we used the Middle Screening Tool (MST) version of the Dyslexia Assessment for Languages of India (DALI) test [@rao2015a; @rao2021], with the following subtests:

* Rapid picture naming (time to name 50 object pictures)

* Word reading (number of correct Hindi words read and time taken [50 words])

* Rhyming (number of correctly identified rhyme pairs [12 items with three words each])

* Phoneme replacement (number of correct Hindi words generated by replacing one phoneme [10 items])

* Semantic fluency (number of Hindi words generated from two semantic categories [vegetables and animals] in one minute)

* Verbal fluency (number of Hindi words generated from two initial consonants in one minute)

* Pseudoword reading (number of correctly pronounced pseudowords and time taken [30 pseudowords])

* Reading comprehension (time taken to read a paragraph of text in Hindi + correct responses to comprehension questions [6 items])

* Dictation (number of Hindi words spelled correctly [20 words])

For mathematics skills, we used the Wide Range Achievement Test, Fifth Edition -- India [WRAT5 -- INDIA\; @wilkinson2017a], with the following subtests:

* Oral math (number of questions answered correctly [15 items])

* Math computation (number of math problems solved correctly [40 items])

For working memory, we used the following tests:

* Corsi block-tapping test forward and backward [longest correctly repeated sequence\; @corsi1972; @kessels2000]

* Digit span test forward and backward [longest correctly repeated sequence\; @miller1956]

For general cognitive ability, we used Raven's Colored Progressive Matrices [CPM/CVS India\; 36 items\; @raven2003].

We analyzed the data from each behavioral (sub-)test with a linear mixed-effects model using the MixedModels package [Version 4.25.3\; @bates2024] in Julia [Version 1.10.4\; @bezanson2017].
Each model predicted the raw (sub-)test scores using a fixed intercepts (reflecting the test score at the beginning of the study), a fixed effect of time (number of months since the beginning of the study), a by-participant random intercept and random slope for the effect of time, and their correlation.
We interpreted $p$ values smaller than $.05$ for the fixed effect of time as a statistically significant increase (or decrease) in behavioral test performance.

## MRI scanning parameters

All scanning was conducted on a GE SIGNA Architect 3T MRI machine with a 48 channel head coil.
After a short head localizer scan, there was one functional run, during which children performed the language experiment described above, and one structural run, during which children watched a TV episode or video of their choice.

The functional run was implemented using a gradient echo (GR) echo planar imaging (EPI) sequence with the following parameters: TE = 35 ms, TR = 2.547 s, flip angle = 88°, number of volumes = 420, field of view = 19.2 cm, in-plane matrix size = 80 × 80 voxels, slice thickness = 2.4 mm, gap between slices = 0.2 mm, voxel size = 2.4 × 2.4 × (2.4 + 0.2) mm, slice orientation = axial, phase encoding direction = anterior/posterior, slice order = interleaved/ascending, multiband acceleration (GE HyperBand) factor = 4.
Slices covered the whole brain including the cerebellum.
We did not collect any field maps and did not correct for potential inhomogeneity or spatial distortion.

The structural run was implemented using a T1-weighted MPRAGE sequence with the following parameters: TE = 3.188 ms, TR = 2.30568 s, TI = 900 ms, flip angle = 8°, field of view = 22.4 cm, in-plane matrix size = 256 × 256 voxels, slice thickness = 0.9 mm, no gap between slices, voxel size = 0.875 × 0.875 × 0.9 mm, slice orientation = axial, phase encoding direction = right/left.

## Preprocessing

Results included in this manuscript come from preprocessing performed using *fMRIPrep* 24.0.1 [@fmriprep1; @fmriprep2, RRID:SCR_016216], which is based on *Nipype* 1.8.6 [@nipype1; @nipype2, RRID:SCR_002502].

### Anatomical data preprocessing

A total of 2--6 T1-weighted (T1w) images were found within the input BIDS dataset.
Each T1w image was corrected for intensity non-uniformity (INU) with `N4BiasFieldCorrection` [@n4], distributed with ANTs 2.5.1 [@ants, RRID:SCR_004757].
The T1w-reference was then skull-stripped with a *Nipype* implementation of the `antsBrainExtraction.sh` workflow (from ANTs), using OASIS30ANTs as target template.
Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using `fast` [FSL, RRID:SCR_002823, @fsl_fast].
An anatomical T1w-reference map was computed after registration of 2--6 T1w images (after INU-correction) using `mri_robust_template` [FreeSurfer 7.3.2, @fs_template].
Brain surfaces were reconstructed using `recon-all` [FreeSurfer 7.3.2, RRID:SCR_001847, @fs_reconall], and the brain mask estimated previously was refined with a custom variation of the method to reconcile
ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle [RRID:SCR_002438, @mindboggle].
Volume-based spatial normalization to one standard space (MNI152NLin2009cAsym) was performed through nonlinear registration with `antsRegistration` (ANTs 2.5.1), using brain-extracted versions of both T1w reference and the T1w template.
The following template was were selected for spatial normalization and accessed with *TemplateFlow* [24.2.0, @templateflow]:
*ICBM 152 Nonlinear Asymmetrical template version 2009c* [@mni152nlin2009casym, RRID:SCR_008796\; TemplateFlow ID: MNI152NLin2009cAsym].

### Functional data preprocessing

For each of the 2--6 BOLD runs found per subject (across all tasks and sessions), the following preprocessing was performed.
First, a reference volume was generated, using a custom methodology of *fMRIPrep*, for use in head motion correction.
Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using `mcflirt` [FSL, @mcflirt].
The BOLD reference was then co-registered to the T1w reference using `bbregister` (FreeSurfer) which implements boundary-based registration [@bbr].
Co-registration was configured with six degrees of freedom.
Several confounding time-series were calculated based on the *preprocessed BOLD*: framewise displacement (FD), DVARS and three region-wise global signals.
FD was computed using two formulations following Power (absolute sum of relative motions, @power_fd_dvars) and Jenkinson (relative root mean square displacement between affines, @mcflirt).
FD and DVARS are calculated for each functional run, both using their implementations in *Nipype* [following the definitions by @power_fd_dvars].
The three global signals are extracted within the CSF, the WM, and the whole-brain masks.
Additionally, a set of physiological regressors were extracted to allow for component-based noise correction [*CompCor*, @compcor].
Principal components are estimated after high-pass filtering the *preprocessed BOLD* time-series (using a discrete cosine filter with 128s cut-off) for the two *CompCor* variants: temporal (tCompCor) and anatomical (aCompCor).
tCompCor components are then calculated from the top 2% variable voxels within the brain mask.
For aCompCor, three probabilistic masks (CSF, WM and combined CSF+WM) are generated in anatomical space.
The implementation differs from that of Behzadi et al. in that instead of eroding the masks by 2 pixels on BOLD space, a mask of pixels that likely contain a volume fraction of GM is subtracted from the aCompCor masks.
This mask is obtained by dilating a GM mask extracted from the FreeSurfer's *aseg* segmentation, and it ensures components are not extracted from voxels containing a minimal fraction of GM.
Finally, these masks are resampled into BOLD space and binarized by thresholding at 0.99 (as in the original implementation).
Components are also calculated separately within the WM and CSF masks.
For each CompCor decomposition, the *k* components with the largest singular values are retained, such that the retained components' time series are sufficient to explain 50 percent of variance across the nuisance mask (CSF, WM, combined, or temporal).
The remaining components are dropped from consideration.
The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file.
The confound time series derived from head motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each [@confounds_satterthwaite_2013].
Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardized DVARS were annotated as motion outliers.
Additional nuisance timeseries are calculated by means of principal components analysis of the signal found within a thin band (*crown*) of voxels around the edge of the brain, as proposed by [@patriat_improved_2017].
The BOLD time-series were resampled onto the following surfaces (FreeSurfer reconstruction nomenclature): *fsnative*, *fsaverage5*.
All resamplings can be performed with *a single interpolation step* by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distortion correction when available,
and co-registrations to anatomical and output spaces).
Gridded (volumetric) resamplings were performed using `nitransforms`, configured with cubic B-spline interpolation.
Non-gridded (surface) resamplings were performed using `mri_vol2surf`
(FreeSurfer).

Many internal operations of *fMRIPrep* use *Nilearn* 0.10.4 [@abraham2014, RRID:SCR_001362], mostly within the functional processing workflow.
For more details of the pipeline, see the section corresponding to workflows in *fMRIPrep*'s documentation (<https://fmriprep.readthedocs.io/en/latest/workflows.html>).

### Copyright Waiver

The above boilerplate text was automatically generated by *fmriprep* with the express intention that users should copy and paste this text into their manuscripts *unchanged*.
It is released under the [CC0](https://creativecommons.org/publicdomain/zero/1.0/) license.

### Additional preprocessing

After running *fMRIPrep*, the preprocessed BOLD fMRI time series data was spatially smoothed with a Gaussian kernel (FWHM = 5.0 mm) and masked using the whole-brain mask computed per-participant by *fMRIPrep*.

## Session-level analysis

Separately for each participant and session, we modeled the preprocessed BOLD fMRI time series data using a mass-univariate general linear model (GLM) implemented in Nilearn [Version 0.10.4\; @abraham2014] for Python [Version 3.9.19\; @vanrossum2009].
At each voxel, the BOLD time series (420 time points) was predicted using a design matrix with the following columns:

* A constant term (1 column).

* One task regressor for each of the six experimental conditions, created by convolving the block design (condition on/off) with the canonical "SPM" hemodynamic response function (HRF) implemented in Nilearn (6 columns).
  These were our regressors of interest.

* The second and third derivatives of each of the task regressors (12 columns), to capture age- and participant-specific deviations from the canonical HRF. 

* The head motions parameters (translations and rotations in three directions) estimated by *fMRIPrep* during head motion correction (6 columns).

* Cosine regressors to high-pass filter the data at 128 s ($\approx$ 0.008 Hz), removing slow-frequency scanner drifts (15 columns).

* The top six anatomical CompCor [@compcor] components estimated by *fMRIPrep* (6 columns).

* Spike regressors for each non-steady state outlier volume at the beginning of the scan as flagged by *fMRIPrep* (0--3 columns, mean = 2.0 columns).

* Spike regressors for each high-motion outlier volume, defined as framewise displacement \textgreater 0.5 mm and flagged by *fMRIPrep* (0--103 columns, mean = 22.8 columns).

For details on how these regressors were computed, see the "Outputs/Confounds" section in *fMRIPrep*'s documentation (<https://fmriprep.org/en/stable/outputs.html#confounds>).

From the fitted model, we computed an effect size map ("beta map") for each of the following contrasts, always reflecting the change in BOLD activation (in arbitrary units) between two experimental conditions:

* Auditory low-level vs. baseline

* Auditory pseudowords vs. baseline

* Auditory pseudowords vs. low-level

* Auditory words vs. baseline

* Auditory words vs. low-level

* Auditory words vs. pseudowords

* Visual low-level vs. baseline

* Visual pseudowords vs. baseline

* Visual pseudowords vs. low-level

* Visual words vs. baseline

* Visual words vs. low-level

* Visual words vs. pseudowords

We refer to all contrasts involving the baseline as "baseline contrasts," which capture the difference in BOLD activity between each different experimental condition and the fixation baseline period between experimental blocks.
Therefore, these contrasts capture not only BOLD activity related to linguistic (phonological and semantic) processing but also BOLD activity related to low-level sensory processing (e.g., visual and auditory processing of the stimuli).
We refer to all other contrasts as "experimental contrasts," which capture the difference in BOLD activity between two different experimental conditions.
The experimental contrasts comparing pseudowords to low-level controls capture BOLD activity related to phonological processing, since pseudowords but not low-level controls contain phonological information, while both do not contain any semantic information.
The experimental contrasts comparing words to pseudowords capture BOLD activity related to semantic processing, since words but not pseudowords contain semantic information, while both contain phonological information.
The experimental contrasts comparing words to low-level controls capture BOLD activity related to both phonological and semantic processing, since words but not low-level controls contain both phonological and semantic information.

## Group-level analysis

### BOLD activity amplitude

To estimate reading related changes in BOLD activity amplitude, we fitted the beta maps from all participants and sessions using a linear mixed-effects model, separately for each contrast (see above).
The dependent variable was the BOLD activation amplitude for a given participant and session, and the predictors were
(1) a fixed intercept, implemented as a column of "1"s and reflecting the BOLD activity at the beginning of the study,
(2) a fixed effect for linear time, implemented as the number of days elapsed since the beginning of the study and reflecting the linear change in BOLD activity due to the reading instruction,
(3) a fixed effect for quadratic time, implemented as the square of linear time and reflecting the nonlinear ("u"-shaped or inverted "u"-shaped) change in BOLD activity due to the reading instruction,
(4) a random intercept, reflecting individual participant’s deviation from the global BOLD activity at the beginning of the study, and random slopes for (5) linear and
(6) quadratic time, reflecting individual participant’s deviations in the linear and nonlinear changes in BOLD activity due to the reading instruction. As is typical in frequentist linear mixed models, one parameter was estimated for each fixed effect (intercept, linear time, and quadratic time) and for each random effect (the standard deviation of the random intercepts and slopes for linear and quadratic time), as well as three correlation parameters between the three pairs of random effects.
The mixed models (one for each contrast) were fitted separately at each voxel inside the brain mask in a mass-univariate fashion.
Model fitting was performed in Julia [Version 1.10.4\; @bezanson2017] using the MixedModels package [Version 4.25.3\; @bates2024].

To correct for multiple comparisons across the 132,215 voxels inside the brain mask, we used the parametric cluster correction algorithm suggested by @cox2017a and @cox2017.
Specifically, we first estimated the spatial smoothness of noise in our dataset by extracting and storing the residual time series from the session-level models (see above), separately for each participant and session.
We then used the `3dFWHMx` function (with the `-acf` option) in AFNI [Version 24.2.01\; @cox1996] to estimate a mixed Gaussian/mono-exponential spatial autocorrelation function with three parameters [@cox2017; @cox2017a\; see also <https://afni.nimh.nih.gov/pub/dist/edu/data/CD.expanded/afni_handouts/afni07_ETAC.pdf>].
We averaged each of these three parameters across participants and sessions to obtain a single spatial autocorrelation function for the entire dataset. 
This function was then fed into the `3dClustSim` function in AFNI to generate novel noise-only maps and estimate a null distribution of cluster sizes. 
Using a cluster-forming voxel-level threshold of $p < .001$ and 10,000 iterations, this resulted in a final cluster-level extent threshold of 19 voxels to control the whole-brain family-wise error (FWE) rate at $p < .05$.
We therefore deemed spatial clusters of BOLD activation statistically significant if they were larger than 19 voxels.
To form clusters, neighboring voxels had to pass the cluster-forming voxel level threshold of $p < .001$ and touch each other with their faces (not just with their edges or nodes; the default "NN1" method in AFNI).

### BOLD activity patterns

We also estimated reading related changes in the between-condition similarity and within-condition stability of BOLD activity patterns using multivariate analysis methods.

First, we investigated if the reading intervention made written word activity patterns more similar to those for spoken word activity patterns.
For this, we used the beta maps from these two conditions and extracted the betas for different regions of interest (ROI; defined below).
We then computed the linear correlation between these vectors and entered these (one value per subject and session) into a linear mixed-effects model, separately for each ROI.
We specified the linear mixed-effects model in the same way as described above, with fixed and random effects for the intercept, the linear effect of time, and the quadratic effect of time.
We repeated the same analysis for the correlation of activity patterns between written and spoken pseudowords.
However, it is important to note at this point that with our block design, we are not comparing the activity patterns in response to individual items (e.g., the written word "monkey" and the spoken word "monkey"), but rather the general patterns of activity when processing written and spoken words.
Though arguably more meaningful, the former comparison is not possible as our trials within each block were presented faster (1 s) than our TR (2.647 s), and we were therefore unable to obtain a beta map for each individual item.

Second, we investigated if the reading intervention made written word activity more "stable," i.e., more consistent across repeated presentations of written words within the same session.
For this, we re-estimated the session-level beta maps as described above but with separate task regressors for each block (108 columns) instead of each condition (6 columns).
We then took the beta map for each written word block (18 maps) and extracted the betas for different ROIs (defined below).
We then computed the linear correlation between each pair of blocks (153 pairs) and averaged these to obtain a single stability (correlation) value for each participant and session.
We entered these values into a linear-mixed effects model, separately for each ROI and specified in the same way as described above.
We repeated the same analysis for the stability of activity patterns for written pseudowords.
We also performed this analysis for spoken words and spoken pseudowords even though they do not directly pertain to our hypotheses.
For all multivariate analyses, we used the same set of anatomically and functionally defined regions of interest.
The anatomically defined regions of interest were the posterior superior temporal sulcus (pSTS), defined as the union of regions STSda, STSdp, STSvp, and STSva from the @glasser2016 atlas, and the ventral occipto-temporal cortex (vOT), defined as the union of regions V8, FFC, and VVC from the @glasser2016 atlas.
The functionally defined regions were different for each condition:
For the similarity between written and spoken words, we used the visual clusters from the "Written words vs. baseline" contrast and the auditory clusters from the "Spoken words vs. baseline" clusters. 
Likewise, for the similarity between written and spoken pseudowords, we used the visual clusters from the "Written pseudowords vs. baseline" contrast and the auditory clusters from the "Spoken pseudowords vs. baseline" clusters.
For the within-condition pattern stability analysis, we used the visual or auditory clusters from that specific condition (e.g., for the stability analysis for written words, we used the visual clusters from the "Written words vs. baseline" contrast).

## Data and code availability

The data from this study are available upon request from the last author.
The code for the experiment, preprocessing, and statistical analysis are available at <https://github.com/SkeideLab/SLANG>.

# Results

## Behavioral testing

At each session throughout the study, children completed a set of behavioral tests to assess their reading skills, mathematics skills, working memory, and general cognitive ability (see Figure\ \ref{fig:behavior}).

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_behavior.pdf}
\caption{
  \label{fig:behavior}\textbf{\emph{Results from behavioral tests of reading skills, mathematics skills, working memory, and general cognitive ability.}}
  Each subplot shows the results (raw scores) from one subtest.
  Colored dots and lines show the results from individual children over the course of the study.
  Note that for subtests where the score is the number (\#) of correct responses, higher values are considered better, whereas for subtests where the score is the number of seconds (s) taken to complete the test, lower values are considered better.
  DALI = Dyslexia Assessment for Languages of India,
  WRAT = Wide Range Achievement Test,
  WM = working memory (Corsi block test),
  CPM = Raven's Colored Progressive Matrices.}
\end{figure*}

The following (sub-)test showed a significant change in test scores over the course of the study:

* Word reading accuracy ($b = 0.42$ more correct words per month, $p = .025$)

* Phoneme replacement ($b = 0.18$ more items correct per month, $p = 0.011$)

* Semantic fluency ($b = 0.26$ more generated words per month, $p = 0.015$)

* Pseudoword reading ($b = 0.20$ more correct pseudowords per month, $p = 0.042$)

* Reading comprehension ($b = 0.06$ more questions answered correctly per month, $p = 0.042$)

* Dictation ($b = 0.27$ more correctly spelled words per month, $p = 0.032$)

* Oral math ($b = 0.07$ more items solved correctly per month, $p = 0.017$)

* Math computation ($b = 0.30$ more items solved correctly per month, $p = 0.018$)

* Digit span forward ($b = 0.08$ more digits per month, $p = 0.017$)

All other (sub-)tests (picture naming, word reading time, rhyming, verbal fluency, pseudoword reading time, passage reading time, Corsi block forward and backward, digit span backward, Raven's Colored Progressive Matrices) showed no statistically significant change over the course of the study (all $p$s > .072).
For all (sub-)tests, there was large interindividual variation between children, both in their initial scores as well as in their longitudinal change over time (see Figure\ \ref{fig:behavior}).

## BOLD activity amplitude

### Auditory baseline contrasts

At the beginning of the study (intercept), spoken low-level controls, spoken pseudowords, and spoken words elicited BOLD activity in the left and right auditory cortex (superior temporal gyrus) and in the left inferior frontal gyrus (see orange clusters in Figures \ref{fig:univariate-noise}A, \ref{fig:univariate-pseudo}A, and \ref{fig:univariate-words}A), as well as a few a BOLD deactivations in parietal and occipital areas (see blue clusters in Figures \ref{fig:univariate-noise}A, \ref{fig:univariate-pseudo}A, and \ref{fig:univariate-words}A).
For spoken low-level controls, there was positive linear change in BOLD activity over the course of the study in one small cluster near the right motor cortex (see orange cluster in Figure\ \ref{fig:univariate-noise}C) and positive quadratic (i.e., "u"-shaped) change in BOLD activity three small clusters in the right middle/inferior temporal lobe and in the right parietal lobe (see orange clusters in Figure\ \ref{fig:univariate-noise}E).
For spoken pseudowords, there was both negative linear and positive quadratic (i.e., "u"-shaped) change in one small cluster in the right parietal lobe (see blue cluster in Figure\ \ref{fig:univariate-pseudo}C and orange cluster in Figure\ \ref{fig:univariate-pseudo}E).
For spoken words, there was positive linear change in BOLD activity over the course of the study in one small cluster in the left inferior parietal lobe (see orange cluster in Figure\ \ref{fig:univariate-words}C), as well as positive quadratic change (i.e., "u"-shaped) change in one small cluster in the right superior parietal lobe (see orange cluster in Figure\ \ref{fig:univariate-words}E).

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-noise_univariate.pdf}
\caption{
  \label{fig:univariate-noise}\textbf{\emph{BOLD activity in response to low-level sensory controls.}}
  Panels \textbf{A} and \textbf{B} show statistically significant clusters (cluster-forming voxel threshold \(p < .001\), uncorrected; cluster size threshold \(p < .05\), FWE-corrected) for the contrast of low-level sensory control blocks vs. fixation baseline at the beginning of the study (intercept; time = 0 months).
  Panels \textbf{C} and \textbf{D} show statistically significant clusters for the linear change in BOLD activity for the same contrast over the course of the study and panels \textbf{E} and \textbf{F} show statistically significant clusters for the quadratic change in BOLD activity for the same contrast over the course of the study.
  Panels \textbf{A}, \textbf{C}, and \textbf{E} show the auditory modality (noise-vocoded speech vs. fixation baseline) and panels \textbf{B}, \textbf{D}, and \textbf{F} show the visual modality (false fonts vs. fixation baseline).
  In all panels \textbf{A}--\textbf{F}, clusters with positive BOLD amplitude (low-level \textgreater{} baseline) are shown in orange and clusters with negative BOLD amplitude (low-level \textless{} baseline) are shown in blue.
  Clusters with higher voxel-level peak statistics are shown in brighter colors.
  Panels \textbf{G} and \textbf{H} show individual participants' change in BOLD amplitude over time (colored dots and lines) as well as the best fitting linear mixed model (dashed black line) for the largest significant clusters in panels \textbf{E} and \textbf{F}, respectively, as indicated by the black asterisk (*) next to the cluster.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-pseudo_univariate.pdf}
\caption{
  \label{fig:univariate-pseudo}\textbf{\emph{BOLD activity in response to pseudowords.}}
  Panels \textbf{A} and \textbf{B} show statistically significant clusters (cluster-forming voxel threshold \(p < .001\), uncorrected; cluster size threshold \(p < .05\), FWE-corrected) for the contrast of pseudoword blocks vs. fixation baseline at the beginning of the study (intercept; time = 0 months).
  Panels \textbf{C} and \textbf{D} show statistically significant clusters for the linear change in BOLD activity for the same contrast over the course of the study and panels \textbf{E} and \textbf{F} show statistically significant clusters for the quadratic change in BOLD activity for the same contrast over the course of the study.
  Panels \textbf{A}, \textbf{C}, and \textbf{E} show the auditory modality (spoken pseudowords vs. fixation baseline) and panels \textbf{B}, \textbf{D}, and \textbf{F} show the visual modality (written pseudowords vs. fixation baseline).
  In all panels \textbf{A}--\textbf{F}, clusters with positive BOLD amplitude (pseudowords \textgreater{} baseline) are shown in orange and clusters with negative BOLD amplitude (pseudowords \textless{} baseline) are shown in blue.
  Clusters with higher voxel-level peak statistics are shown in brighter colors.
  Panels \textbf{G} and \textbf{H} show individual participants' change in BOLD amplitude over time (colored dots and lines) as well as the best fitting linear mixed model (dashed black line) for the largest significant clusters in panels \textbf{E} and \textbf{F}, respectively, as indicated by the black asterisk (*) next to the cluster.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-words_univariate.pdf}
\caption{
  \label{fig:univariate-words}\textbf{\emph{BOLD activity in response to words.}}
  Panels \textbf{A} and \textbf{B} show statistically significant clusters (cluster-forming voxel threshold \(p < .001\), uncorrected; cluster size threshold \(p < .05\), FWE-corrected) for the contrast of word blocks vs. fixation baseline at the beginning of the study (intercept; time = 0 months).
  Panels \textbf{C} and \textbf{D} show statistically significant clusters for the linear change in BOLD activity for the same contrast over the course of the study and panels \textbf{E} and \textbf{F} show statistically significant clusters for the quadratic change in BOLD activity for the same contrast over the course of the study.
  Panels \textbf{A}, \textbf{C}, and \textbf{E} show the auditory modality (spoken words vs. fixation baseline) and panels \textbf{B}, \textbf{D}, and \textbf{F} show the visual modality (written words vs. fixation baseline).
  In all panels \textbf{A}--\textbf{F}, clusters with positive BOLD amplitude (words \textgreater{} baseline) are shown in orange and clusters with negative BOLD amplitude (words \textless{} baseline) are shown in blue.
  Clusters with higher voxel-level peak statistics are shown in brighter colors.
 Panels \textbf{G} and \textbf{H} show individual participants' change in BOLD amplitude over time (colored dots and lines) as well as the best fitting linear mixed model (dashed black line) for the largest significant clusters in panels \textbf{E} and \textbf{F}, respectively, as indicated by the black asterisk (*) next to the cluster.}
\end{figure*}

### Visual baseline contrasts

At the beginning of the study (intercept), written low-level controls, written pseudowords, and written words elicited BOLD activity in the left right occipital and inferior temporal cortices (see orange clusters in Figures \ref{fig:univariate-noise}B, \ref{fig:univariate-pseudo}B, and \ref{fig:univariate-words}B) as well as BOLD deactivations in the left and right medial occiptial lobes (see blue clusters in Figures \ref{fig:univariate-noise}B, \ref{fig:univariate-pseudo}B, and \ref{fig:univariate-words}B).
For written low-level controls, there was no linear change in BOLD activity over the course of the study (see Figure\ \ref{fig:univariate-noise}D) but positive quadratic (i.e., "u"-shaped) change in BOLD activity two small clusters in the right occipital lobe (see orange clusters in Figure\ \ref{fig:univariate-noise}F).
For written pseudowords, there was negative linear change in one small cluster in the left inferior frontal lobe (see left frontal blue cluster in Figure\ \ref{fig:univariate-pseudo}D) and both negative linear change and positive quadratic (i.e., "u"-shaped) change in one small cluster in the right occipital lobe (see posterior blue cluster in Figure\ \ref{fig:univariate-pseudo}D and orange cluster in Figure\ \ref{fig:univariate-pseudo}F).
For written words, there was both negative linear and positive quadratic (i.e., "u"-shaped) change in one small cluster in the right occipital lobe (see small blue cluster in Figure\ \ref{fig:univariate-words}D and posterior orange cluster in Figure\ \ref{fig:univariate-words}F).
Additionally, there was positive quadratic (i.e., "u"-shaped) change in one small cluster in the right parietal lobe (see anterior orange cluster in Figure\ \ref{fig:univariate-words}F).

### Auditory experimental contrasts

At the beginning of the study (intercept), the difference between spoken pseudowords and spoken low-level controls elicited widespread BOLD activity mainly in the left and right auditory cortex (superior temporal gyrus; see large orange clusters in Figure\ \ref{fig:univariate-pseudo-minus-noise}A).
There was negative linear change in BOLD activity over the course of the study in one small cluster in the right anterior inferior parietal lobe (see small blue cluster in Figure\ \ref{fig:univariate-pseudo-minus-noise}C).
There was positive quadratic change in BOLD activity over the course of the study in one small area in the left superior parietal lobe (see small orange cluster in Figure\ \ref{fig:univariate-pseudo-minus-noise}E).

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-pseudo-minus-noise_univariate.pdf}
\caption{
  \label{fig:univariate-pseudo-minus-noise}\textbf{\emph{BOLD activity in response to pseudowords vs. low-level controls.}}
  Panels \textbf{A} and \textbf{B} show statistically significant clusters (cluster-forming voxel threshold \(p < .001\), uncorrected; cluster size threshold \(p < .05\), FWE-corrected) for the contrast of pseudoword blocks vs. low-level control blocks at the beginning of the study (intercept; time = 0 months).
  Panels \textbf{C} and \textbf{D} show statistically significant clusters for the linear change in BOLD activity for the same contrast over the course of the study and panels \textbf{E} and \textbf{F} show statistically significant clusters for the quadratic change in BOLD activity for the same contrast over the course of the study.
  Panels \textbf{A}, \textbf{C}, and \textbf{E} show the auditory modality (spoken pseudowords vs. noise-vocoded speech) and panels \textbf{B}, \textbf{D}, and \textbf{F} show the visual modality (written pseudowords vs. false fonts).
  In all panels \textbf{A}--\textbf{F}, clusters with positive BOLD amplitude (pseudowords \textgreater{} low-level) are shown in orange and clusters with negative BOLD amplitude (pseudowords \textless{} low-level) are shown in blue.
  Clusters with higher voxel-level peak statistics are shown in brighter colors.
  Panels \textbf{G} and \textbf{H} show individual participants' change in BOLD amplitude over time (colored dots and lines) as well as the best fitting linear mixed model (dashed black line) for the largest significant clusters in panels \textbf{E} and \textbf{F}, respectively, as indicated by the black asterisk (*) next to the cluster.}
\end{figure*}

At the beginning of the study (intercept), the difference between spoken words and spoken low-level controls elicited widespread BOLD activity mainly in the left and right auditory cortex (superior temporal gyrus; see large orange clusters in Figures \ref{fig:univariate-words-minus-noise}A).
There was no evidence for linear or quadratic change over the course of the study in any brain areas (see Figure\ \ref{fig:univariate-words-minus-noise}C and E).

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-words-minus-noise_univariate.pdf}
\caption{
  \label{fig:univariate-words-minus-noise}\textbf{\emph{BOLD activity in response to words vs. low-level controls.}}
  Panels \textbf{A} and \textbf{B} show statistically significant clusters (cluster-forming voxel threshold \(p < .001\), uncorrected; cluster size threshold \(p < .05\), FWE-corrected) for the contrast of word blocks vs. low-level control blocks at the beginning of the study (intercept; time = 0 months).
  Panels \textbf{C} and \textbf{D} show statistically significant clusters for the linear change in BOLD activity for the same contrast over the course of the study and panels \textbf{E} and \textbf{F} show statistically significant clusters for the quadratic change in BOLD activity for the same contrast over the course of the study.
  Panels \textbf{A}, \textbf{C}, and \textbf{E} show the auditory modality (spoken words vs. noise-vocoded speech) and panels \textbf{B}, \textbf{D}, and \textbf{F} show the visual modality (written words vs. false fonts).
  In all panels \textbf{A}--\textbf{F}, clusters with positive BOLD amplitude (words \textgreater{} low-level) are shown in orange and clusters with negative BOLD amplitude (words \textless{} low-level) are shown in blue.
  Clusters with higher voxel-level peak statistics are shown in brighter colors.
  Panel \textbf{G} shows individual participants' change in BOLD amplitude over time (colored dots and lines) as well as the best fitting linear mixed model (dashed black line) for the largest significant cluster in panel \textbf{F}, as indicated by the black asterisk (*) next to the cluster.}
\end{figure*}

The difference between spoken words and spoken pseudowords did not elicit any reliable BOLD activity at the beginning of the study and showed no evidence for change over the course of the study (see Figure\ \ref{fig:univariate-words-minus-pseudo}A, C, and E).

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-words-minus-pseudo_univariate.pdf}
\caption{
  \label{fig:univariate-words-minus-pseudo}\textbf{\emph{BOLD activity in response to words vs. pseudowords.}}
  Panels \textbf{A} and \textbf{B} show statistically significant clusters (cluster-forming voxel threshold \(p < .001\), uncorrected; cluster size threshold \(p < .05\), FWE-corrected) for the contrast of word blocks vs. pseudoword blocks at the beginning of the study (intercept; time = 0 months).
  Panels \textbf{C} and \textbf{D} show statistically significant clusters for the linear change in BOLD activity for the same contrast over the course of the study and panels \textbf{E} and \textbf{F} show statistically significant clusters for the quadratic change in BOLD activity for the same contrast over the course of the study.
  Panels \textbf{A}, \textbf{C}, and \textbf{E} show the auditory modality (spoken words vs. spoken pseudowords) and panels \textbf{B}, \textbf{D}, and \textbf{F} show the visual modality (written words vs. written pseudowords).
  In all panels \textbf{A}--\textbf{F}, clusters with positive BOLD amplitude (words \textgreater{} pseudowords) are shown in orange and clusters with negative BOLD amplitude (words \textless{} pseudowords) are shown in blue.
  Clusters with higher voxel-level peak statistics are shown in brighter colors.}
\end{figure*}

### Visual experimental contrasts

At the beginning of the study (intercept), the difference between written pseudowords and written low-level controls elicited BOLD activity in one cluster in the right dorso-lateral prefrontal cortex (see orange cluster in Figure\ \ref{fig:univariate-pseudo-minus-noise}B) and BOLD deactivation in one cluster in the right secondary visual cortex (see blue cluster in Figure\ \ref{fig:univariate-pseudo-minus-noise}B).
There was negative linear change over the course of the study in the right dorso-lateral prefrontal cortex (see blue cluster in Figure\ \ref{fig:univariate-pseudo-minus-noise}D).

The difference between written words and written low-level controls did not elicit any reliable BOLD activity at the beginning of the study (see Figure\ \ref{fig:univariate-words-minus-noise}B) but there was negative linear change over the course of the study in one cluster in the left supplementary motor area (see small blue cluster in Figure\ \ref{fig:univariate-words-minus-noise}D) and negative quadratic (inverted "u"-shaped) change over the course of the study in one cluster in the left posterior medial wall (near the ventral posterior cingulate cortex; see the blue cluster in Figure\ \ref{fig:univariate-words-minus-noise}F).

The difference between written words and written pseudowords elicited BOLD deactivation in one cluster in the right dorso-lateral prefrontal cortex (see blue cluster in Figure\ \ref{fig:univariate-words-minus-pseudo}B).
There was no evidence for linear or quadratic change over the course of the study (see Figure\ \ref{fig:univariate-words-minus-pseudo}D and F).

## BOLD activity patterns

### Audio-visual pattern similarity

For all baseline contrasts (low-level vs. baseline, pseudowords vs. baseline, and words vs. baseline) and ROIs, the auditory and visual BOLD response patterns were correlated significantly at the beginning of the study (intercept; all $p\text{s} < .001$; see Figures \ref{fig:similarity-noise}--\ref{fig:similarity-words}).
This is expected given that the exact same fixation baseline periods were used as the comparison condition for both the auditory and visual baseline contrasts.
There was no evidence for linear change (all $p\text{s} > .072$) in pattern similarity over the course of the study for any contrast pair or ROI.

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-noise_similarity.pdf}
\caption{
  \label{fig:similarity-noise}\textbf{\emph{Audio-visual pattern similarity for low-level controls.}}
  Each panel \textbf{A}--\textbf{H} shows the development of audio-visual pattern similarity for the contrast of low-level sensory control blocks vs. fixation baseline over the course of the study in one region of interest (ROI).
  Audio-visual pattern similarity was computed by correlating the beta weights of all voxels inside the ROI for the auditory contrast (noise-vocoded speech vs. fixation baseline) with those for the visual condition (false fonts vs. fixation baseline).
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-noise}).
  Colored dots and lines indicate the pattern similarities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-pseudo_similarity.pdf}
\caption{\label{fig:similarity-pseudo}\textbf{\emph{Audio-visual pattern similarity for pseudowords.}}
  Each panel \textbf{A}--\textbf{H} shows the development of audio-visual pattern similarity for the contrast of pseudoword blocks vs. fixation baseline over the course of the study in one region of interest (ROI).
  Audio-visual pattern similarity was computed by correlating the beta weights of all voxels inside the ROI for the auditory contrast (spoken pseudowords vs. fixation baseline) with those for the visual condition (written pseudowords vs. fixation baseline).
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-pseudo}).
  Colored dots and lines indicate the pattern similarities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-words_similarity.pdf}
\caption{\label{fig:similarity-words}\textbf{\emph{Audio-visual pattern similarity for words.}}
  Each panel \textbf{A}--\textbf{H} shows the development of audio-visual pattern similarity for the contrast of word blocks vs. fixation baseline over the course of the study in one region of interest (ROI).
  Audio-visual pattern similarity was computed by correlating the beta weights of all voxels inside the ROI for the auditory contrast (spoken words vs. fixation baseline) with those for the visual condition (written words vs. fixation baseline).
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-words}).
  Colored dots and lines indicate the pattern similarities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

For all experimental contrasts (pseudowords vs. low-level, words vs. low-level, words vs. pseudowords) and ROIs, the auditory and visual BOLD response patterns were not significantly correlated at the beginning of the study (intercept; all $p\text{s} > .090$; see Figures \ref{fig:similarity-pseudo-minus-noise}--\ref{fig:similarity-words-minus-pseudo}).
There was a linear increase in audio-visual pattern similarity over the course of the study for the contrast of words versus low-level controls in the left ventral occipito-temporal (vOT) ROI ($b = 0.013$, $p = 0.034$, see Figure\ \ref{fig:similarity-words-minus-noise}C).
There was no evidence for linear change in audio-visual pattern similarity over the course of the study for any of the other experimental contrasts and ROIs (all $p\text{s} > .121$; see Figures \ref{fig:similarity-pseudo-minus-noise}--\ref{fig:similarity-words-minus-pseudo}).

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-pseudo-minus-noise_similarity.pdf}
\caption{\label{fig:similarity-pseudo-minus-noise}\textbf{\emph{Audio-visual pattern similarity for pseudowords vs. low-level controls.}}
  Each panel \textbf{A}--\textbf{H} shows the development of audio-visual pattern similarity for the contrast of pseudoword blocks vs. low-level sensory control blocks over the course of the study in one region of interest (ROI).
  Audio-visual pattern similarity was computed by correlating the beta weights of all voxels inside the ROI for the auditory contrast (spoken pseudowords vs. noise-vocoded speech) with those for the visual condition (written pseudowords false fonts).
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-pseudo-minus-noise}).
  Colored dots and lines indicate the pattern similarities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-words-minus-noise_similarity.pdf}
\caption{\label{fig:similarity-words-minus-noise}\textbf{\emph{Audio-visual pattern similarity for words vs. low-level controls.}}
  Each panel \textbf{A}--\textbf{H} shows the development of audio-visual pattern similarity for the contrast of word blocks vs. low-level sensory control blocks over the course of the study in one region of interest (ROI).
  Audio-visual pattern similarity was computed by correlating the beta weights of all voxels inside the ROI for the auditory contrast (spoken words vs. noise-vocoded speech) with those for the visual condition (written words false fonts).
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-words-minus-noise}).
  Colored dots and lines indicate the pattern similarities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-words-minus-pseudo_similarity.pdf}
\caption{\label{fig:similarity-words-minus-pseudo}\textbf{\emph{Audio-visual pattern similarity for words vs. pseudowords.}}
  Each panel \textbf{A}--\textbf{H} shows the development of audio-visual pattern similarity for the contrast of words blocks vs. pseudoword blocks over the course of the study in one region of interest (ROI).
  Audio-visual pattern similarity was computed by correlating the beta weights of all voxels inside the ROI for the auditory contrast (spoken words vs. spoken pseudowords) with those for the visual condition (written words written pseudowords).
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-words-minus-pseudo}).
  Colored dots and lines indicate the pattern similarities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

### Within-condition pattern stability

For almost all conditions and ROIs, blocks from the same condition were positively correlated at the beginning of the study (intercept; all $p\text{s} < .029$; see Figures \ref{fig:stability-audios-noise}--\ref{fig:stability-images-words}), except for written low-level controls in the left spoken low-level controls ROI ($p = 0.570$, see Figure\ \ref{fig:stability-images-noise}E), for written pseudowords in the left spoken pseudoword ROI ($p = 0.346$, see Figure\ \ref{fig:stability-images-pseudo}E) and in the right spoken pseudoword ROI ($p = 0.084$, see Figure\ \ref{fig:stability-images-pseudo}F), and for written words in the left spoken word ROI ($p = 0.201$, see Figure\ \ref{fig:stability-images-words}E).
There was a decrease in BOLD pattern stability for spoken pseudowords in the left pSTS ROI ($b = -0.003$, $p = 0.029$, see Figure\ \ref{fig:stability-audios-pseudo}A) and in the right pSTS ROI ($b = -0.003$, $p = 0.039$, see Figure\ \ref{fig:stability-audios-pseudo}B), as well as for spoken words in the left pSTS ROI ($b = -0.002$, $p = 0.040$, see Figure\ \ref{fig:stability-audios-words}A), in the right pSTS ROI ($b = -0.004$, $p = 0.013$, see Figure\ \ref{fig:stability-audios-words}B), and in the right spoken words ROI ($b = -0.006$, $p = 0.046$, see Figure\ \ref{fig:stability-audios-words}F).
Note that we did not have any *a priori* hypothesis for longitudinal changes in pattern stability for these auditory conditions but instead expected changes in pattern stability in the written pseudoword and word conditions, which we did not observe here.

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-audios-noise_stability.pdf}
\caption{
  \label{fig:stability-audios-noise}\textbf{\emph{Pattern stability for spoken low-level controls.}}
  Each panel \textbf{A}--\textbf{H} shows the development of within-condition pattern similarity for the contrast of spoken low-level sensory control blocks vs. fixation baseline over the course of the study in one region of interest (ROI).
  Pattern stability was computed by estimating beta weights separately for each spoken low-level sensory control block in the experiment, then computing all pairwise correlations of these beta weights inside the ROI, and then averaging all of these pairwise correlations to obtain a single correlation value for each participant and session.
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-noise}).
  Colored dots and lines indicate the pattern stabilities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-images-noise_stability.pdf}
\caption{\label{fig:stability-images-noise}\textbf{\emph{Pattern stability for written low-level controls.}}
  Each panel \textbf{A}--\textbf{H} shows the development of within-condition pattern similarity for the contrast of written low-level sensory control blocks vs. fixation baseline over the course of the study in one region of interest (ROI).
  Pattern stability was computed by estimating beta weights separately for each written low-level sensory control block in the experiment, then computing all pairwise correlations of these beta weights inside the ROI, and then averaging all of these pairwise correlations to obtain a single correlation value for each participant and session.
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-noise}).
  Colored dots and lines indicate the pattern stabilities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-audios-pseudo_stability.pdf}
\caption{\label{fig:stability-audios-pseudo}\textbf{\emph{Pattern stability for spoken pseudowords.}}
  Each panel \textbf{A}--\textbf{H} shows the development of within-condition pattern similarity for the contrast of spoken pseudoword blocks vs. fixation baseline over the course of the study in one region of interest (ROI).
  Pattern stability was computed by estimating beta weights separately for each spoken pseudoword block in the experiment, then computing all pairwise correlations of these beta weights inside the ROI, and then averaging all of these pairwise correlations to obtain a single correlation value for each participant and session.
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-pseudo}).
  Colored dots and lines indicate the pattern stabilities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-images-pseudo_stability.pdf}
\caption{\label{fig:stability-images-pseudo}\textbf{\emph{Pattern stability for written pseudowords.}}
  Each panel \textbf{A}--\textbf{H} shows the development of within-condition pattern similarity for the contrast of written pseudoword blocks vs. fixation baseline over the course of the study in one region of interest (ROI).
  Pattern stability was computed by estimating beta weights separately for each written pseudoword block in the experiment, then computing all pairwise correlations of these beta weights inside the ROI, and then averaging all of these pairwise correlations to obtain a single correlation value for each participant and session.
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-pseudo}).
  Colored dots and lines indicate the pattern stabilities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-audios-words_stability.pdf}
\caption{\label{fig:stability-audios-words}\textbf{\emph{Pattern stability for spoken words.}}
  Each panel \textbf{A}--\textbf{H} shows the development of within-condition pattern similarity for the contrast of spoken word blocks vs. fixation baseline over the course of the study in one region of interest (ROI).
  Pattern stability was computed by estimating beta weights separately for each spoken word block in the experiment, then computing all pairwise correlations of these beta weights inside the ROI, and then averaging all of these pairwise correlations to obtain a single correlation value for each participant and session.
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-words}).
  Colored dots and lines indicate the pattern stabilities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics{figures/task-language_space-MNI152NLin2009cAsym_desc-images-words_stability.pdf}
\caption{\label{fig:stability-images-words}\textbf{\emph{Pattern stability for written words.}}
  Each panel \textbf{A}--\textbf{H} shows the development of within-condition pattern similarity for the contrast of written word blocks vs. fixation baseline over the course of the study in one region of interest (ROI).
  Pattern stability was computed by estimating beta weights separately for each written word block in the experiment, then computing all pairwise correlations of these beta weights inside the ROI, and then averaging all of these pairwise correlations to obtain a single correlation value for each participant and session.
  ROIs were defined either anatomically (posterior superior temporal sulcus [STS] and ventral occipito-temporal cortex [vOT]) or functionally from the significant clusters observed for the relevant baseline contrasts in the BOLD activity amplitude analysis (see Figure\ \ref{fig:univariate-words}).
  Colored dots and lines indicate the pattern stabilities of individual participants over time.
  The black dashed line indicates the fit of a linear mixed-effects model to these data.}
\end{figure*}

# Discussion

We used dense-sampling fMRI to longitudinally measure changes in children's brain activity as they are learning to read.
Seventeen children received reading instruction in their native language (Hindi) and alphabet (Devanagari) for approximately 16 months, during which they also participated in up to 6 fMRI scanning and behavioral testing sessions.
In this sample, we found
(a) some evidence for an improvement in reading performance based on standardized reading skill tests,
(b) limited evidence for longitudinal increases (linear and non-linear) in BOLD activity in response to spoken and written words, and
(c) limited evidence for longitudinal increases in word-related audiovisual BOLD pattern similarity.
We will discuss each of these findings in turn as well as the limitations and research implications of the present study.

## Changes in behavioral test scores

As would be expected from approximately 1.5 years of systematic reading instruction, children's test scores on various aspects of reading performance improved over the course of the study.
This included correctly reading real Hindi words and pseudowords as well as language skills closely related to reading, such as phoneme replacement and semantic fluency.
We additionally observed improvements in some non-reading related tests (maths and working memory), which may be the consequence of general cognitive development and/or informal schooling from parents or siblings.

## Non-linear change in BOLD activity amplitude

We had hypothesized that learning to read would lead to an increase in pseudoword- and word-related BOLD activity amplitude in areas that are associated with reading (e.g., the VWFA in the vOT cortex) and audiovisual integration (e.g., the pSTS).
We had expected this change to be either linear, that is, increasing from the beginning to the end of the study, negative quadratic (inverted "u"-shaped), that is, increasing in the beginning of the study and then decreasing towards the end of the study.
We did not observe such a pattern for any experimental contrast or brain region, except for the contrast between written words and low-level visual controls (false fonts; see Figure\ \ref{fig:univariate-words-minus-noise}); however, this effect was located at the medial wall of the brain (next to the posterior cingulate cortex), an area that has not typically been described as relevant for reading.
For different contrasts, areas showed evidence for linear and/or quadratic change over the course of the study, but this change typically did not follow the expected pattern (e.g., negative linear change or positive quadratic change) and did not occur in areas that relevant for reading or audiovisual integration.
Most importantly, most effects were observed for the baseline contrasts (contrasting one experimental condition against the fixation baseline; see Figures \ref{fig:univariate-noise} to \ref{fig:univariate-words}).
These contrasts are difficult to interpret as they capture not only higher-level (e.g., orthographic, phonological, and semantic) processing, but also low-level sensory processing.
The experimental contrasts (contrasting different word-like stimuli with different levels of phonological and semantic information, e.g., written words vs. pseudowords) showed little to no reliable longitudinal change in BOLD activity (see Figures \ref{fig:univariate-pseudo-minus-noise} to \ref{fig:univariate-words-minus-pseudo})
There are several reasons that could explain these null effects, including the low sample size in our study, an intervention that was too short or too weak, or a lack of sufficient high-quality data per participant and session.
These limitations will be discussed in more detail below.

## Increase in audio-visual pattern similarity

We had hypothesized that learning to read would lead BOLD activity patterns to become more similar in response to written and spoken (pseudo-)words, especially in areas associated with reading (e.g., the VWFA in the vOT cortex) and audiovisual integration (e.g., the pSTS).
Indeed, we did find an increase in audio-visual multi-voxel BOLD pattern similarity, as responses for written words (versus written low-level controls) became more similar over time to the responses to spoken words (versus spoken low-level controls) in the left ventral occipito-temporal cortex (see Figures \ref{fig:similarity-words-minus-noise}C).
However, beyond the limitations mentioned above (and discussed in detail below), it is important to mention that our study design was not ideal for capturing similarities in BOLD activity patterns.
This is because we used a block design with which it was not possible to estimate activity patterns for individual stimuli (e.g., the written word "monkey") but only for experimental conditions (i.e., written words in general).
Assuming that different stimuli elicit different BOLD activity patterns (e.g., depending on their individual phonology and semantics), we would have needed to use an event-related design to accurately capture these patterns and their potential increase between different sensory input modalities.
However, it may also be that our initial hypothesis was wrong and that written and spoken words are processed in completely separate processing streams, converging only at some very abstract semantic level (likely situated in a region that was not captured by any of our ROIs).
While audio-visual integration regions like the pSTS have been shown to respond strongly to input from both modalities [e.g., @blau2010; @calvert1997; @vanatteveldt2004; @wilson2018], they may not be engaged in tasks where stimuli are always presented in one modality at a time (either spoken or written) and do not need to be evaluated for audio-visual congruence.
Finally, it may also be that automatic audio-visual processing of written words ony develops only relatively late after starting to learn to read, and that this developmental change therefore was not captured during the duration of our study.

Finally, we also tested for longitudinal increases in multi-voxel BOLD response pattern stability, that is, if response patterns to repeated presentations of the same category of stimuli (e.g., written words) became more similar to one another as children learnt to read.
We did find some evidence for a longitudinal changes in pattern stability, but only spoken pseudowords and words in the bilateral STS.
Of note, pattern stability for these conditions and areas *decreased* rather than *increased* over the course of the study.
Since we did not predict any change in pattern stability for the spoken conditions, we prefer to refrain from interpreting these findings.

## Limitations

Every empirical research study has limitations and this one is definitely not an exception.
First and foremost, the sample size of this study was extremely small, both in terms of the number of participants ($N = 15$ in the reading intervention group) and in terms of the number of longitudinal sessions per participant ($T = 6$ for most participants).
With this small sample size, statistical power for detecting effects is very low, which can lead to (a) undetected true effects (false negatives), (b) potentially unreplicable obtained effects (low positive predictive value), and (c) potentially inflated effect sizes for obtained effects [ @button2013; @ioannidis2005; @szucs2017].
BOLD signal changes for higher-level (non-sensory) experimental manipulations (e.g., words vs. pseudowords) are typically subtle in size compared to the neural, thermal, and scanner-induced noise of fMRI data.
When wanting to track changes longitudinally from session to session, the effects of interest can be presumed to be even smaller (since one is interested in the manipulation \times time interaction rather than in the main effect of the manipulation).
These problems of small sample size, small effect size, and  low statistical power are potentiated when scanning children as their shorter attention span and increased head motion lead to both shorter scanning sessions (i.e., less trials/volumes) and lower data quality (i.e., higher framewise displacement and signal intensity changes).
To reliably answer the research questions posed in the Introduction, one would presumably need a sample size that is at least 10 times larger than ours in terms of participants and ideally also significantly larger in terms of the number of sessions per participant, both of which we failed to acquire due to monetary and time constraints.

Second, the reading intervention employed in the present study might not have been very effective, as we were unable to closely monitor the quantity and quality of the teaching sessions.
While the behavioral test scores (see Figure\ \ref{fig:behavior}) indicate some improvement on relevant subtests of reading ability for some children, we by and large cannot be sure how many children became significantly better readers over the course of the study.
Therefore, it is not surprising that we found little to no reliable change in word-related BOLD activity amplitudes and patterns.
On a similar note, the duration of our intervention (approx. 16 months) might not have been long enough for children to become relatively proficient readers, especially since the Devanagari alphabet contains a relatively large number of visually complex letters.
Our study design therefore was not ideal to capture the full process of learning to read and might have missed especially the late, *orthographic* stage of reading development (see Introduction).

Finally, there were minor issues in our experimental design that could be improved in the future, including
(a) relatively long blocks, which might have induced tiredness and additional head motion for some participants,
(b) relatively short baseline intervals between blocks, which lead to overlap of BOLD activity from one block to the next,
(c) no optimization of the order of blocks with regards to the efficiency of the experimental design [@henson2004], and
(d) individual stimuli being shorter (1 s) than the TR (2.55 s), which precluded any item-level analysis (e.g., with regards to BOLD pattern similarity and stability).

## Implications for future research

Based on the limitations mentioned directly above, we advise the reader not to draw any strong conclusions from the empirical effects reported in this paper (or any lack thereof).
However, we still believe that this study can stimulate and facilitate future research in at least two important ways.

First, our overall study design can be seen as a proof of principle for overcoming geographic and cultural biases in developmental cognitive neuroscience.
Most previous research on learning to read and its neural correlates has been carried out with participants from the Global North and in languages with alphabetic writing systems.
In the future, it will be crucial to test if the empirical findings and theories derived from these populations will generalize to different cultures and writing system, or if those require novel hypotheses and theories [@frost2012; @share2008; @share2014; @share2021].
We have shown that it is indeed possible---though certainly challenging---to collect relatively high-quality neuroimaging data in geographic areas and from socio-economic strata that have traditionally been neglected in cognitive neuroscience [@blasi2022; @henrich2010].
In the context of learning to read and other learning interventions, such population can indeed offer unique advantages, e.g., because illiteracy or lack of public schooling can allow for targeted and controlled learning interventions for limited periods of time, as we attempted in the present study^[Note that in collaboration with a local non-governmental organization, we ensured that participants in our study would get enrolled at a public primary school after the conclusion of the study, something they would otherwise most likely never have gotten the opportunity to.]
We therefore encourage more researchers to engage in cross-cultural collaboration projects, to test the generalizability of empirical findings and theories across different geographic and socio-economic settings, or to at the very least explicitly acknowledge the somewhat limited epistemic scope of findings obtained solely from convenience samples in the Global North.

Second, we developed a novel computational approach for analyzing longitudinal fMRI data with whole-brain linear mixed-effects models.
Compared to traditional statistical models for group-level fMRI analysis (typically variants of $t$-tests an analyses of variance [ANOVAs]), linear mixed-effects models come with a number of advantages:

* They can adequately control for multiple repeated measures obtained from the same participants.

* They can model longitudinal time as a continuous predictions variable instead of requiring discrete "sessions".

* They do not require a balanced study design (e.g., the same number of sessions for all participants), and are therefore better able to handle dropout and missing data.

* They can be used to include multiple predictor variables of different type (e.g., linear time, quadratic time, group membership, gender, ...).

* They can be used to relate within-individual fMRI effects to between-individual characteristics [i.e., brain--behavior associations\; but note that this typically requires large sample sizes\; @marek2022].

There have been previous implementations of whole-brain linear mixed-effects models, typically using the R programming language [@chen2013a; @madhyastha2018].
We believe that our implementation using the Julia programming [@bezanson2017; @bates2024] provides another step forward towards their wider applicability, as model fitting in Julia is orders of magnitude faster and less prone to convergence errors, both of which is critical when testing the same model on hundreds of thousands of voxels in mass univariate fashion.
We combined this with a recently established method for whole-brain multiple comparison correction that is compatible with the outputs of the linear mixed-effects models to control the whole-brain family-wise error rate [@cox2017a; @cox2017].
We hope that this analysis approach and our code (available at <https://github.com/SkeideLab/SLANG-analysis/blob/SLANG/scripts/univariate.py>) can be reused for future longitudinal fMRI studies or other studies with complex data acquisition schedules that would benefit from the flexibility provided by linear mixed-effects models.

## Conclusion

In this longitudinal fMRI studies, we repeatedly measured children's brain activity in response to written and spoken words as they learnt to read.
We found some evidence for non-linear changes in word-related BOLD activity amplitude but not in the direction or anatomical locations that we had hypothesized.
We also found some evidence that learning to read leads to more similar BOLD activity patterns for visual and auditory stimuli in ventral occipito-temporal cortex.
We found no evidence that learning to read leads to more stable BOLD activity response pattern in response to words to pseudowords.
While this study had a number of limitations, including a very small sample size, future studies might want to reuse our overall study design, including the focus on a participant population and writing system typically neglected in developmental cognitive neuroscience, as well as our analysis methods for longitudinal whole-brain fMRI analysis.

\newpage

# References

\raggedright
\setlength{\cslhangindent}{0.25in}
<div id="refs"></div>
\justify
